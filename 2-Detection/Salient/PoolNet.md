### A Simple Pooling-Based Design for Real-Time Salient Object Detection
- 论文地址：https://arxiv.org/abs/1904.09569
- 作者： Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Jiashi Feng, Jianmin Jiang
- 机构： 南开大学
- 发表：CVPR2019
- 介绍文章： https://mp.weixin.qq.com/s/urgkUcu2ZWQMGPZdArWzYg 
- 代码地址： https://github.com/backseason/PoolNet 


#### 介绍
这是一篇发表于 CVPR 2019 的关于显著性目标检测的 paper，在 U 型结构的特征网络中，高层富含语义特征捕获的位置信息在自底向上的传播过程中可能会逐渐被稀释，另外卷积神经网络的感受野大小与深度是不成正比的。

目前很多流行方法都是引入 Attention（注意力机制），但是本文是基于 U 型结构的特征网络研究池化对显著性检测的改进，具体步骤是引入了==两个模块GGM (Global Guidance Module，全局引导模块) 和 FAM (Feature Aggregation Module，特征整合模块)，进而锐化显著物体细节，并且检测速度能够达到 30FPS。==因为这两个模块都是基于池化做的改进所以作者称其为 PoolNet。

#### 模型架构

![1565861984833](C:\Users\j00496872\Desktop\Notes\raw_images\1565861984833.png)



**两个模块**

**GGM（全局引导模块）**

我们知道高层语义特征对挖掘显著对象的详细位置是很有帮助的，但是中低层的语义特征也可以提供必要的细节。因为在 top-down 的过程中，**高层语义信息被稀释，而且实际上的感受野也是小于理论感受野**，所以对于全局信息的捕捉十分的缺乏，导致显著物体被背景吞噬。

因此作者提出了 GGM 模块，GGM 其实是 **PPM（Pyramid Pooling module，金字塔池化模块）**的改进并且加上了一系列的 **GGFs（Global Guiding Flows，全局引导流）**，这样做的好处是，在特征图上的每层都能关注到显著物体，另外不同的是，GGM 是一个独立的模块，而 PPM 是在 U 型架构中，在基础网络（backbone）中参与引导全局信息的过程。 

其实这部分论文说得并不是很清晰，没有说 **GGM** 的详细结构，我们可以知道 **PPM** [7] 的结构如下：

![1565862111542](C:\Users\j00496872\Desktop\Notes\raw_images\1565862111542.png)

该 PPM 模块融合了 4 种不同金字塔尺度的特征，第一行红色是最粗糙的特征–全局池化生成单个 bin 输出，后面三行是不同尺度的池化特征。为了保证全局特征的权重，如果金字塔共有 N 个级别，则在每个级别后使用 1×1 的卷积将对于级别通道降为原本的 1/N。再通过双线性插值获得未池化前的大小，最终 concat 到一起。 

如果明白了这个的话，其实 **GGM** 就是在 **PPM** 的结构上的改进，**PPM** 是对每个特征图都进行了金字塔池化，所以作者说是嵌入在 U 型结构中的，但是他加入了 **global guiding flows（GGFs）**，即 Fig1 中绿色箭头，引入了对每级特征的不同程度的**上采样映射**（文中称之为 identity mapping），所以可以是个独立的模块。

简单地说，作者想要 FPN 在 top-down 的路径上不被稀释语义特征，**所以在每次横向连接的时候都加入高层的语义信息**，这样做也是一个十分直接主观的想法。 



**FAM（特征整合模块）**

**特征整合模块也是使用了池化技巧的模块**，如下图，先把 **GGM** 得到的高层语义与该级特征分别上采样之后横向连接一番得到 **FAM** 的输入 b，之后采取的操作是先把 b 用 {2,4,8} 的三种下采样得到蓝绿红特征图然后 avg pool（平均池化）再上采样回原来尺寸，最后蓝绿红紫（紫色是 FAM 的输入 b）四个分支像素相加得到整合后的特征图。

![1565862158785](C:\Users\j00496872\Desktop\Notes\raw_images\1565862158785.png)

**FAM 有以下两个优点：** 

1. 帮助模型降低上采样（upsample）导致的混叠效应（aliasing）；

2. 从不同的多角度的尺度上纵观显著物体的空间位置，放大整个网络的感受野。 

第二点很容易理解，从不同角度看，不同的放缩尺度看待特征，能够放大网络的感受野。对于第一点降低混叠效应的理解，用明珊师姐说的话，混叠效应就相当于引入杂质，GGFs 从基础网络最后得到的特征图经过金字塔池化之后需要最高是 8 倍上采样才能与前面的特征图融合，这样高倍数的采样确实容易引入杂质。

作者就是因为这样才会提出 FAM，进行特征整合，**先把特征用不同倍数的下采样，池化之后，再用不同倍数的上采样，最后叠加在一起。**因为单个高倍数上采样容易导致失真，所以补救措施就是高倍数上采样之后，再下采样，再池化上采样**平均下来可以弥补错误**。

![1565862193272](C:\Users\j00496872\Desktop\Notes\raw_images\1565862193272.png)

上图就是为了说明 FAM 的优点的，经过高倍上采样之后的图像（b）和（d）容易引入许多杂质，致使边缘不清晰，但是经过 FAM 模块之后的特征图就能**降低混叠效应**。



#### 实验结果

论文在常用的 6 种数据集上做了实验，有 ECSSD [8], PASCALS [9], DUT-OMRON [10], HKU-IS [11], SOD [12] 和 DUTS [13], 使用二值交叉熵做显著性检测，平衡二值交叉熵（balanced binary cross entropy）[14] 作为边缘检测（edge detection）。

以下是文章方法跟目前 state-of-the-arts 的方法的对比效果，绿框是 GT，红框是本文效果。可以看到无论在速度还是精度上都有很大的优势。

![1565862239845](C:\Users\j00496872\Desktop\Notes\raw_images\1565862239845.png)

![1565862265675](C:\Users\j00496872\Desktop\Notes\raw_images\1565862265675.png)



![1565862287993](C:\Users\j00496872\Desktop\Notes\raw_images\1565862287993.png)

论文还针对三个改进的技术 PPM、GGFs 和 FAMs 的不同组合做了实验，(a) 是原图，(b) 是 Ground truth，(c) 是 FPN 的结果，(d) 是 FPN+FAMs，(e) 是 FPN+PPM，(f) 是 FPN+GGM，(g) 是 FPN+GGM+FAMs。

![1565862320698](C:\Users\j00496872\Desktop\Notes\raw_images\1565862320698.png)



![1565862346648](C:\Users\j00496872\Desktop\Notes\raw_images\1565862346648.png)



#### 总结

该 paper 提出了两种基于池化技术的模块 GGM（全局引导模块）和 FAM（特征整合模块），改进 FPN 在显著性检测的应用，而且这两个模块也能应用在其他金字塔模型中，具有普遍性，但是 FAM 的整合过程我认为有点像是**用平均中和了上采样带来的混叠效应**，但是不够优雅，**先下采样池化再上采样带来的损失可能代价太大**。