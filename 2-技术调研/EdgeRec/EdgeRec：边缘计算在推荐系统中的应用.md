## EdgeRec：边缘计算在推荐系统中的应用

文章地址：https://cloud.tencent.com/developer/news/778595

**导读：**推荐系统在今天互联网产品和应用中起着举足轻重的地位。一般的推荐系统是基于云端计算中心到边缘 ( 比如手机、平板电脑等 ) 的结构，这其中会由于网络带宽和延迟等导致结果的延迟。边缘的实时计算可以利用用户的实时信息提供更好的推荐。本次分享题目为 “EdgeRec：边缘计算在推荐系统中的应用”。

主要内容包括：

- 边缘计算背景介绍
- 端上重排系统
- 端上混排系统
- 端上智能请求
- 端上训练与千人千模



### **01  边缘计算背景介绍**

**1. 边缘计算 v.s. 云计算**

![image-20220209150145711](D:\Notes\raw_images\image-20220209150145711.png)

- 在目前云计算时代，互联网应用和用户的规模呈现爆炸式的增长；
- 同时5G的普及和带宽的增加给对云端的整个系统带来了存储的压力；
- 搜索推荐广告的大规模神经网络模型的应用对云端系统造成极大的计算负载压力; 
- 云计算的集中式的计算模式，也增加了运维成本和故障的风险。

边缘计算，对比与传统的云计算，有着以下的优势。

- 首先，目前用户手机等终端设备的计算和存储能力发展迅速；
- 另一方面，数据的本地化可以缓解云存储的压力，同时也增强了数据的隐私；
- 计算的本地化可以缓解云计算过载问题；
- 边缘计算也降低了通信成本，可以增强用户的交互和体验；
- 去中心化也可以规避==中心化的故障==。

总结起来边缘计算的优点就是：稳定性，高带宽，低延时，隐私性。



**2. 重新思考工业界信息流推荐？**

![image-20220209150220681](D:\Notes\raw_images\image-20220209150220681.png)

推荐系统是把经过排序的最贴近用户兴趣的推荐列表展示给用户。信息流推荐是一个人机交互系统，其目标是整体收益的最大化。

这与传统搜索引擎的目标，比如==上下文环境、相关性==，有着一定的区别。==目前的推荐系统并没有同信息流推荐的场景成长起来，而更多的是借鉴于搜索和广告的技术，比如点击率(CTR)预估等==。

信息流推荐的问题在工程系统和算法模型上都面临着严峻的考验。由于其==人机交互性==，==复杂的上下文环境==，==强调整体的收益==等，信息流推荐要求模型在==策略复杂度大幅度提升==的同时，兼顾系统的==实时性与灵活性==。



**3. 推荐系统新架构**

![图片](D:\Notes\raw_images\zHbzQPKIBPhV7nOlSsXV8w9M11.jpg)



![img](D:\Notes\raw_images\v2-287098a2bde610a1893eb5155a296b40_720w.jpg)

得力于边缘计算的特点与优势，我们将整个展现决策系统放在离用户交互最近的设备上，并且重新定义了整个信息流推荐的新的体系架构。



### **02  端上重排**

**1. 业务问题-推荐系统输入/反馈实效**

![image-20220209145819655](D:\Notes\raw_images\image-20220209145819655.png)

目前已有的整个信息流采用了Client-and-Server的推荐架构。这种架构有诸多缺点。

- 首先由于整个系统的每秒查询数（QPS）的限制，人们一般会采用分页请求机制。分页请求会导致策略调整的不及时。比如如果分页请求是20页，在分页之前，用户的任何行为都无法对整个推荐结果生产生任何影响。
- 其次推荐模型的个性化==强依赖于用户的行为特征==, 但目前云端==获取用户行为的延迟在秒级甚至分钟级==，很多用户==细粒度的行为==都无法被建模到。
- 最后用户==偏好的变化==与推荐系统对用户的感知和对==内容调整时机==并不能产生匹配。这会导致推荐的内容并非用户当前时刻想要看到的，这会导致用户的浏览和点击意愿的下降。

通过“首页猜你喜欢”场景的分坑位点击率统计可以看到一个波峰到波谷的现象，这反映了由于分页请求机制导致的兴趣无法实时匹配问题。



**2. 端上实时用户感知**

![图片](D:\Notes\raw_images\zHbzQPKIBPhV7nOlSsXV8w9M21.jpg)

为了解决用户行为延迟和细粒度和粒度粗的问题，我们提出了端上实时用户感知的框架，同时从个性化、正反馈、负反馈、交互动作和实时性这五个方面进行了思考。

首先用户行为数据对于个性化意义重大。比如各种 Deep Interest Evolution Network (DIEN)，Deep Interest Network(DIN)等工作。我们之前的工作==一般只考虑用户和商品的“正反馈”交互（如点击、成交）==，==而很少考虑到“负反馈”交互（如曝光）==。实际上==实时的“负反馈”交互非常重要，比如商品多次曝光后点击率会呈明显的下降趋势==。

同时我们需要考虑用户与商品的交互动作，比如==点击后在详情页的行为能反映对商品真正的偏好==，也有可能是存在==伪点击==。个性化的“实时行为”相对于“长期行为”对信息流推荐场景同样重要。

但是如何对用户端上的用户行为进行建模面临着以下两个难点。首先是如何对用户行为的异构性进行序列建模；其次是如何优化端上序列模型推理的性能。



![图片](D:\Notes\raw_images\zHbzQPKIBPhV7nOlSsXV8w9M211.jpg)

上图是我们在端上建设的用户实时行为的特征体系，其中包含了20多种细粒度用户行为动作特征。这其中分为两类：

第一类是用户在商品的上的==曝光行为动作特征==，比如曝光时长、滚动速度等；

第二类是用户在点击商品后==详情页的行为动作特征==，比如各区块是否点击、详贴页的停留时长等。



![图片](D:\Notes\raw_images\zHbzQPKIBPhV7nOlSsXV8w9M2111.jpg)

在异构行为的序列建模中，==用户行为被定义成<商品，动作>的2元组==。我们同时建模了商品曝光和点击两类行为特征。对于模型的设计，这里重点提两点，

- 第一是我们==把商品曝光行为序列和商品点击行为序列先**分别单独**进行建模==。这是因为点击行为一般比较稀疏，而曝光行为非常多。如果先融合成一条行为序列再进行建模，很有可能模型会被曝光行为所主导。
- 第二是，我们事先==分别对商品特征和动作特征encode然后再进行fusion==。这里主要考虑商品特征和行为动作特征属于异构输入。如果下游任务需要对商品特征进行attention，只有对同构的输入才有意义。

在端上推理过程中，我们做了三方面的优化来保证序列模型的运行效果。

- 首先是==用户感知模型==与==决策模型==解耦异步运行；
- 同时随着用户的行为进行实时推理；
- 并且将得到的用户状态在手机本地进行持久化，做到随取随用。

在这样的框架下，端上用户感知支持了下游多个决策模型，并且提供了端上通用的端上用户学习框架。



**3. 端上重排-Context-aware Re-ranking with Behavior Attention Networks (CRBAN)**

![图片](D:\Notes\raw_images\zHbzQPKIBPhV7nOlSsXV8w9M212.jpg)

为了解决排序策略调整实际延迟的问题，我们提出了以下端上重排方案。其目标是==根据用户的正负反馈来实时改变信息流的推荐顺序==。其难点在于如何高效的利用异构用户行为序列进行建模。因此我们提出了 Context-aware Re-ranking with Behavior Attention Networks (CRBAN)模型，其特点是==在重排模型的框架下引入了用户实时行为上下文==。

==相比于DNN一类的模型，我们除了考虑用户交互商品的特征，也会考虑用户交互商品的**行为动作特征**==。基于前文介绍的异构行为序列建模，我们可以看到在重排任务中，attention的query和key分别是待排序商品和行为序列商品，而value是行为商品和行为动作fusion之后的结果。其动机可以解释为对待排序商品集合中的某一个商品，模型先学习用户交互过的商品的特性，然后重点关注相关的商品。与此同时我们再观察用户在这些商品上的具体的动作表现是什么，比如曝光时长、经验的行为等。我们把以上特征综合起来一起作为待排序商品的打分参考。在整个控制变量的离线实验中表明我们提出的==不同组件非常有效==。

![image-20220223155043025](D:\Notes\raw_images\image-20220223155043025.png)



**4. 端上重排-系统设计**

![图片](D:\Notes\raw_images\zHbzQPKIBPhV7nOlSsXV8w9M2.jpg)

除了算法模型创新优化，我们也首次提出并建设了端上的实时排序系统架构。该系统融合了端和云的各自优势，并且并不影响对目前已有云端的推荐链路——==支持端上推荐系统的热插拔==。另一方面面对搜索推荐深度学习模型的特点和端上环境的限制，我们构造了一套端上支持部署大规模神经网络的解决方案。

尤其值得着重提出的是，推荐模型一般会达到几百GB甚至TB的大小。如此大的模型存储量目前不可能部署到用户的手机端。实际上商品的embedding矩阵，比如商品ID，占了整个模型存储的大部分。但排序的候选商品相对稀疏。因此我们提出了将模型中的embedding矩阵重新拆分部署到云端，并通过云端返回相应的商品带回到端上，然后再在端上通过模型重新组织图关系来进行inference打分。我们目前用==MNN作为目前整个端上的深度学习推进引擎==。



**5. 端上重排-业务效果**

![图片](D:\Notes\raw_images\image-20220209145926792.png)

上图左部分显示了端上重排取得了显著的业务效果提升；右部分显示的端上重排对首页信息流分坑位CTR的影响。我们可以看到端上重排一定程度上缓解了由于分页请求带来的效果衰减。部分成果及技术细节发表在CIKM-2020。



**6. 端上重排2.0-生成式排序**

![图片](D:\Notes\raw_images\zHbzQPKIBPhV7nOlSsXV8w9M1.jpg)

重排任务本质上是希望在整个推荐体系多轮排序的最后一轮能够找到最好的商品的排序结果给用户进行展示。但是目前无论是从粗排，精排，还是learning-to-rank等重排模型，都依赖于单点打分，贪心排序。它们都只是在整个计算性能约束下的效果折中方案。而我们实际上想要达到的目标是如何在N个候选商品中找出K个商品的排列，使得整体的点击次数最大化。这是一个组合优化问题，其难点在于整个策略空间非常大（$A_N^K$)。比如如果从100个商品里面选50个，那么策略空间就是($A_{150}^{100}$)的排列数。策略空间大会带来线上样本稀疏的问题。目前基于监督学习的方法只能做到在有限的监督样本里探索有限的自由排序。我们的方案是依托于端上重排的链路。由于端上的触发时机非常实时，端上分页策略会非常短，整个策略空间就会小很多，比如从A_100^50降到A_100^40；另一方面端上不需要和云端交互，同时不需要依赖云端负载的计算，由于都是本地打分，对复杂模型的支持会更好。

同时在模型上我们提出了Generator-Evaluator的模型架构。Generator通过Pointer-Net实现“N中选K”的候选生成，同时训练Evaluator用来对排序进行打分，最后通过Policy Gradient的梯度策略方法来训练整个Evaluator。我们提出这种生成式重排相比于线上贪心排序的提升效果非常显著。部分技术细节发表在 SIGKDD-2019。



**7. 端上重排2.0-序列检索系统**

![image-20220209150026478](D:\Notes\raw_images\image-20220209150026478.png)

在实践重排的过程中，我们可以将整个重排架构统一成序列检索系统。重排复杂度是指数的，比其他单点预测，比如精排，要大很多。因此我们将重排任务采用了类似漏斗的结构：分别由“重排-召回”和“重排-精排”组成。在重排-召回阶段，我们通过策略生成，比如Beam Search的策略，来生成候选序列，也可以使用上面提到的生成模型，如Ptr-Net，来训练对齐重排-精排。通过这两种方式都可以来生成代打分序列。重排-精排可以通过监督任务来学习线上真实的曝光序列，我们称其为推荐系统的Simulator。Simulator可以完整的拿到整个曝光上下文的“穿越”信息。通过Simulator也可以预测每条待打分序列的价值，比如PV或者IPV，最后选出TOP1的序列。

重排的架构非常类似于召回和精排的关系：召回可以给精排提供候选集，同时精排也可以通过类似于蒸馏的方法来训练整个召回网络。此策略也对我们的整个场景带来了显著的业务效果提升。



### **03  端上混排系统**

**1. 信息流中的多元异构内容**

![图片](D:\Notes\raw_images\zHbzQPKIBPhV7nOlSsXV8w9M.jpg)

这里介绍我们在端上混排的工作。目前信息流已经成为业界推荐产品的主流形式。上图展现了在手淘首页信息流、手淘搜索和美团首页信息流的形式。我们可以发现信息流推荐并不只是单一内容的聚合，而是==集合非常多的多元异构形式==来对用户进行展示。比如在首页信息流里，我们会插入短视频、内容、广告等多元内容。因此==信息流推荐里混排相比于重排已经成为业务中不可缺少的模块==。



**2. 业务问题-推荐系统上下文环境**

![image-20220209150448458](D:\Notes\raw_images\image-20220209150448458.png)

信息流推荐是人机交互的系统，并有各种无法忽略的上下文环境。在信息流推荐里，上下文主要包含以下两个方面：

- 首先是用户近期行为的上下文，它反映了用户对某种类型卡片的实时交互行为及用户对不同类型内容的意图；
- 其次==位置周围卡片的上下文==。同样的内容在不同的位置和展现上下文下的情况下点击效率是不同的。

比如上图中红框标志的同样的广告卡片，左边卡片周围是包含直播视频活动等各种异构卡片，中间第二个卡片周围大部分主要是正常的商品。我们可以预见到两个相同卡片的点击率是不同的。==周围的信息会对卡片的点击率造成很大的影响==。



**3. 端上混排架构**

![图片](D:\Notes\raw_images\zHbzQPKIBPhV7nOlSsXV8w9M121.jpg)

每次展现决策都需要尽量的贴近符合用户最近的意图。这对重排和混排都是一样的。我们首次提出了端上混排架构。

该架构包含了两个阶段：

- 首先是混排-预估阶段，其重点在于不考虑任何组合的业务逻辑，基于精确的上下文来预估混排卡片的效率指标；
- 其次是混排-规划阶段，其着重于在业务规则限制下，比如同种类型卡片的打散规则，卡片的曝光占比等需求的限制下，寻找最优的展现排序。



**4. 端上动态混排算法**

![image-20220209150609186](D:\Notes\raw_images\image-20220209150609186.png)

在混排预-估阶段，我们提出 Edge Surrounding-Aware Network（ESAN）模型。该模型重点==建模了位置周围的内容和用户实时行为这两种上下文来预估target内容的点击率==。

整个==模型引入了端上的用户行为序列信息和周围商品的信息作为待插入内容的上下文特征==。因此==整个模型的实际打分数变成了候选的混排内容数与待插入位置数的乘积==。这样的打分数，相比于待排序内容的candidates数量会成倍增加，这会对整个打的压力造成一定的影响。因此我们把打分放到了用户的手机端上。ESAN的模型会输出插卡点击率的预估矩阵。在混排规划阶段，我们需要重点关注两个业务规则。首先是PV占比业务规则，它需要定量控制各业务卡片在整个信息流的曝光比例。第二是打散：为了保持用户的体验，同种的业务卡片在信息流的坑位需要有一定的间隔。因此混排规划的问题是在满足上面两个约束的情况下，如何规划混排卡片的插入坑位，使整体的混排效率预估最大化。我们把这个问题归约成带约束动态背包的问题，其中问题的输入是ESAN 输出的插卡点击率矩阵，在问题的规约上，我们把打散作为约束；PV占比的需求，可以看作是背包容量；坑位的PV贡献占比，可以看作是物品的重量；每个位置的插卡点击率，可以看作是物品的价值。这是一个整数规划问题，问题的解空间是指数级的。在线上我们采用beam search和基于分支限界的记忆化搜索的方法来进行求解。在==信息流短视频的混排效果上，CTR有7%的显著提升==。



### 04 端上智能请求

#### **背景**

在信息流推荐中，分页请求机制极为常见，一方面受限于网络带宽，为了减小延迟优化用户体验，另一方面为了能够留下调整推荐内容的空间，针对用户的推荐内容会被分批推送到端上。但考虑到QPS压力，一般分页的长度固定，而且不会特别小，这就带来一些问题：

1. **推荐内容更新的时机与用户意图变更的时机无法匹配。**如图1所示，用户的兴趣意图和目标商品随用户浏览而变化，但分页请求导致端上商品池更新时机固定，云端的推荐内容排序无法实时的对用户需求意图改变做出反馈。
2. **无法实时的对用户意图进行检测。**只有实时的用户行为特征才能反映用户当下的需求。每次分页请求后，服务端可以拿到用户的行为意图特征，并基于此进行召回排序。但由于分页请求时机固定，加上网络的带宽和延迟，用户的实时行为中大量全域细节行为无法上传，导致云端推荐模型对用户意图的推测出现偏差。
3. **无法动态分配请求资源，导致大量无效请求，而且无法实时调节QPS压力。**如下图，在分页请求的基础上，在每一个分页中以不同概率随机插入动态请求，随着概率的上升，用户的购买率也随之上升。由此可见更多的请求能够带来更好的推荐，导致更多的购买。由于==不同用户兴趣意图变更时机不同，不同时刻对不同用户施加一次请求的产生的价值也不同==，非个性化的统一分页请求会导致大量请求资源的浪费，线上效果提升与资源分配不匹配。

端上是请求的发起者和推荐内容展示终端，对用户行为有最实时的感知处理能力，因此我们希望通过**端上智能请求**来做个性化请求决策，提升推荐系统的实时感知和实时反馈的能力，同时预估请求价值，合理分配请求资源。

**端上智能请求**会==检测用户在端上检测用户兴趣意图的变化，通过**请求价值预估**得到在当下给该用户施加请求产生价值大小，再通过请求动态规划算法来做出是否请求的决策，让请求产生真正的线上收益==。如下图所示。

![img](https://pic4.zhimg.com/80/v2-b2ddca7055552e7f9cf5663fcb2fe36b_720w.jpg)

双11期间，在我们电商首页信息流推荐场景中，端上重排结合智能请求算法和请求价值预估模型，相比只结合动态请求规则(每页固定增加一次请求)，==线上成交金额提升超过**3%**==，说明智能请求能够实时感知用户兴趣变化，在更好的时机更新端上商品池，为用户带来更好的推荐，促使购买行为的发生。

#### **请求价值预估Request Uplift**

在请求价值预估中，我们使用 “==因果推断-请求价值增益模型==”，来==预测请求导致的购买率增益，而不是购买率本身==。如下图所示，由于不同用户对请求的敏感程度有差异，只有部分用户或者在部分情况下，给与更多的请求能够使线上产生真正的收益。部分情况下用户的行为不会因请求而改变，而有一小部分情况下请求甚至会有负面影响\[4][5]。所以我们用 “因果推断-请求价值增益模型” 分别建模施加请求和不施加请求下的购买率和对应的购买增益，推测出在特定情况下，由于增加请求导致的真实购买增益，来作为当前请求的价值。

![img](https://pic4.zhimg.com/80/v2-95f75eee67b1ffc374968eceef31d68f_720w.jpg)

#### **离线指标与线上效果**

我们把模型做出是否请求的决策之后n坑内是否发生购买行为作为Label。control组，treatment组合总体的AUC作为离线指标。

| all     | control | treatment |
| ------- | ------- | --------- |
| 0.79564 | 0.81003 | 0.78040   |

双11期间，端上重排[1]结合智能请求模型相比只结合智能请求规则(每页固定增加一次请求)，线上**成交笔数提升2%**，**成交金额提升3%**，说明智能请求能够实时的检测用户兴趣意图变化，在更加合适的时机发起请求来更新端上商品池，为用户带来更好的推荐，刺激购买行为的发生。同时，我们将智能请求的业务效果与规则请求持平后，发现**请求QPS能够降低32%**，对云端计算资源缩减也有很高的价值。

智能请求模型会在更合适的时机做出施加请求的决策，促使请求之后的购买率远大于不请求。如表中所示，==模型决定请求之后10坑内的购买率(%)是不请求的360%，是随机请求的259%==。数据表明智能请求模型大大提高了请求之后的购买率。

|      | 无请求  | 请求(随机) | 智能请求(模型) |
| ---- | ------- | ---------- | -------------- |
| 5    | 0.04816 | 0.06685    | 0.17521        |
| 10   | 0.09295 | 0.12935    | 0.33508        |
| 15   | 0.13439 | 0.18854    | 0.47695        |
| 20   | 0.17371 | 0.24266    | 0.83394        |





### **05  端上训练与千人千模**

**1. 业务问题-千人一模与数据隐私**

![图片](D:\Notes\raw_images\zHbzQPKIBPhV7nOlSsXV8w911M.jpg)“**千人千面**”是过去几年搜推广业务增长的关键点，如何个性化信息检索结果成为了算法的工作重点，给搜推广业务带来了巨大的业务增长。目前，“千人千面”是源自特征层面的个性化，比如普遍使用的用户历史行为特征、用户长短期统计等，而模型都是使用统一的大模型。同时，用户的行为数据会从客户端传回服务端，然后在云端训练统一的推荐模型。大模型的方式，能够很好的表达Item-to-Item、User-to-User的相关关系，在保持个性化的能力下提升模型的泛化性。

- 然而，这种千人一模的方式，也带来了**推荐公平性**的问题。==目前大部分场景里，20%的高活用户贡献了80%的训练样本，这也导致了模型会更加偏向于高活用户的习惯进行推荐==，从下面图中曲线可以看出，日均PV较低的用户其模型打分准度（GAUC）也较低。
- 另外，千人一模的方式，也会有**数据隐私保护**的问题。因此，我们希望能为每个用户提供其个性化模型，以达到极致的个性化。

但以目前首页信息流约1.2亿日活，“千人千模”这种模式在云端训练和存储并不现实。因此我们希望借助端计算的能力，在用户的手机本地来训练和部署整个性化的模型，同时这也能达到一定的用户隐私的保护。

#### **训练个人模型的难点**

虽然个人模型有很高的研究价值，但是该工作的难点也很多。首先我们使用了最常用的方法，基于所有用户日志训练出元模型，并在单个用户日志上FineTune的方式，产出用户个性化模型，整体流程如下图1。通过该方式发现的问题有：

1. ==长期训练不稳定==：如下图2，随着个人数据的多次训练，逐渐==过拟合于用户本人局部数据==，导致用户-用户的协同过滤信息缺失，最终auc反而会变差。
2. ==用户行为不稳定==：如下图3，取第N天大模型每个人用户表征层的输出值平均值，拼成一个向量做聚类，通过K-means算法聚成两个类别，并通过PCA降维到两维，可视化成红色和蓝色两类。而当使用第2天蓝色部分用户的数据，继续通过该模型输出用户表征层输出值时，却发现表现为黄色部分。即==不同天之间用户的行为有很大的变化==。

为了验证该问题，我们取了不同CTR区间的人群，使用N-1天全人群大模型做Meta Model，并使用单独人群第N天数据做FineTune得到FineTune Model，验证FineTune模型对比Meta Model的在N+2天数据上评估的提升。并分别对比该人群全部用户和稳定用户（第N+2天用户Ctr仍在该区间内）的效果，整体。结果如下图4，==发现稳定用户部分的提升，相比全部用户部分，GAUC提升会高出不少。而这块也是用户个性化模型的增长空间==。

![img](https://pic3.zhimg.com/80/v2-cea1e159ce7b9810681f678e7cd8e5de_720w.jpg)

#### **MetaFusion个性化模型融合算法**

为了解决以上提出的两个问题，我们提出了MetaFusion框架和相应的算法，其中框架如下图所示：

![img](https://pic1.zhimg.com/80/v2-d4d08805abe4ec5265f1ade3a854700c_720w.jpg)

为了解决单人数据训练过拟合的问题，我们决定使用人群来训练人群的FineTune模型，通过==将单个用户扩大到用户群体，可以一定程度解决训练样本稀少和数据不均衡问题==，对于人群的划分，其目标同个性化模型FineTune类似，是为了缓解在推荐中高活用户的主导现象，同时也旨在为存在冲突性行为的不同用户建立更加个性化的推荐模型。

为了解决用户行为不稳定的问题，我们根据用户自身信息来做Meta模型和FineTune模型的个性化融合，识别用户的不稳定程度，并对两个模型进行融合。其原因是，即使在分类人群的数据样本Ui上训练拟合了，但Ui在后一天的数据上的特征可能已经变化了（即有一部分用户已经不在之前划分的区间标准内了），如5.2部分的图3所示，因此依旧用拟合训练集的模型不一定有效，需要学习用户特征的变化情况来提高分类模型的稳定性。

在原有全量数据的基础上，我们可以得到集中元模型，其模型参数用$W_m$表示，对于每一个人群而言，我们利用其对应部分的第T天的数据作为训练集样本，T+1天的数据作为测试集。在训练集上得到模型参数$W_p$，该模型我们认为其拟合第T天的用户行为，为了使模型==在T+1天保持有一定的稳定性，引入了Transfer-Layer==，并结合用户稳定性统计特征$U_s$，融合$W_p$和$W_m$的模型参数，学习$W_p$和$W_m$的模型变化和用户稳定性统计特征之间的关系，最终融合出$W_f$。==我们认为Transfer Layer是有更高的泛化性的，能支持多天之间的用户状态变化==。模型结构如下图所示。

![img](https://pic2.zhimg.com/80/v2-94c3a8c95159ab0091481dc60fb1cfe5_720w.jpg)

离线效果上如下表，可以看到MetaFusion后，GAUC提升比单独FineTune后都有相应的提升。==可以认为“稳定用户FineTune GAUC提升”是用户个性化模型的能力上限，Meta-Fusion后的确向该值逼近了一些==。我们将通过Meta-Fusion产生的用户个性化模型利用端智能的分离部署能力，部署在了用户个人手机上。==在线效果上，在不同人群上普遍有2%的Ctr和点击量的提升==。

| CTR区间   | FineTune GAUC提升 | 稳定用户FineTune GAUC提升 | Meta Fusion Model GAUC提升 |
| --------- | ----------------- | ------------------------- | -------------------------- |
| 0-0.02    | 0.00317           | 0.00893                   | 0.00567                    |
| 0.02-0.04 | 0.00331           | 0.0072                    | 0.00423                    |
| 0.09-1    | 0.00842           | 0.00903                   | 0.00875                    |



### 总结

EdgeRec建设的==三年内==，每一年都有新进展，==双十一GMV提升从19年的+5%，到20年的+8%，到今年的+13%==，每年都展现了其巨大的增长空间，成为推荐系统中不可或缺的系统。截至目前，端上推荐系统体系的潜力做了一定兑现。接下来，端智能需要与云上算法做端云算法间的协同，端上发挥其信息量大、信息实时的优势，云端发挥其候选存储空间更大的优点，通过端云协同提升业务效果。



### 参考文献

[1] Gong Y, Jiang Z, Feng Y, et al. EdgeRec: recommender system on edge in Mobile Taobao[C]//Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2020: 2477-2484.

[2] Chen, Fei, et al. "Federated meta-learning for recommendation." arXiv preprint arXiv:1802.07876 (2018).

[3] Liu C, Li X, Cai G, et al. Non-invasive Self-attention for Side Information Fusion in Sequential Recommendation[J]. arXiv preprint arXiv:2103.03578, 2021.

[4] Gutierrez, Pierre, and Jean-Yves Gérardy. "Causal inference and uplift modelling: A review of the literature." International Conference on Predictive Applications and APIs. PMLR, 2017.

[5] Künzel, Sören R., et al. "Metalearners for estimating heterogeneous treatment effects using machine learning." Proceedings of the national academy of sciences 116.10 (2019): 4156-4165.

[6] Goldenberg, Dmitri, et al. "Free Lunch! Retrospective Uplift Modeling for Dynamic Promotions Recommendation within ROI Constraints." Fourteenth ACM Conference on Recommender Systems. 2020.



---

**文章作者：**

**龚禹**   阿里巴巴 | 算法专家

龚禹 ( 花名：凛至 )，2017年硕士毕业于上海交通大学。曾在SIGIR、KDD、AAAI等发表多篇论文，其中IRGAN曾获SIGIR2017最佳论文提名。研究方向包括了推荐系统与自然语言处理等，目前专注于边缘计算与推荐系统的结合，主导的EdgeRec系统已经在手淘推荐场景大规模落地。