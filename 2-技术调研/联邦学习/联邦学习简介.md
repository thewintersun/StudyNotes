## 联邦学习简介

开源地址：https://github.com/FederatedAI/FATE

本质：联邦学习本质上是一种**分布式**机器学习技术，或机器学习**框架**。

目标：联邦学习的目标是在保证数据隐私安全及合法合规的基础上，实现共同建模，提升AI模型的效果。

前身：联邦学习最早在 2016 年由谷歌提出，原本用于解决安卓手机终端用户在本地更新模型的问题；



![image-20220126120634376](D:\Notes\raw_images\image-20220126120634376.png)

<img src="D:\Notes\raw_images\image-20220126145857795.png" alt="image-20220126145857795" style="zoom:67%;" />

<img src="D:\Notes\raw_images\image-20220126150010605.png" alt="image-20220126150010605" style="zoom: 80%;" />

### 联邦学习的分类

我们把每个参与共同建模的企业称为参与方，根据多参与方之间数据分布的不同，把联邦学习分为三类：横向联邦学习、纵向联邦学习和联邦迁移学习。

![image-20220126150955150](D:\Notes\raw_images\image-20220126150955150.png)



### 横向联邦学习

**适用场景：**

横向联邦学习的本质是**样本的联合**，适用于参与者间业态相同但触达客户不同，即特征重叠多，用户重叠少时的场景，比如不同地区的银行间，他们的业务相似（特征相似），但用户不同（样本不同）

**学习过程：**

![image-20220126151208562](D:\Notes\raw_images\image-20220126151208562.png)

step 1：参与方各自从服务器A下载最新模型；

step 2：每个参与方利用本地数据训练模型，==加密梯度上传给服务器A==，服务器A聚合各用户的梯度更新模型参数；

step 3：服务器A返回更新后的模型给各参与方；

step 4：参与方更新各自模型。

**步骤解读：**

在传统的机器学习建模中，通常是把模型训练需要的数据集合到一个数据中心然后再训练模型，之后预测。

在横向联邦学习中，可以看作是**基于样本的分布式模型训练**，分发全部数据到不同的机器，每台机器从服务器下载模型，然后利用本地数据训练模型，之后返回给服务器需要更新的参数；服务器聚合各机器上的返回的参数，更新模型，再把最新的模型反馈到每台机器。

在这个过程中，每台机器下都是**相同且完整的模型**，且机器之间不交流不依赖，在预测时每台机器也可以**独立预测**，可以把这个过程看作成基于样本的分布式模型训练。谷歌最初就是采用横向联邦的方式解决安卓手机终端用户在本地更新模型的问题的。



### 纵向联邦学习

**适用场景：**

纵向联邦学习的本质是**特征的联合**，适用于用户重叠多，特征重叠少的场景，比如同一地区的商超和银行，他们触达的用户都为该地区的居民（样本相同），但业务不同（特征不同）。

**学习过程：**

![image-20220126151806811](D:\Notes\raw_images\image-20220126151806811.png)

纵向联邦学习的本质是交叉用户在不同业态下的特征联合，比如商超A和银行B，在传统的机器学习建模过程中，需要将两部分数据集中到一个数据中心，然后再将每个用户的特征join成一条数据用来训练模型，所以就需要双方有用户交集（基于join结果建模），并有一方存在label。其学习步骤如上图所示，分为两大步：

第一步：**加密样本对齐**。是在系统级做这件事，因此在企业感知层面==不会暴露非交叉用户==。 ( 如何对齐？如何知道是同一个用户？)

第二步：对齐样本进行模型加密训练：

step 1: 由第三方C向A和B发送公钥，用来加密需要传输的数据；

step 2: A和B分别计算和自己相关的特征中间结果，并加密交互，用来求得各自梯度和损失；

step 3: A和B分别计算各自加密后的梯度并添加掩码发送给C，同时B计算加密后的损失发送给C；

step 4: C解密梯度和损失后回传给A和B，A、B去除掩码并更新模型。

**步骤解读：**我们以线性回归为例具体说明其训练过程。

存在数据集 $\left\{ x_{i}^{A} \right\},i\ \in D_{A} \left\{ x_{i}^{B} ,y_{i}^{B}\right\},i\ \in D_{B}$ ,A和B分别初始化模型参数 $\Theta_{A},\Theta_{B}$

其目标函数为：
$$
min_{\Theta_{A} ,\Theta_{B}}\sum_{i}^{}{||\Theta_{A}x_{i}^{A}+ \Theta_{B}x_{i}^{B}-y_{i}}||^{2}+\frac{\lambda}{2}\left( ||\Theta_{A}||^{2}+||\Theta_{B}||^{2}\right)
$$
令：$u_{i}^{A}=\Theta_{A}x_{i}^{A}, u_{i}^{B}=\Theta_{B}x_{i}^{B}$ ，且对原目标函数同态加密后可表示为：$[[L]]=[[\sum_{i}^{}{((u_{i}^{A}+ u_{i}^{B}-y_{i}}))^{2}+\frac{\lambda}{2}\left( ||\Theta_{A}||^{2}+||\Theta_{B}||^{2}\right)]] $, 

$[[\bullet]] $ 表示同态加密， $$[[L_{A}]]=[[\Sigma_{i}\left( u_{i}^{A}\right)^{2}+\frac{\lambda}{2}||\Theta_{A}||^{2}]] $$,     $$\left[ \left[ L_{B} \right] \right]=[[\sum_{i}^{}{((u_{i}^{B}-y_{i})^{2})+\frac{\lambda}{2}}\Theta_{B}^{2}]] $$,      $$[[L_{AB}]]=2\Sigma_{i}\left([[u_{i}^{A}]]\left( u_{i}^{B}-y_{i}\right)\right) ,$$

因此有 $[[L]] = [[L_{A}]]+[[L_{B}]]+[[L_{AB}]] $, 

同理令 $ [[d_{i}]] = [[u_{i}^{A}]]+[[u_{i}^{B}-y_{i}]]$,

梯度可表示如下：
$$
[[\frac{\delta L}{\delta \Theta_A}]] = \sum_i [[d_i]]x_i^A + [[\lambda\Theta_A]]
$$

$$
[[\frac{\delta L}{\delta \Theta_B}]] = \sum_i [[d_i]]x_i^B + [[\lambda\Theta_B]]
$$

具体训练步骤如下：

![image-20220126153455108](D:\Notes\raw_images\image-20220126153455108.png)

**在整个过程中参与方都不知道另一方的数据和特征，且训练结束后参与方只得到自己侧的模型参数，即半模型。**

**预测过程：**

由于各参与方只能得到与自己相关的模型参数，预测时需要双方协作完成，如下图所示：

<img src="D:\Notes\raw_images\image-20220126153801914.png" alt="image-20220126153801914" style="zoom:80%;" />

**共同建模的结果：**

- 双方均获得数据保护
- 共同提升模型效果
- 模型无损失



### **联邦迁移学习**

**适用场景：**

当参与者间特征和样本重叠都很少时可以考虑使用联邦迁移学习，如不同地区的银行和商超间的联合。主要适用于以深度神经网络为基模型的场景。

**迁移学习介绍：**

迁移学习，是指利用数据、任务、或模型之间的相似性，将在源领域学习过的模型，应用于目标领域的一种学习过程。

其实我们人类对于迁移学习这种能力，是与生俱来的。比如，我们如果已经会打乒乓球，就可以类比着学习打网球。再比如，我们如果已经会下中国象棋，就可以类比着下国际象棋。因为这些活动之间，往往有着极高的相似性。生活中常用的“举一反三”、“照猫画虎” 就很好地体现了迁移学习的思想。

![img](https://pic2.zhimg.com/80/v2-5dc4c9fda0acb50e93dfa78e0510d045_720w.png)

迁移学习的核心是，找到==源领域和目标领域之间的相似性==，举一个杨强教授经常举的例子来说明：我们都知道在中国大陆开车时，驾驶员坐在左边，靠马路右侧行驶。这是基本的规则。然而，如果在英国、香港等地区开车，驾驶员是坐在右边，需要靠马路左侧行驶。那么，如果我们从中国大陆到了香港，应该如何快速地适应 他们的开车方式呢？诀窍就是找到这里的不变量：不论在哪个地区，驾驶员都是紧靠马路中间。这就是我们这个开车问题中的不变量。 找到相似性 (不变量)，是进行迁移学习的核心。

**学习过程：**

联邦迁移学习的步骤与纵向联邦学习相似，只是中间传递结果不同（实际上每个模型的中间传递结果都不同）。

这里重点讲一下联邦迁移的思想：

![image-20220126154205970](D:\Notes\raw_images\image-20220126154205970.png)

源域： $ D_{A}=\left\{ (x_{i}^{A},y_{i}^{A}) \right\}_{i=1}^{N_{A}} $，目标域： $D_{B}=\left\{ (x_{j}^{B}) \right\}_{j=1}^{N_{B}} $，我们假设源域和目标域间存在共同样本 $ D_{AB}=\left\{ (x_{i}^{A},x_{i}^{B}) \right\}_{i=1}^{N_{AB}} $，对于其共同样本存在 $ D_{C}=\left\{ (x_{i}^{B},y_{i}^{A}) \right\}_{i=1}^{N_{C}} $， $ u_{A},u_{B} $ 分别为源域和目标域间的隐层特征不变量，我们定义对目标域的分类函数为：
$$
 \varphi\left( u_{j}^{B} \right) =\frac{1}{N_{A}}\sum_{i}^{N_{A}}{y_{i}^{A}u_{i}^{A}(u_{j}^{B})'}=\Phi^{A}\Omega(u_{j}^{B})
$$
目标函数：
$$
arg\min_{\Theta^{A}, \Theta^{B}}{L_{1}}=\sum_{i}^{N_{c}}{l_{1}(y_{i}^{A},\varphi\left( u_{i}^{B}\right))}
$$

$$
arg\min_{\Theta^{A}, \Theta^{B}}{L_{2}}=\sum_{i}^{N_{AB}}{l_{2}(u_{i}^{A},u_{i}^{B})}
$$

整体目标函数为： 
$$
arg\min_{\Theta^{A}, \Theta^{B}}{L}=L_{1}+\gamma L_{2}+\frac{\lambda}{2}(||\Theta^{A}||^{2}+||\Theta^{B}||^{2})
$$
使用$BP$ 算法，根据目标函数 $L$  分别对$ \Theta^{A},\Theta^{B} $求梯度，双方交互计算梯度和损失需要用到的中间结果，重复迭代直至收敛。

整个学习过程是利用A、B之间共同样本来学习两者间各自的特征不变量表示 $ u_{A},u_{B}，$同时利用 $A$ 的所有样本和  $label \  y_{A} $ 和 $A$的不变量特征 $u_{A}$ 学习分类器。在这个阶段中，$[联邦] $ 体现在 $A,B$ 可以通过安全交互中间结果共同学习一个模型，$[迁移] $体现在 $B$ 迁移了 $A$ 的分类能力。在预测时， $ u_{B}$ 依赖于由 $u_{A},y_{A} $ 组成的分类器，因此和纵向联邦相同需要两者协作来完成。

本节参考文章：Secure Federated Transfer Learning

