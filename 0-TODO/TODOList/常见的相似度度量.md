**二．相似度度量：余弦(P) 、调整余弦(P) 、皮尔逊(P) 、Jaccard(P)**

**1、余弦相似度**：Cosine Similarity

相当于计算向量夹角的余弦值，以此作为两个个体间相似度大小的衡量

![img](https://pic1.zhimg.com/80/v2-8064e828130d24ad1ddb7e51326aae60_hd.png)

![img](https://pic1.zhimg.com/80/v2-a36bea8e9b36a9076b2a22e8860b8fa0_hd.png)

![img](https://pic3.zhimg.com/80/v2-8d0a13dbf1f11d79271567f084fdcf5a_hd.png)

```python
#余弦系数计算用户相似度
#余弦相似度=sum(rxs * rys)/sqrt(rxs2 * rys2),其中s为AnB
def sim_cos(prefs,person1,person2):
	si = {}
	for it in prefs[person1]:
		if it in prefs[person2]:
			si[it] =1
	if len(si) ==0:
		re turn0
	pSum =sum([prefs[person1][it]*prefs[person2][it]forit insi])
	sum1Sq =sum([pow(prefs[person1][it],2)forit insi])
	sum2Sq =sum([pow(prefs[person2][it],2)forit insi])
	den =math.sqrt(sum1Sq *sum2Sq)
	if den ==0:
		return 0
	r =float(pSum)/den
	return r
```

**2、调整余弦相似度**：Adjusted Cosine Similarity

相当于计算向量夹角的余弦值，以此作为两个个体间相似度大小的衡量，由于余弦相似度对数值不敏感，如个体X、Y对两个条目的评分分别为(1,2)、(4,5)，其余弦相似度为0.98，调整后需要减去各自的均值，计算相似度为-0.8.通过求出每位用户的平均打分，调整评分向量为评分偏差向量，再求解余弦相似度。

![img](https://pic3.zhimg.com/80/v2-7fa56c4a3e55f18e23836689477a9686_hd.png)

![img](https://pic3.zhimg.com/80/v2-8d0a13dbf1f11d79271567f084fdcf5a_hd.png)

```python
#调整余弦系数计算用户相似度
#其中s属于AnB，i属于A，j属于B；和皮尔逊的差异在分母
def sim_ajcos(prefs,person1,person2):
	si = {}
	for it in prefs[person1]:
		if it in prefs[person2]:
			si[it] =1
	if len(si) ==0:
		return 0
	avg1 =float(sum(prefs[person1][it]forit inprefs[person1]))/len(prefs[person1])
	avg2 =float(sum(prefs[person2][it]forit inprefs[person2]))/len(prefs[person2])
	pSum =sum([(prefs[person1][it]-avg1)*(prefs[person2][it]-avg2)forit insi])
	sum1Sq =sum([pow(prefs[person1][it]-avg1,2)forit inprefs[person1]])
	sum2Sq =sum([pow(prefs[person2][it]-avg2,2)forit inprefs[person2]])
	den =math.sqrt(sum1Sq *sum2Sq)
	if den ==0:
		return 0
	r =float(pSum)/den
	return r
```



**3：皮尔逊相关系数。**

即为相关分析中的相关系数r，分别对X和Y基于自身总体标准化后计算两个向量夹角的余弦值。相当于对数据进行中心化处理，即数据移动了一个样本平均值以使其均值为零。
。cov(x,y)表示协方差，E(x)和μx表示期望，σ表示标准差。第二个是相对于样本，代码中即是样本公式：，最后一个公式是代码中的公式。



```python
#皮尔逊相关系数计算用户相似度
#通用的方法，下面一种只是本方法在某些情况下的特例

def sim_pearson(prefs,p1,p2):
	si={}
	for item in prefs[p1]: 
		if item in prefs[p2]: si[item]=1
	if len(si)==0: return 0
    
	n=len(si)
		
		#计算开始
	sum1=sum([prefs[p1][it] for it in si])
	sum2=sum([prefs[p2][it] for it in si])
    
	sum1Sq=sum([pow(prefs[p1][it],2) for it in si])
	sum2Sq=sum([pow(prefs[p2][it],2) for it in si])   
    
	pSum=sum([prefs[p1][it]*prefs[p2][it] for it in si])
    
	num=pSum-(sum1*sum2/n)
	den=math.sqrt((sum1Sq-pow(sum1,2)/n)*(sum2Sq-pow(sum2,2)/n))
    #计算结束 
	if den==0: return 0
    
	r=num/den
    
	return r
#皮尔逊相关系数计算用户相似度
#通用的方法，下面一种只是本方法在某些情况下的特例
def sim_stdpearson(prefs,person1,person2):
	si = {}
	for it in prefs[person1]:
		if it in prefs[person2]:
			si[it] = 1
	lensi = len(si)
	if lensi == 0: return 0
	sum1 = sum(prefs[person1][it] for it in prefs[person1])
	sum2 = sum(prefs[person2][it] for it in prefs[person2])
	avg1 = float(sum1)/len(prefs[person1])
	avg2 = float(sum2)/len(prefs[person2])
	sum1Sq = sum([pow(prefs[person1][it]-avg1,2) for it in si])
	sum2Sq = sum([pow(prefs[person2][it]-avg2,2) for it in si])
	pSum = sum([(prefs[person1][it]-avg1)*(prefs[person2][it]-avg2) for it in si])
	den = math.sqrt(sum1Sq*sum2Sq)
	if den == 0: return 0
	r = pSum/den
	return r
```



4、Jaccard相似度（狭义）。还存在一个广义Jaccard相似度，狭义Jaccard相似度在某些方面并不大适用，因为它只能判断两者中的元素是否一致，拿上例中的电影例子来说就是，小明对有且只有对三个电影做出了评价，同样小红也是有且只有对三个电影做出了评价，注意是做出了评价，而不是做出了什么评价，这个时候他们的相似度就是1，也就是相同。公式：

```python
#Jaccard计算用户相似度
#Jaccard=|AnB|/|AuB|
def sim_jaccard(prefs,person1,person2):
	si_union = {}#并集
	si_inter = {}#交集
	si_union =dict(prefs[person1],**prefs[person2])
	for	it in prefs[person1]:
		if it in prefs[person2]:
			si_inter[it] =min(prefs[person1][it],prefs[person2][it])
	sum1 =sum(si_inter[it]for it in si_inter)
	sum2 =sum(si_union[it]for it in si_union)
	if sum2 ==0:
		return 0
	r =float(sum1)/sum2
	return r
```



余弦、调整余弦、皮尔逊相似度均适用于：基于用户相似度的CF和基于产品相似度的CF；

区别在于：前者是基于评分矩阵中的行向量相似度求解，而后者是基于评分矩阵中的列向量相似度求解。

数据稀疏问题

数据稀疏的一种解决方法：填充

## **常见距离与相似度度量**

欧氏距离

定义在两个向量（两个点）上：点![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D)和点![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7By%7D)的欧氏距离为：

![[公式]](https://www.zhihu.com/equation?tex=d_%7BEuclidean%7D%3D%5Csqrt%7B%28%5Cmathbf%7Bx%7D-%5Cmathbf%7By%7D%29%5E%5Ctop+%28%5Cmathbf%7Bx%7D-%5Cmathbf%7By%7D%29%7D)

闵可夫斯基距离

Minkowski distance， 两个向量（点）的![[公式]](https://www.zhihu.com/equation?tex=p)阶距离：

![[公式]](https://www.zhihu.com/equation?tex=d_%7BMinkowski%7D%3D%28%7C%5Cmathbf%7Bx%7D-%5Cmathbf%7By%7D%7C%5Ep%29%5E%7B1%2Fp%7D)

当![[公式]](https://www.zhihu.com/equation?tex=p%3D1)时就是曼哈顿距离，当![[公式]](https://www.zhihu.com/equation?tex=p%3D2)时就是欧氏距离。

马氏距离

定义在两个向量（两个点）上，这两个点在同一个分布里。点![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D)和点![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7By%7D)的马氏距离为：

![[公式]](https://www.zhihu.com/equation?tex=d_%7BMahalanobis%7D%3D%5Csqrt%7B%28%5Cmathbf%7Bx%7D-%5Cmathbf%7By%7D%29%5E%5Ctop+%5CSigma%5E%7B-1%7D+%28%5Cmathbf%7Bx%7D-%5Cmathbf%7By%7D%29%7D)

其中，![[公式]](https://www.zhihu.com/equation?tex=%5CSigma)是这个分布的协方差。

当![[公式]](https://www.zhihu.com/equation?tex=%5CSigma%3D%5Cmathbf%7BI%7D)时，马氏距离退化为欧氏距离。

互信息

定义在两个概率分布![[公式]](https://www.zhihu.com/equation?tex=X%2CY)上，![[公式]](https://www.zhihu.com/equation?tex=x+%5Cin+X%2C+y+%5Cin+Y).它们的互信息为：

![[公式]](https://www.zhihu.com/equation?tex=I%28X%3BY%29%3D%5Csum_%7Bx+%5Cin+X%7D+%5Csum_%7By+%5Cin+Y%7D+p%28x%2Cy%29+%5Clog+%5Cfrac%7Bp%28x%2Cy%29%7D%7Bp%28x%29p%28y%29%7D)

余弦相似度

衡量两个向量的相关性（夹角的余弦）。向量![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D%2C%5Cmathbf%7By%7D)的余弦相似度为：

![[公式]](https://www.zhihu.com/equation?tex=%5Ccos+%28%5Cmathbf%7Bx%7D%2C%5Cmathbf%7By%7D%29+%3D+%5Cfrac%7B%5Cmathbf%7Bx%7D+%5Ccdot+%5Cmathbf%7By%7D%7D%7B%7C%5Cmathbf%7Bx%7D%7C%5Ccdot+%7C%5Cmathbf%7By%7D%7C%7D)



理解：向量的内积除以向量的数量积。

皮尔逊相关系数

衡量两个随机变量的相关性。随机变量![[公式]](https://www.zhihu.com/equation?tex=X%2CY)的Pearson相关系数为：

![[公式]](https://www.zhihu.com/equation?tex=%5Crho_%7BX%2CY%7D%3D%5Cfrac%7BCov%28X%2CY%29%7D%7B%5Csigma_X+%5Csigma_Y%7D)

理解：协方差矩阵除以标准差之积。

范围：[-1,1]，绝对值越大表示（正/负）相关性越大。

Jaccard相关系数

对两个集合![[公式]](https://www.zhihu.com/equation?tex=X%2CY)，判断他们的相关性，借用集合的手段：

![[公式]](https://www.zhihu.com/equation?tex=J%3D%5Cfrac%7BX+%5Ccap+Y%7D%7BX+%5Ccup+Y%7D)

理解：两个集合的交集除以并集。

扩展：Jaccard距离=1-J。



## **概率分布的距离度量**

KL散度

Kullback–Leibler divergence，相对熵，衡量两个概率分布![[公式]](https://www.zhihu.com/equation?tex=P%28x%29%2CQ%28x%29)的距离：

![[公式]](https://www.zhihu.com/equation?tex=D_%7BKL%7D%28P%7C%7CQ%29%3D%5Csum_%7Bi%3D1%7D+P%28x%29+%5Clog+%5Cfrac%7BP%28x%29%7D%7BQ%28x%29%7D)



这是一个非对称距离：![[公式]](https://www.zhihu.com/equation?tex=D_%7BKL%7D%28P%7C%7CQ%29+%5Cne+D_%7BKL%7D%28Q%7C%7CP%29).



JS距离

Jensen–Shannon divergence，基于KL散度发展而来，是对称度量：

![[公式]](https://www.zhihu.com/equation?tex=JSD%28P%7C%7CQ%29%3D+%5Cfrac%7B1%7D%7B2%7D+D_%7BKL%7D%28P%7C%7CM%29+%2B+%5Cfrac%7B1%7D%7B2%7D+D_%7BKL%7D%28Q%7C%7CM%29)



其中![[公式]](https://www.zhihu.com/equation?tex=M%3D%5Cfrac%7B1%7D%7B2%7D%28P%2BQ%29)。

MMD距离

Maximum mean discrepancy，度量在再生希尔伯特空间中两个分布的距离，是一种核学习方法。两个随机变量的距离为：

![[公式]](https://www.zhihu.com/equation?tex=MMD%28X%2CY%29%3D%5Cleft+%5CVert+%5Csum_%7Bi%3D1%7D%5E%7Bn_1%7D%5Cphi%28%5Cmathbf%7Bx%7D_i%29-+%5Csum_%7Bj%3D1%7D%5E%7Bn_2%7D%5Cphi%28%5Cmathbf%7By%7D_j%29+%5Cright+%5CVert%5E2_%5Cmathcal%7BH%7D)

其中![[公式]](https://www.zhihu.com/equation?tex=%5Cphi%28%5Ccdot%29)是映射，用于把原变量映射到高维空间中。

理解：就是求两堆数据在高维空间中的均值的距离。

Principal angle

也是将两个分布映射到高维空间（格拉斯曼流形）中，在流形中两堆数据就可以看成两个点。Principal angle是求这两堆数据的对应维度的夹角之和。对于两个矩阵![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D%2C%5Cmathbf%7BY%7D)，计算方法：首先正交化（用PCA）两个矩阵，然后：

![[公式]](https://www.zhihu.com/equation?tex=PA%28%5Cmathbf%7BX%7D%2C%5Cmathbf%7BY%7D%29%3D%5Csum_%7Bi%3D1%7D%5E%7B%5Cmin%28m%2Cn%29%7D+%5Csin+%5Ctheta_i)

其中![[公式]](https://www.zhihu.com/equation?tex=m%2Cn)分别是两个矩阵的维度，![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_i)是两个矩阵第$i$个维度的夹角，![[公式]](https://www.zhihu.com/equation?tex=%5CTheta%3D%5C%7B%5Ctheta_1%2C%5Ctheta_2%2C%5Ccdots%2C%5Ctheta_t%5C%7D)是两个矩阵SVD后的角度：

![[公式]](https://www.zhihu.com/equation?tex=%5Cmathbf%7BX%7D%5E%5Ctop%5Cmathbf%7BY%7D%3D%5Cmathbf%7BU%7D+%28%5Ccos+%5CTheta%29+%5Cmathbf%7BV%7D%5E%5Ctop)



HSIC

希尔伯特-施密特独立性系数，Hilbert-Schmidt Independence Criterion，用来检验两组数据的独立性：

![[公式]](https://www.zhihu.com/equation?tex=HSIC%28X%2CY%29+%3D+trace%28HXHY%29)

其中![[公式]](https://www.zhihu.com/equation?tex=X%2CY)是两堆数据的kernel形式。

Earth Mover’s Distance

推土机距离，度量两个分布之间的距离，又叫Wasserstein distance。以最优运输的观点来看，就是分布![[公式]](https://www.zhihu.com/equation?tex=X)能够变换成分布![[公式]](https://www.zhihu.com/equation?tex=Y)所需要的最小代价：

一个二分图上的流问题，最小代价就是最小流，用匈牙利算法可以解决。

![[公式]](https://www.zhihu.com/equation?tex=emd%28X%2CY%29%3D%5Cmin%7B%5Cfrac%7B%5Csum_%7Bi%2Cj%7Df_%7Bij%7Dd%28%5Ctextbf%7Bx%7D_i%2C%5Ctextbf%7By%7D_j%29%7D%7B%5Csum_%7Bj%7Dw_%7Byj%7D%7D%7D%2C+s.t.+%5Csum_%7Bi%7Df_%7Bij%7D%3Dw_%7Byj%7D%2C+%5Csum_%7Bj%7Df_%7Bij%7D%3Dw_%7Bxi%7D.)



## 代码

可用的代码我找了一个比较全的，用matlab写的，特别简单，直接可以用，输入是两个矩阵，输出是各种距离。地址在这里：[shicai/matlab](https://link.zhihu.com/?target=https%3A//github.com/shicai/matlab/blob/master/sc_pdist2.m)



**References**

[1] [距离和相似度度量方法 - 皮皮blog - 博客频道 - CSDN.NET](https://link.zhihu.com/?target=http%3A//blog.csdn.net/pipisorry/article/details/45651315)

[2] [Earth Mover’s Distance -- 推土机距离](https://link.zhihu.com/?target=http%3A//chaofan.io/archives/earth-movers-distance-%E6%8E%A8%E5%9C%9F%E6%9C%BA%E8%B7%9D%E7%A6%BB)



[作者简介]王晋东(不在家)，中国科学院计算技术研究所博士生，目前研究方向为机器学习、迁移学习、人工智能等。作者联系方式：微博@[秦汉日记](https://link.zhihu.com/?target=http%3A//www.weibo.com/wjdbr/profile%3Frightmod%3D1%26wvr%3D6%26mod%3Dpersoninfo%26is_all%3D1) ，个人网站[Jindong Wang is Here](https://link.zhihu.com/?target=http%3A//jd92.wang)。