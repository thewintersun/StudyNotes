## Analyzing and Improving the Image Quality of StyleGAN

论文地址：https://arxiv.org/abs/1912.04958

作者：Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila

机构：NVIDIA

代码地址：https://github.com/NVlabs/stylegan2



### 摘要

The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven（数据驱动） unconditional (非条件的）generative image modeling （生成图像模型）. ==We expose and analyze several of its characteristic artifacts（伪影）, and propose changes in both model architecture and training methods to address them==. 

In particular, we ==redesign generator normalization==, ==revisit progressive growing==, and ==regularize the generator to encourage good conditioning in the mapping from latent vectors to images==. In addition to improving image quality, this ==path length regularizer== yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably detect if an image is generated by a particular network. 

We furthermore visualize how well the generator utilizes its output resolution, and ==identify a capacity problem==, motivating us to train larger models for additional quality improvements. 

Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived（感知到的） image quality.



### 介绍

很多人发现 StyleGAN 生成的图像会有很有特点的伪影。这篇论文给出了产生这些伪影的两个原因，并描述了可以如何通过修改架构和训练方法来消除这些伪影。

首先，我们调研了 blob-like artifacts 的原由，生成器制造了它们来规避架构中的一个设计缺陷——我们重新设计了生成器中的 normalization 来移除这种伪影。

其次，我们分析了解到，artifacts 还和 progressive growing 有关系，我们用新的方法代替了 progressive growing， 训练开始于低分辨率图像, 然后逐渐转移专注到更高的分辨率——而不是在训练过程中改变网络的拓扑结构。

This new design also allows us to reason about the effective resolution of the generated images,
which turns out to be lower than expected, ==motivating a capacity increase== (Section 4).

FID 和 P&R 都是基于分类网络的评测方法，最近被证明更注重于纹理而不是shapes, 因而这些metrics并不能全面地评估图像的质量。 我们观察到 ==perceptual path length (PPL)== metric [24], introduced as a method for estimating the quality of latent space interpolations, correlates with consistency and stability of shapes.
在此基础上，我们将合成网络正则化，以支持平滑映射(第3节)，并获得明显的质量改进。为了抵消它的计算开销，我们还建议减少执行所有正则化的频率，我们注意到这样做不会影响效率。

### Normalization Artifacts

![1580716551850](D:\Notes\raw_images\1580716551850.png)

Figure 1. Instance normalization causes ==water droplet -like artifacts== in StyleGAN images. 

这个问题不是只在最后生成的图像上才有，而是在生成的过程中每一层都有，从64*64像素开始。这是一个系统问题污染所有StyleGAN生成的图片。 我们把问题定位到AdaIN操作上, AdaIN分别将每个特征图的均值和方差进行标准化, thereby potentially destroying any information found in the magnitudes（梯度） of the features relative to each other. 

这个假设作者是通过实验证明的，作者去掉AdaIN层后，这种水滴似的伪影就消失了。

![1580781267293](D:\Notes\raw_images\1580781267293.png)

(a) 原始的StyleGAN, 其中A表示从W中学习仿射变换产生一种样式，B表示噪声broadcast操作。

(b) 同样的结构，将AdaIN分解为显式的归一化（normalization）,然后是调制（modulation）,它们都是在每个特征映射的均值和标准偏差上运行的。我们还介绍了学习权重(w)、偏差(b)和常数输入(c),并重新绘制灰色框 - 表示一种活跃的样式。增加偏差后总是应用激活函数(leaky ReLU)。

修改：

有趣的是,==最初的StyleGAN是在样式块内施加偏差和噪声,导致它们相对的影响与当前样式的大小成反比==。我们观察到,通过将这些操作移动到样式块之外,从而获得更可预测的结果,在此块中,它们在规范化数据上运行。此外,我们注意到,在此之后,==规范化和调制足以操作标准偏差 (即mean操作是不需要的）==。==对恒定输入的偏差、噪声和归一化的应用也可以安全地删除,没有明显的缺点==。![1580780463418](D:\Notes\raw_images\1580780463418.png)

(c) 我们对原始架构进行了一些更改。在开始时,我们删除了一些多余的操作, 将b和B的添加移动到样式活动区域之外，并只调整每个特征映射的标准偏差。

(d)修订后的架构使用“demodulation”操作代替Instance Normalization操作，应用在卷积层的权重上。

![1580782131325](D:\Notes\raw_images\1580782131325.png)

![1580782095764](D:\Notes\raw_images\1580782095764.png)