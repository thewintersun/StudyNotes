## Graph Embedding 及其在知乎的实践

文章来源：https://mp.weixin.qq.com/s/OKk5qVNRxRTkz_p4ti0xWQ

时间：2019-11-28

在CSDN 主办的 2019 中国 AI 开发者大会的机器学习专场中，知乎算法团队负责人孙付伟在机器学习专场中做了关于图嵌入及其在知乎的实践的知识分享。

Graph Embedding 技术是一种将图的拓扑结构进行向量表示的方法，从而获取到网络关系信息，应用于推荐等多种场景。

内容包括三个部分：

- **Graph Embedding 的简单介绍**

  ​     DeepWalk

​            微软 LINE

​            Node2Vec

​            SDNE

​            EGES

- **Graph Embedding 在知乎的应用**

​            用户的表示

​            知乎收藏夹数据挖掘

- **分享总结和未来规划**



### **业务背景**

这些年来深度学习发展速度非常快，但一直存在一个问题，现实世界中很多数据是以图的形式存在的，比如==社交关系、搜索行为、购买行为等，它们形成的都是一种图的表示关系。==但是传统的 Embedding，包括在自然当中图关系的挖掘，其实做的远远不够。

知乎现在已经是一个综合性的问答社区，截止到去年 11 月份，用户数超过 2.2 亿，累计超过 3000 万的问题提出，并由此带来 1.3 亿的优质回答。除此之外还有大量的文章、盐选专栏、电子书等等其他多种产品形态。在这样一个大的问答社区当中，本质上来说，其实一个问答社区是一种连接关系，万物都是一个连接关系。

在问答社区里面包含了三大块：**包括人和人之间的连接，人与内容之间的连接以及内容与内容的连接。**比如说在知乎的搜索和推荐当中，就存在着大量的连接关系，我们会根据用户的关注页关系来推荐内容。还有就是内容自身的连接，在知乎有一个特殊的功能“收藏夹”，并且这个收藏夹是公开的，大概有几千万个，意味着其他用户是可以看到别人的收藏。这就会形成内容和内容之间的连接。在知乎这样一个万物都是连接的问答社区当中，如何有效表征这种连接关系，是我们这几年一直在思考一个点。

![img](https://mmbiz.qpic.cn/mmbiz_jpg/8VOiack4f29eLg7kuQJzmicTV3rtP4Jiboonib0flNyN5OkqVaqUkzJXrws3n8mptbe74LlEE9neYTNH5GjlWWcDsQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

图二是以知乎站内一些数据做的连接关系的展现，其实一个大的连接点，会形成千千万万的连接。==用户关系挖掘会有助于社群关系的连接提升，也有助于用户群体的发现。==

**Graph Embedding**

既然是图关系，我们为什么要用到 Graph Embedding 呢？在面对图结构的时候，传统的序列 Embedding 经常不知道该怎么去处理。图这种连接关系是没有序列的表示，怎么去做这件事？为了解决这样一个问题，从 2014 年一直到现在，Graph Embedding 变成了业内一个全新的研究方向。

**Deep Walk**

首先我们看第一个，2014 年 Deep Walk 提出可以算作 Graph Embedding 迈出来的第一步工作。这篇文章的思想相对来说还是比较简单的，==它是借助了 Word2Vec 之前的序列表示，随机游走方式，将图以序列方式表征==。

![img](https://mmbiz.qpic.cn/mmbiz_png/8VOiack4f29eLg7kuQJzmicTV3rtP4Jiboo2GVEwC7sdT4GnUVibN2E75wdoZw6sc2MgrIkdcHX3FPE9XCOotC5KoA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

举一个例子，上图更容易跟现实结合起来，最左侧是用户购买，或者贡献购买之间的关系。根据贡献购买之间的关系之后，走一步 random walk 生成，形成商品关系之间的序列，形成序列之后，就可以借助于 Graph Embedding 思想进行表示。可以说，它已经打开了 Graph Embedding 这扇门，让大家了解其实图也是一个我们需要去深入连接的点。

**LINE**

2014 年 Deep Walk 打开这扇门之后，2015 年微软发表了 LINE 这篇文章，这篇文章有两个核心的概念需要说明一下，它明确定义了图当中的相似度如何表征。

![img](https://mmbiz.qpic.cn/mmbiz_png/8VOiack4f29eLg7kuQJzmicTV3rtP4JibooL9eSq3NURurIRsetK7iaYhThOJvRr0icLqeqo2ZOrSaxSZoPvp78icBOA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这里有两个概念，==一个是1阶相似度，节点和节点之间存在直接的连接，叫1阶相似度。==比如图四右侧的节点 6 和 节点 7 以及节点7 和节点8，这种连接关系都是 1 阶连接，当它存在边，它的边就是它的相似度，如果不存在边，它的 1 阶相似度就是 0。

==2 阶相似度主要说的是两个节点之间并不存在直接连接关系，但是它的邻居是大量重合的，主要衡量的是两个节点的网络结构相似的程度。==比如图四中节点 5 和节点 6 的1 阶相似度是 0，2 阶相似度可以计算出一个值。通过这种方式就可以把图的关系刻画出来。==在现实业务的数据挖掘中除了用到节点的 1 阶关系，还要用到它的 2 阶关系，通过 Embedding 的方式得到网络结构的表征，从而应用于推荐这些场景。==

讲完概念之后我们再来看 1 阶相似度和 2 阶相似度如何定义问题的。

机器学习里有一个叫优化目标。==优化目标主要预测的是交叉熵的优化目标，这只是表征了正样本==。现实还存在负样本，对负样本采用负采样的方式，把优化目标做一个改造，形成这样一个表征，这是 1阶相似度。

2 阶相似度，优化目标和上面是类似的。这是基于上面这种关系我们把它的优化目标做一个简单说明，这篇文章里面做了三点优化技巧。

> 首先，1 阶和 2 阶分开训练的，因为它们都有自己的优化目标，这是这篇文章里面特殊的技巧——分开训练，在一些应用场景里面再 concat 到一起做预测。
>
> 其次是$w_{ij}$ ，权重范围相当大，所以去做预测推荐的时候学习率是很难控制的，文章采用了一个技术方案，边采样（EdgeSample），从而降低这个问题的影响。
>
> 在现实中经常会遇到冷启动问题，当一个节点边较少的时候，怎么去处理。这篇文章考虑了节点冷启动问题，它用的一个手段是，往外扩充，用它的 2阶邻居，它的 3 阶邻居节点来处理 2 阶相似度，放在 2 阶相似度上来表征。

**Node2Vec**

2016 年有两篇文章，其中一篇是斯坦福的 Node2Vec 想必大家也很熟悉，这篇文章在 Deep Walk 的扩展，我们可以想一下 random walk 整个流程，类似于深度优先搜索的挖掘路径，==这篇文章和 Deep Walk 唯一差别是，它既考虑了 DFS，又考虑了 BFS，可以把结构信息学得更全面一些==。

![img](https://mmbiz.qpic.cn/mmbiz_png/8VOiack4f29eLg7kuQJzmicTV3rtP4Jiboo5yWicia04XicKuDl6SWmXLxzOVQ9ScZ3DDbr6wEfhZPwVpclTkTE211cg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

考虑了同质性和结构性，==同质性就是它们形成的序列越接近，Embedding 表征越相似==，比如图五中 U 和 *S1*、*S2* 表征越接近越好。==结构性就是两个结构越接近，学出来的 Embedding 表征越相似==。如果是 BFS，我倾向于U 和 *S6* 更接近一些。如果是 DFS，相近的这些 Embedding 表征更相似。所以，这篇文章是做了结构性和同质性的某种中和，但是怎么来调节它们俩之间的权重，怎么来控制，其实用了一个跳转概率，来控制 BFS 和 DFS 的倾向性。

![img](https://mmbiz.qpic.cn/mmbiz_png/8VOiack4f29eLg7kuQJzmicTV3rtP4JibooCNU0Dnaaq7I2SenARhpIcUIVzsfVIFaDzpNGOiaBzjTf1ZWasu4SXgw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

以图六为例，当一个 t 节点进入到下一个节点 V 的时候，到 V 这个节点之后，下一步应该朝哪个节点走呢？==这里定义两个参数，分别是 p 和 q，通过这两个参数来控制 V 朝 *X2*、*X3* 随机走，还是返回。从 V 到 t 的跳转概率越大就偏重于 DFS，越小就偏重于 BFS。==

![img](https://mmbiz.qpic.cn/mmbiz_png/8VOiack4f29eLg7kuQJzmicTV3rtP4JibooMHmGhfAedLFsMtWoyWf03NTdtygyTSl5l9BXqJfqQXRON2KK2rCicWw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这篇文章把 p 和 q 做了一系列参数实验。==当越靠近 DFS 的时候，学出来的是簇的信息，学 Embedding 表征越来越相近。如果是靠近 BFS，节点连接的关系信息学的会更相近一些==。

**SDNE**

第二篇文章，SDNE 这篇文章，这两篇文章都发表在 KDD 2016 年的两篇文章，上面那篇文章是在 Deep Walk 基础上做扩展，这篇文章是在 LINE 基础上做扩张。==图的网络结构信息分成 local 和 global ，local 是 LINE 里面 1 阶关系，直接相连关系，global 说的是邻居之间跟它的连接关系，这是它的一个对应==。

![img](https://mmbiz.qpic.cn/mmbiz_png/8VOiack4f29eLg7kuQJzmicTV3rtP4JiboolLmeXTTibsYYgiapElvIoMI1pjibBFhWLiaDnJ1aD1FialXqWbZbCicpKj7g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这篇文章又用了另外一种思路，==非监督的 autoencoder 方式，来得到 Embedding 表示==，我们先来看图八，global Embedding 的图结构。它的 Embedding 表示什么呢？和节点 i 相连的节点权重的表征，autoencoder 之后会形成新的表征，去衡量它原始的表征和新的表征之间的相似度，就是优化的点。xi用邻居的信息来表征它自身特征的，其实是 global 的概念。(原文当中的图是错的，这个地方是 local，这个是global，其实我们反过来看，应该是画错了。)

针对 j 节点也一样，也会学出来这个表征，最终 i这个节点的表示是中间这一层，我们是要去度量中间这两个节点的相似度，来去表征。这篇文章可以说是用了更复杂的 autoencoder 的思想，在 LINE 的基础上做了扩展。

![img](https://mmbiz.qpic.cn/mmbiz_png/8VOiack4f29eLg7kuQJzmicTV3rtP4Jiboo1iccs0sC9418fBceIB45ibvmxO7bmracSbN9IJaWQOpUjcFXuOj0E5Uw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

​																			图九 阿里凑单算法结构

这篇文章其实也有应用，在阿里凑单系统里面，这是他们开放的一个博客里面有所提到，凑单系统里面做这件事，怎么做呢？拿到用户同时购买的内容序列，形成这样一个图的表示。在这个图的表示里面去用加权的 walk 做采样，为什么做采样？这个表征是说它与它相连的那些节点之间的权重，当数据量规模像淘宝这么大的情况下，连接关系是特别多的。先对样本做采样，采样的方式是对热门数据做加权，它想表征出来让热门数据推荐的更充分一些，然后做了这样一个加权，这个里面用的结构其实就是 SDNE。不过它在SDNE 里边加了这样一个全连接，能跟它的现有系统做一个比较，这是它的一个变化。

它在双11 预热期间，在点击和多样性这两个指标上，据公开表述说，点击率提升了 13%，它做成实时化之后，在这个基础上又提升了 4%。它们内部多样性指标直接翻了接近 1 倍。

**EGES**

在 2018 年阿里又写了一篇文章，主要用在解决图表征冷启动的一些问题，有些节点信息较少，我们怎么去处理。EGES 是在 2018 年 KDD 上发表的，==核心点主要是在 Deep Walk 基础之上加入了对冷启动商品的处理，加入的方式叫 side information，两个商品之间尽管没有共现的关系，尽管共现关系特别少，但是还有其他额外的知识信息，知识图的信息是可以用起来的==。

比如说属于同一个店铺，属于同一个品类，属于同一类价格区间等等，大概十几类 side information，把这个信息变成了它在图当中利用特殊的相似度链接，把它拼接起来，形成一个新的更完备的图。后面的工作就比较简单了，形成这个链接之后，不同的连接关系，通过 side information 信息学的某种表征，把向上连接作为不同权重来去学习它。

![img](https://mmbiz.qpic.cn/mmbiz_png/8VOiack4f29eLg7kuQJzmicTV3rtP4JibooOZXZXm8iagMIpicgT7JRtNRoZkcgibx4z6qlk9I3l1dSibhG9y0HQpZ7XA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这篇文章思路是比较简单的，但是它具有很强的实操性，在业内已经有几个公司都在用这篇思路来做 Embedding 召回，一会儿我们就讲讲在知乎上的效果。

### **知乎的实践**

上面简单说了一下 Graph Embedding 这些年主要的技术点，下面主要讲一讲 Graph Embedding 在知乎的两个实践，现在 Graph Embedding 在知乎应该有三四个分别的应用，我先拿这两个，因为这两个和上面的文章是一一对应的。

**用户 Embedding 表示**

第一个，关于用户 Embedding 表示，这个是我们在 2017 年的时候做的一个尝试，我们在知乎大概有几千万的存在大量连接关系的用户，为了强化他们在社区上的互相影响，并且让他们形成一种社区的概念，做用户的推荐，或者说用户群体的挖掘，变成我们不得不考虑的一件事。当时考虑一个点，用 Graph Embedding 思路，我们去学一下 Embedding，看看它的效果。==这里面的思想其实比较简单，就是纯的 Deep Walk，通过筛选，大概筛选出 2400 万用户群体，根据它的关系挖掘，形成这样的关系链路，形成关系序列，最后就是用 Deep Walk 思想做了预测。==

![img](https://mmbiz.qpic.cn/mmbiz_jpg/8VOiack4f29eLg7kuQJzmicTV3rtP4JiboosV6wRVJw3icLUI2gibCn1v1tA1xF9DBsSpfN8cUA5rXHY6zAUeQlIlXQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

上图是我们跑完之后的效果图，做机器学习和深度学习的会比较了解这些同学。

**收藏夹数据挖掘**

上面的思想比较简单，2017年做的。==今年上半年我又做了第二件事，EGES，阿里2018年的那篇文章，知乎有几千万收藏夹的数据，用户自己编辑的收藏夹。==大家可能也会关注一些文章，比如常用什么是梯度下降法，机器学习最好的课程是什么等一系列文章，形成了收藏夹。还有成长相关的文章，比如自律，提升自我竞争力等等。当然，这些数据全是用户自己对外公开的数据，因为收藏夹数据要么是自己看的收藏夹，要么是公开的收藏夹，我们用的数据都是公开的，用户愿意分享的收藏夹数据。

这个收藏夹数据包含了大量有价值的内容，2018年的时候，我们都在用一些协同过滤思想来做这件事，挖掘它们之间的关系。当时遇到两个问题没太好解决。

> 第一个问题，长尾的内容不太好表示。
>
> 第二个问题，默认了用户的这个序列，在我们协同过滤里边浏览的序列，但是在实际当中，它是没有先后关系的。所以，在矩阵协同分解的时候，我们认为它无法表达更强的传播能力，让它学的多样性不够，总体效果也可能会有些损失，等一会儿我们来看看是不是这样。

其实思想还是借助于业内他们的思路，他们是基于共同购买，我们是基于收藏夹的内容，做一些筛选，构建Graph，把内容作为节点，而不是把收藏夹变成其中一个节点，这个其实是有一个考虑的。大家可以想想为什么？我们的内容可能和收藏夹是不成比例的，我们收藏的内容，如果把它变成一个节点，它学出来的会有些问题。当然，这是我们踩的一些坑，后面挖掘出来的东西。如果它贡献了，我们就认为它存在一条边。==在收藏夹数据里边，我们跟淘宝可能不太一样的地方是，我们更期望的是推荐出来它的真正 Embedding 相似，而不是推荐一些热门的==。所以，在这里我们会对热门内容降采样，并且考虑做序列挖掘的时候，考虑了BFS挖掘信息。我们还引入了自身的 side information 信息，例如topic，两篇文章同一作者等等这样的信息，加入到我们的 Embedding 学习当中。

我们的训练集有110万节点，构建了 3 亿条边，Random walk 和 EGES 差不多，每个节点会进行10 次 walk，最终效果是下面这个结果。

![img](https://mmbiz.qpic.cn/mmbiz_png/8VOiack4f29eLg7kuQJzmicTV3rtP4JibooIumdEwtjvQ45A9KiajVYJhCuTVnzXo3dlNV3vcTKUQYIYpfVZ9cibU2Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

本来还有它作为 Embedding 召回在线上应用的东西，但是那些东西不太适合在这个场景去说，所以我就没有把那些东西添上。我们先以一个事例说明效果。同样的一个问答，说为什么白居易写了《琵琶行》却不娶琵琶女，我们可以看用协同过滤推出来的效果，右侧是我们基于EGES推出来的。基本上是偏重于诗词，还有一些是雅称，还有一些是关于鲁迅文学相关的，大概是这样一个推荐的效果，这是在召回上。我们现在这个东西会用在好几个推荐场景，比方说个性化推送，比方说做内容回答的推荐等等，还有问题的推荐。这是两个在知乎的具体实践。



### **总结**

==我们认为 Graph Embedding 在这些年已经成为推荐系统、计算广告等领域一种新的流行做法，并且Graph这种图的表示还在如火如荼的开展当中，并且我认为它的影响会持续放大。==我们现在跟着业内最前沿的脚步往前走，看看如何更好地把最新的技术应用到知乎具体的算法场景当中。

其次，关于未来规划，我们会涉及到两点，==一是会员购买预测，==知乎现在的会员业务中涉及到像淘宝一样购买场景，但这里的一个问题是它的数据更稀疏，如何在稀疏的数据当中学习 Embedding 表示，是我们要考虑的一个点。==第二个是个性化推送的召回，目前已经上线==。关于更多的技术尝试，我们在用户画像当中只用到了 Embedding 挖掘，==但用户的持续兴趣、关注兴趣、浏览文章等等，有大量信息是可以去用的，但是这个信息我们现在还没有做进一步的尝试，这将是我们下一步要尝试的点==。最后，根据 SDNE 那篇文章，我们认为它那个表征会更自然一些，现在想法是会把side information 信息与 SDNE 结合，看看会不会有别样的效果提升。



**参考文献**

https://arxiv.org/pdf/1503.03578.pdf

https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf

https://arxiv.org/pdf/1803.02349.pdf

https://cs.stanford.edu/~jure/pubs/node2vec-kdd16.pdf

