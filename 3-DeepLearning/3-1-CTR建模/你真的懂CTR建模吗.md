## 你真的懂点击率（CTR）建模吗？

文章地址：https://cloud.tencent.com/developer/article/1842740

点击率(CTR，Click-Through Rate)以及派生的各种用户行为概率 ( 如商品购买率、推荐好友接受率、短视频3秒曝光率等) 是广告、推荐、搜索等互联网应用中大家耳熟能详的词汇。以点击率为例，如何建立高效的CTR预估模型是领域从业者们的核心能力，也是头部企业长期重兵投入、持续优化的核心技术。

近些年来，得益于深度学习带来的巨大红利，CTR预估建模技术发展迅速，顶会论文、技术博客等都有大量精妙而深刻的介绍，加之开源浪潮带来的工具普及，这项技术的入门似乎变得极其容易：跑跑已经发表甚至开源的论文模型，调一调结构，或根据自己的业务特点做一些微创新，总是能取得不错的结果。

然而，隐藏在这些类似制造业的熟练工序背后，有很多基础性的、细思极恐的知识点，若缺失它们虽不至导致熟练工的技能立刻坍塌，但却总是让技术的大厦存在不小的裂缝。另一方面，产品、运营等角色往往对CTR模型的理解停留在似是而非的直观层面，也有必要做个严肃的普及。

今天我们不讲复杂的建模技术，仅来聊聊CTR模型背后的一些小知识。抛几个问题开开胃：

- CTR的物理意义是什么？用户的点击概率可以被准确预测吗？
- CTR模型为什么普遍采用二分类建模而不是回归建模？
- 训练集中竟然存在矛盾样本，传统machine learning任务似乎闻所未闻，如何理解？
- 为什么采用AUC指标来度量模型性能，而不是传统的模型准确率？
- CTR模型的理论AUC上界是什么，跟什么相关，为什么会存在？
- 模型预测值的准确性是什么意思？为什么需要校准？校准背后的原理是什么？

#### **▐  1. CTR的微观不可预测性**

不失一般性，下文统一以展示广告场景为例来做介绍。点击率，即发生点击的概率，度量的是“某时某刻某地用户对看到的某个广告点击的可能性”。那问题来了，这个可能性能否被准确预测？

简便起见，记“某时某刻某地用户看到某个广告”这个事件为 $E$。根据大数定律，如果我们能够在平行空间将事件 $E$ 独立运行 $N$ 次，并观察到其中 $T(T≤N)$ 次发生了点击。当$N$足够大时我们可以断言：
$$
p(click=1)≈\frac TN
$$
显然 $p(click=1)∈[0,1]$，随着 $N$ 趋近于无穷大，这个值越逼近真实值。然而事与愿违：客观世界中，我们能且只能观察到事件 $E$ 发生1次。换句话说，事件 $E$ 发生后点击概率的真实值（true-ground）我们永远无法获得，因此==微观层面来看CTR是无法被准确预测的，我们的预测只是对真实值的某种猜测==。

为了更准确地理解这个结论，我们给出数学化的描述。事件$E$ 发生后的结果只能是点击或不点击两种情况，我们用Bernoulli分布来刻画，对应参数为$θ_E(x)$，其中 $x$ 表示事件 $E$ 的所有背景知识，包括用户信息、广告信息、场景及上下文信息，也就是feature。

点击的概率 $P_E(y=1|x)$ 服从参数为 $θ_E(x)$  的Bernoulli分布, 其中 $y$ 代表事件的结果，也就是label。

微观视角上看来每个事件$E$是无关联的，伯努利分布的参数 $θ_E(x)$  仅靠对事件$E$ 的一次抽样无法准确学习，即：==微观层面的CTR不可被准确预测==。

#### **▐  2. CTR建模任务背后的假设**

CTR建模的任务是，对于未来的每一个类似事件，我们要给出用户可能发生点击概率的预测值。上一节的分析表明，微观层面是CTR不可被准确预测的。为了解决这个困难，我们只能对问题做适当的简化，引入一些假设，使得问题近似可解。

#### **2.1 CTR建模任务的第一层简化**

业界当前主流的做法是 ==假设 $θ_E(x)$  跟单个事件 $E$ 本身无关，仅跟事件的特征有关==，即： $θ_E(x)=θ(x) $。直观地讲，就是假设数据集中所有人的点击行为不再孤立地仅仅跟自己有关，彼此之间有一个内在的共性规律，事件发生后是否被点击（y取值为1或者0）都服从参数为 $θ(x)$ 的伯努利分布。这样，我们观测到的所有事件，就构成了对联合分布 $(X,Y)$ 的独立同分布采样，进而CTR建模就转化为对 $p(y|x)$  这个条件分布的learning问题，这是可学习的。当然简化的方法不止一种，不同的假设对应不同的模型空间。例如：

> $θ_E(x)=θ(x_{ad})$ 认为“是否发生点击”仅跟广告信息有关，此时模型退化为非个性化模型；
>
> $θ_E(x)=θ_{user}(x)$  认为不同用户对“是否发生点击”的概率服从不同的分布，但同一个用户多次发生的事件服从同一个分布，那么此时的CTR 建模就退化为“一人一世界”模型。

#### **2.2 CTR建模任务的第二层简化**

上述假设，建立了对全局样本服从参数为 $θ(x)$ 的Bernoulli分布的学习框架。然而常见的CTR模型给出的预测结果并不是分布，而是某个确定的点击率值。这难不倒我们，求解分布的某种统计量，例如期望均值，是很自然的选择。对于Bernoulli分布而言，其期望值即为参数$θ$ 。因此在上述假设下我们可以进一步得到：$p(y=1|x)=θ(x)$ 。注意：这里的 $θ$是一个跟$x$有关的常量。

抛掉过程中所有的细节假设，我们可以从一个全新的角度来理解它：对于事件 $E$ ，特征为 $x$，它发生点击的概率为一个跟特征$x$取值有关的量。假设这个关系由函数$f(⋅)$给出，即：$θ(x)=f(x)$。

我们对上述表述稍加整理：事件 $E$ 发生点击$（y=1）$的概率是其特征 $x$ 的函数，即: $p(y=1|x)=θ(x)$

神奇！一下子回到了我们熟悉的CTR模型形式化。虽然看着简单，让我们再回顾下，这个形式化背后事实上是经过了两轮简化的：

> “某时某刻某地用户看到某个广告”，是否会发生点击，不是完全取决于这个用户，它受一个公共的规律限制：所有类似事件发生点击的概率服从一个参数为 $θ(x)$ 的Bernoulli分布，参数$θ(x)$是事件自身属性（特征 $x$）的函数： $θ(x)=f(x)$ 。注意：这里的函数 $f(⋅)$ 同样要求所有用户都服从；
>
> 点击率本身是个分布，实际模型输出分布的均值。对于Bernoulli分布而言均值恰好为$θ(x)=f(x)$ ，因此预测的点击率为 $p(y=1|x)=θ(x)$ 。

再次提醒：尽管我们通过简化使得CTR任务可以求解，但模型输出的CTR只是在给定假设空间的预测值。预测值不一定等于真实值，因为我们根本不知道真实值是什么，预测仅仅是对真实值的某种猜测。这个猜测距离真实值的距离，取决于简化假设与实际问题相符的程度。

#### **▐  3. CTR模型的形式化选择**

现在很清楚了，CTR模型刻画的是条件分布$p(y|x)$，$x$为特征，$y$取值为0或1。点击发生的概率为$p(y=1|x)$，取值范围是$[0,1]$。

有一个容易混淆的小知识点：既然是预测CTR，为什么不采用回归模型？答案：从传统的machine learning视角，我们现在的数据集是{(x,y)，}，y取值为0和1，这是一个典型的二分类问题。只是我们要求模型输出的不仅仅是分类的label，同时需要输出属于这个label的概率值。 

$CTR=p(y=1|x)$ 只是y取值为1的概率，而不是y的取值。很多人在此处产生了混淆。

进一步地，CTR模型将 $p(y|x)$ 参数化，通过data-driven的方法对参数进行拟合。任意选定一个模型形式，如LR或者DNN，都是对 $(X,Y)$ 的联合分布或者函数 $f(x)$ 做了进一步的假设。

例如，LR模型假设了数据集$\{(x,y)\}$是广义线性可分且$f(x)= sigmoid(w^Tx+b)$，DNN模型假设了$f(x) = sigmoid( DNN(x) )$。 选择了不同的模型形式，本质是对上一节反复强调的“所有人的点击行为要服从一个公共的规律”这个假设的具象化，换言之，不同的模型形式对应于不同的假设空间。

从原理上来说，在==同一个特征体系下不同模型形式对应的模型性能差异，代表着不同假设跟真实情况逼近程度的差异==。

#### **▐  4. CTR模型的性能评价之一：序的准确性**

现在，通过收集大量的展现-点击样本构造${(x,y)}$ 数据集即可训练一个CTR模型。一般而言，特征的构造对模型性能有关键性的影响。SOTA 的方法都是采用大规模ID化特征体系，对“某时某刻某地用户看到某个广告”这个事件，从全方面的视角进行刻画，如用户的历史行为有哪些、广告的创意是什么内容、上下文场景信息有什么，甚至今天的天气情况、是不是节假日等信息都充分收集，希望获得影响用户点击行为的所有可能因素，从而让模型能够成功捕捉和预测点击发生的概率。
#### **4.1 矛盾样本现象**

一个常常会出现、但传统的machine learning理论却鲜有提及的现象是，==训练集中会出现矛盾样本：$x$ 相同但y取值不同==。例如，同一个用户很短时间里面看了同一个广告2次，一次发生了点击、另一次没有，而在这个时间段里面特征 $x$ 的取值没有发生任何变化（这是很有可能的，事实上很多实际应用中$x$ 都是以静态特征为主，缺乏反映用户状态的实时信息）。这种矛盾样本现象常会引起初学者的困扰。

矛盾样本的根源，来自第二节对问题的简化操作：“假设 $θ_E(x)$ 跟单个事件 $E$ 本身无关，仅跟事件的特征有关”。换句话说，我们强行抹掉了每一个独立事件本身的属性，认为所有事件独立同分布。如果把每个事件唯一区分，如精确到纳秒的事件发生时间作为特征加入到x中，那矛盾样本自然就解开了。换个角度来看，矛盾样本的存在说明我们的特征丢掉了原始问题的部分信息，导致训练集是带有噪声的，进而导致模型的准确率永远不可能达到$100 \%$。事实上在传统的机器学习理论中，因数据存在噪声分类器有个最小错误率的界，即贝叶斯错误率（Bayes error rate）。

有意思的是，跟上一节阐述的“不同模型形式选择差异性代表不同假设跟真实情况逼近程度的差异”类似，特征的设计其实是从另外一个角度引入了对问题的逼近误差。原始的问题中，每一个事件都是独一无二的，换句话说不应该存在矛盾样本。

$p(y=1|x)=f(x)$ 这个建模框架中，==$x$ 的设计决定了简化问题的能力天花板，$f$ 的设计决定了逼近天花板的程度==。

**4.2 模型性能评估器的选择**

另一个常见问题是，为什么CTR模型的性能度量不采用常见的分类器准确率而是AUC。不用准确率的原因大部分同学都能想到：CTR任务正负样本比例倾斜严重，模型对正样本预测全错的情况下准确率都非常高，换句话说，准确率的评估分辨率太低。使用AUC的原因则相对来说冷僻，这里解释下。

先回到本质，两个CTR模型的好坏从什么角度进行比较？我们没法在微观层面对每一个样本的预测值进行度量，一个可行的选择是从宏观层面对样本的比较关系进行度量。一个样本如果实际发生了点击，那么$Y=1$ 的预测概率应该大于 $Y=0$ 的预测概率；反之亦然。进一步放宽些，我们希望数据集中，所有正类样本的预测 score 大于负类样本，也就是模型的预测序尽可能逼近真实序。恰好统计领域有一种假设检验叫 $Wilcoxon-Mann-Witney Test$ ，它测试的是任意给一个正类样本和一个负类样本，正类样本的 score 有多大概率大于负类样本的 score。有牛人证明了AUC跟 $Wilcoxon-Mann-Witney Test$ 等价[1]，从而==AUC的物理意义就很清晰了：“任意给一个正类样本和一个负类样本，正类样本的score大于负类样本的score”  的概率值==。

再回过头来看，CTR建模从根源上就是对原始问题的简化产物。既然我们都不知道CTR真实值，那么绝对意义上的预测准确评估也就没有意义，退而求其次，对观测到的数据度量模型预测的结果序，就是个还不错的选择了。当然，除了AUC之外，我们还可以选择其余的评估方式比如 $lift$ 等，它们对应于其它的物理含义，这里不做展开了。

#### **4.3 理论AUC上界**

讲完了AUC就必须要谈谈另一个非常重要的概念：理论AUC上界。

先讲下如何计算：对于给定的训练数据集 ${(x,y)}$，由于矛盾样本的存在，一定有同一个 $x$ 但不同 $y$ 的情况。对数据集做个归并操作：所有$x$ 相同的样本，统计其CTR 值（统计N个重复样本里面y=1的个数T，CTR=T/N；如果x不存在重复，即N=1时，CTR=y 即可），将这个值作为一个理论最优分类器的预测值，计算此时模型的 AUC，得到的就是理论 AUC 上界。==AUC上界对应于传统机器学习里的贝叶斯错误率，是有噪声数据下模型学习能够达到的最高水平==。

从计算过程可以看出，==理论AUC上界度量的其实是样本中的混淆度（即矛盾样本出现的程度）==，它是给定数据集后模型能力的天花板，可以用来判断模型迭代的边际收益。

本质上，理论AUC上界取决于特征的设计。举个例子，如果特征仅仅跟AD有关，与用户或者场景信息无关，这个时候模型其实就退化为一个简单的、广告维度的统计模型，非个性化，显然在这种特征空间下，模型能够达到的AUC天花板将远低于个性化的情况。

另一方面，过高的理论AUC上界也不是好事。例如，将每一个样本id加入到特征里面，原则上理论AUC=1。但==这种过细的、不具备泛化能力的特征，会对模型的学习过程造成强烈的干扰，尤其是对DNN这样解空间巨大、局部点遍布的函数簇，反而可能导致模型的性能下降==。



#### **▐  5. CTR模型的性能评价之二：值的准确性**

这个话题是最充满迷思、也是我多年来一直反反复复跟很多人强调的。很多外行的同学、甚至不少从事CTR模型技术研发的同学，经常挂在嘴边的一个概念：CTR建模的任务就是做准了。这里的“准”，潜台词是：某个广告，或者某个广告计划，真实CTR是 $1\%$ ，你模型预测出来怎么是 $1.2\%$ ，一定是有问题。

相信认真看完前面内容的同学，对这个点应该能突口而出：CTR模型没法评估预测值的准确性，因为我们并不知道真实值。
同学A提出反对的意见：不对呀，真实数据中，我的确可以看到某个广告，曝光了10000次，被点击了100次， $1\%$ 不是难道真实的吗？要准确地回答同学A的疑问，那我们需要引入一个新的视角来审视CTR模型：坐标系。

我们做个详细的剖解：对于模型而言，它是在给定数据集 $\{(x,y)\} $ 上拟合函数  $f(x)$ ，此时模型是在最细粒度的特征空间 $X$ 来进行参数的拟合学习，换言之，模型学习的时候是基于全空间坐标系（简称$FS$坐标系）。同学A是在广告维度进行的统计观察，他看到广告的曝光和点击次数，本质上对应的是仅广告特征的坐标系（简称AD坐标系）。==坐标系不同，相应统计量背后的数据分布也就不同，因此不可直接比较==。

事实上，对于这个广告的10000次曝光，在$FS$坐标系下模型会预测每个曝光的细粒度pCTR值。注意：这里对每一个曝光的预测pCTR，模型的输入是完整特征集合X，而不是仅仅广告特征。同学A直接将这些pCTR值取平均得到该广告的平均预测pCTR值，相当于是对“除去广告特征之外的特征子空间对应的残差坐标系（简称Res坐标系）”进行了积分操作。只不过这个积分操作是针对某一个特定的广告进行的，而这个广告只是在Res空间的10000次曝光采样。实际广告系统中，广告受特定的人群定向以及出价的影响，其竞得的曝光明显跟全局流量不同分布。那就清楚了：模型预测这10000次曝光的pCTR基于的是对整个Res特征空间的分布拟合，而同学A统计的真实CTR仅仅是站在Res空间中单个广告视角的拟合，这是有偏的，从而导致这10000次曝光上CTR的预测平均值跟真实统计平均值不同。换言之，不是模型不准确，而是你要的东西跟模型学习的东西不一致，各说各话自然说不到一块去。

到这里，我们可以先对CTR模型的值准确性话题做个小结：

- 真正意义上的值准确性是不存在的，因为我们观测不到微观意义上每一个事件的真实CTR值；
- 在第2节的假设下我们可以对每一个事件（样本）给出预测的pCTR值，这只是代表在特定坐标系(特征空间)、特定模型形式(模型空间) 两种选择下的数学值，是对微观意义上真实CTR的逼近；
- 计算的CTR预测是否准确，需要站在同一个坐标系中。不同的坐标系会进一步带来观测视角的偏差，这个偏差跟模型本身无关。

同学B进一步提出了疑问：你讲的我明白了，但实际广告业务中非常重要的自动出价（auto-bidding）技术，需要模型站在每一个AD的视角给出准确的pCTR值，这样才能帮助AD完成每一个曝光粒度的最优出价。现在你说模型给不了，这该怎么办？

当然是有办法的，先说一个容易的、也是绝大多数团队采用的解法：后置校准。站在坐标系的角度来看，校准相当于是一个面向特定观测者所在的坐标系构建的级联CTR模型。给定在$FS$坐标系下训练的模型 $p(y=1|x) = f(x)$ ，校准进一步站在新的观测坐标系做了变换： $p(y=1|x) = g( f(x) )$ 。这很容易理解，我就不多做解释。

值得注意的是，校准本质上是在基础模型之上，引入了第二个学习目标。基础模型的目标是在给定解空间拟合数据、最大化AUC（序的准确性）；校准模型的目标是在后验的统计意义上调整pCTR值的大小，使得预测值尽可能逼近观测到的统计值（值的准确性）。然而，这种两段式建模方式，虽然第二阶段的校准可以尽量保序，从而不影响模型的AUC表现，但两段式建模非最优。因为基础模型预测的pCTR分布代表了模型对数据的归纳；现在既然已经知道有特定维度上这个归纳不准确，end-2-end联合建模显然能够触及更高的天花板。这个方向的工作我们团队正在推进。

#### **▐  6. 课后思考题**

除了以上介绍的这些常见的小知识点，事实上跟CTR相关的还有很多基础但重要的问题，比如：

1. 虽然由于坐标系不同，预测的pCTR值跟真实CTR值可以有偏差，但为什么有些维度的特征没有偏差，有些维度偏差却很大？特别地，一些粗粒度特征的偏差一般都不大，为什么？
2. 模型为什么容易高估？往往离线评估预测值/统计观测值基本接近1，而在线服务时高于1
3. CTR模型的样本来自模型自身的选择，持续的循环是否会导致系统陷入局部陷阱？如何判断？
4. CTR模型真的要解决bias问题吗？尽管我对大量长尾的、竞价失败的广告甚至都没有见过其样本，我需要花大力气去优化他们吗？

这些问题留作思考题，欢迎大家积极讨论。

#### **参考文献：**

[1] Areas beneath the relative operating characteristics ($ROC$) and relative operating levels ($ROL$) curves: Statistical significance and interpretation