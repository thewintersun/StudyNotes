## Deep Neural Networks for YouTube Recommendations

论文地址: https://research.google/pubs/pub45530/

发表：*Proceedings of the 10th ACM Conference on Recommender Systems*, ACM, New York, NY, USA (2016) (to appear)

作者：Paul Covington Jay Adams Emre Sargin

机构：Google



### 摘要

YouTube represents one of the largest scale and most sophisticated industrial recommendation systems in existence. In this paper, we describe the system at a high level and focus on the dramatic performance improvements brought by deep learning. ==The paper is split according to the classic two-stage information retrieval dichotomy: first, we detail a deep candidate generation model and then describe a separate deep ranking model==. We also provide practical lessons and insights derived from designing, iterating and maintaining a massive recommendation system with enormous user-facing impact.



### 一、背景介绍

我们将以YouTube在2016年发表的论文《Deep Neural Networks for YouTube Recommendations》为背景进行YouTube的深度神经网络推荐模型的介绍。

**YouTube推荐系统的三大难点**

1. **数据规模**：YouTube 的用户和视频量级都是十亿级别的，==需要分布式学习算法和高效的部署==。
2. **新颖性**：推荐系统需要及时对==新上传的视频==和==用户的新行为==作出响应。
3. **数据噪音**：由于稀疏和外部因素影响，用户的==历史行为很难预测==。大部分 YouTube ==视频只有隐式反馈==（即用户对视频的观看行为），缺少显式反馈（即用户对视频的评分）。此外，==视频的元信息不够有结构性==。我们的算法需要对训练数据的这些因素稳健（robust）。

### **二、系统概览**

<img src="https://pic2.zhimg.com/80/v2-c1c78fd70da18a887d2dbf273a6d4269_720w.jpg" alt="img" style="zoom:80%;" />

​																					Youtube推荐系统整体架构

**第一部分 召回网络**：此阶段主要目的是从百万级的视频中检索出一小部分的视频用于之后的排序操作，这部分需要处理的==数据量非常大，速度要求快==，所有==使用的模型和特征都不能太复杂==。召回网络会根据用户的历史信息（比如用户的历史观看、搜索等）进行召回，这一阶段召回的视频==满足用户泛化的兴趣==，用户之间的相似度则通过粗略的特征来表示，如用户观看视频的ID，搜索query和用户画像。

**第二部分 排序网络**：此阶段会使用更加丰富和精细的用户和视频特征，预测用户对召回的视频打分，然后根据分数进行排序，依次展示给用户。这部分最主要是能够精准的将视频推送给用户，所以需要更加复杂的模型和特征来提升推荐效果。

**第三部分 线下评估**：评估指标有precision、recall、ranking loss等，最终的效果还是需要线上做A/B测试，关注的指标包括：点击率、观看时间等；需要指出的是，线上线下有时候并不能保持一致的结果。

接下来一起看看**Matching**和**Ranking**的具体设计思路，同时会给出具体实现代码，帮助加深理解算法原理。在介绍召回模型前，先看看传统的召回思路。

### 三、Matching

**3.1 传统召回思路**

先离线计算好商品的Embedding以及用户的Embedding，线上召回的时候，根据用户的Embedding去和所有商品的Embedding内积，找出TopN。这种传统的方式需要解决以下几个问题：

- 商品的Embedding空间和用户的Embedding空间如何保证在同一个空间。
- 需要计算与所有商品的内积，存在性能问题。
- 诸如奇异值分解的方法，输入协同矩阵，特征比较单一。

**问题建模**

在召回阶段，Youtube把推荐问题看成一个超大规模多分类问题，即Softmax分类。定义为基于特定的用户$U$和其上下文$C$ ，在 $t$ 时刻，将视频库中$V$指定的视频$w_{t}$划分为第$i$类的概率。公式如下：
$$
P(w_{t}=i|U,C) = \frac{e^{v_{i}u}}{\sum_{j\in V}^{}{e^{v_{j}u}}} \\
$$
其中 $u\in \mathbb{R}^N$ 表示（用户，上下文）的高维embedding，$ v_{j}\in \mathbb{R}^N$ 表示每个候选视频的embedding向量。 $v_{j}u$ 表示第 $ j $ 个视频的embedding向量，这里每个视频都有一个embeeding向量表示。

至此，该DNN的目标是在给定用户历史行为与上下文的情况下，学习user embedding向量 $u$ ，作为输入送到softmax classifier，用以生成初步候选集作为视频召回结果。

**3.3 负类采样（重要性采样softmax）**

在每次计算中，softmax的分母，都需要遍历视频库 $V$ 中所有的视频，并且用户上下文向量与视频向量之间的点积，$exp$ 等操作造成计算量过大，因此如何高效训练成为一个问题。其解决方法采用了==负采样机制（sample negative classes ）==提升训练速度，并使用==重要性加权（importance weighting）==的方式校正这个采样。==对于每个样本，对真实标签和采样得到的负类，最小化其交叉熵损失函数==。相比经典 Softmax，这有几百倍的速度提升。

**3.4 召回模型网络结构**

在做NLP任务时，如何将文本或者文本中的一字一句，表示成结构化的，计算机能够理解的形式是第一步。经常采用方法的就是word2vec，就是将所有的word表示成低维稠密的embedding向量，最后将词的embedding向量喂给神经网络进行学习。

![img](https://pic1.zhimg.com/80/v2-901125442d1ae63b2fd9a9fa02f4bd20_720w.jpg)

​																								召回模型的网络结构

Youtube的召回模型也受此启发，采用了word embedding的技巧来计算每一个视频的embedding，然后将视频的embedding，用户搜索视频的embedding分别计算average，再加入用户的属性、视频质量等特征，采用两个完全相连的ReLU层和softmax函数来预测用户下一个看的视频是什么。

使用DNN的原因之一，在DNN中连续性变量和类别型变量都很容易输入到模型中，包括一些人口统计特征（Demographic features），对最终的效果起着十分重要的作用。用户的地域，设备等都可以作为embedding向量输入到DNN中去。简单的二值化特征（如性别）和数值型特征（如年龄）可以直接输入到DNN中，数值型需要经过归一化到[0,1]再输入到模型中。

**（1）输入层**

- 用户观看视频序列ID：对视频ID的embedding向量进行mean pooling，得到视频观看向量（watch vector）。
- 用户搜索视频序列ID：对视频ID的embedding向量进行mean pooling，得到视频搜索向量（search vector）。
- 用户地理特征和用户设备特征：均为一些离散特征，可以采用embedding方法或者直接采用one-hot方法（当离散维度较小时），得到用户的地理向量和设备向量。
- Example Age：在推荐系统中很重要的一点是视频的新颖性，用户更倾向于观看新视频，但机器学习模型都是基于历史观看视频记录学习，所以在某种程度上，模型和业务是相悖的，所以文中构建了一个特征example age，简单的可以理解为是视频的年龄，初始值设为0，随着时间的增长，记录视频的年龄。加入之后效果十分明显，如图所示：

<img src="https://pic2.zhimg.com/80/v2-849bf65dfe5ffa0cb01b5cfe44ba1eb9_720w.jpg" alt="img" style="zoom:80%;" />

- 人口属性特征：用于提供先验，使其对新用户也能做出合理的推荐。具体来说，对用户的地理区域和使用的设备进行 Embedding 并将特征进行拼接（concatenation）。

**（2）MLP层**

使用常见的塔状设计，底层最宽，往上每层的神经元数目减半，直到 Softmax 输入层是 256 维（1024ReLU->512ReLU->256ReLU）。

**（3）Softmax 层**

深度神经网络的视频的 embedding 向量$v_i$ 和用户的 embedding 向量 $u$，召回任务预测用户 $u$ 在 $t$ 时刻观看的视频：
$$
P(w_{t}=i|U,C) = \frac{e^{v_{i}u}}{\sum_{j\in V}^{}{e^{v_{j}u}}} 
$$
**（4）输出层**

输出层的维度和视频ID的embeeding向量维度一致，最终得到用户向量 $u$。

> 将最后softmax层的输出矩阵的列向量当作item embedding vector，而将softmax之前一层的值当作user embedding vector

通过该网络结构的学习，最终可以得到所有视频的embedding向量 $V$，其维度为 $pool\_size\times k$，其中$pool\_size$为训练集视频资源的大小，$k$为embedding的维度。我们还可以得到所有用户的输出向量 $u$，其中每个用户向量的维度为 $k$ 维，和视频的embedding 向量维度一致。

**3.5 召回优化**

在线服务阶段，通过视频向量$V$和用户向量$U$进行相似度计算，为了满足时延要求，在进行实际的召回计算时采用的是最近邻查询的方式。对于每个用户向量 $u$ ，对视频库中的所有视频根据向量 $v$ 做最近邻算法，得到top-N的视频作为召回结果。

**3.6 样本选择和上下文选择**

在有监督学习问题中，最重要的选择是label了，因为label决定了你做什么，决定了你的上限，而feature和model都是在逼近label。我们的几个设计如下：

- **使用更广的数据源**：不仅仅使用推荐场景的数据进行训练，其他场景比如搜索等的数据也要用到，这样也能为推荐场景提供一些explore。
- ==**为每个用户生成固定数量训练样本**：我们在实际中发现的一个practical lessons，如果为每个用户固定样本数量上限，平等的对待每个用户，避免loss被少数active用户domanate，能明显提升线上效果==。
- **抛弃序列信息**：我们在实现时尝试的是去掉序列信息，对过去观看视频/历史搜索query的embedding向量进行加权平均。这点其实违反直觉，可能原因是模型对负反馈没有很好的建模。
- **不对称的共同浏览（asymmetric co-watch）问题**：所谓asymmetric co-watch值的是用户在浏览视频时候，往往都是序列式的，开始看一些比较流行的，逐渐找到细分的视频。下图所示图(a)是held-out方式，利用**上下文信息**预估中间的一个视频；图(b)是predicting next watch的方式，则是利用**上文信息**，预估下一次浏览的视频。我们发现图(b)的方式在线上A/B test中表现更佳。*而实际上，传统的协同过滤类的算法，都是隐含的采用图(a)的held-out方式，忽略了不对称的浏览模式。*

![img](https://pic4.zhimg.com/80/v2-4c34494e753fa7ad6525f3533db31147_720w.png)

### 四、Ranking

召回阶段已经给出了候选集，在排序阶段，其目的是对给定的小规模候选集进行精细化的排序。通常，排序阶段还涉及对多个不同召回源的视频进行有效集成，这给排序阶段增加了难度。

![img](https://pic1.zhimg.com/80/v2-731710d8c61e125aa3228d1cded124d8_720w.jpg)

​																								排序模型的网络结构

**4.1 训练阶段**

选择的基础模型是DNN+weighted logistic regression。==负样本是有曝光无点击的视频，正样本是有曝光有点击的视频，预测用户的观看时长==。正样本记录了用户观看视频的时长，为了预测观看时长，专门设计了一个加权逻辑回归模型。（加入了观看时长这一衡量标准，也是为了用来解决噪声的，因为很多时候用户点击观看一个视频并不代表用户真的喜欢，满意这个内容）

这个模型是基于交叉熵损失函数的逻辑回归模型训练的。但是我们用观看时长对正样本做了加权，负样本都用单位权重（即不加权）。这样，我们通过逻辑回归学到的优势就是$(\sum_{}^{}{T_i})/(N-k)$，其中$N$是样本数量，$k$ 是正样本数量，$T_i$ 是观看时长。假设正样本集很小，那么我们学到的优势就近似$E[T](1+P)$，P是点击概率，E[T]是观看时间的期望值。因为P很小，那么这个乘积就约等于E[T]。我们用指数函数$e^{x}$作为最终的激活函数来产生近似观看时长的估计值。

**4.2 特征表示**

我们的特征与分类和连续/序列特征的传统分类方法是不一样的，从两个维度对特征进行分析：

1. 取值数量：分为单值特征，比如当前待展示待打分的视频ID；和多值特征，比如用户过去观看的N个视频ID；
2. 特征描述内容：我们还根据它们描述项目的属性（“印象”）还是用户/上下文（“查询”）的属性来对特征进行分类。 每个请求计算一次查询特征，同时为每个已评分项目计算展现特征。

**（1） 特征工程**

虽然DNN隐含了特征工程，但是作者还是做了很多特征工程（个人认为，这说明网络模型并不是很适合这个数据，或者说这个网络结构不像CNN那样高效）。最重要的三方面特征如下：

1. 用户历史行为：用户之前和那些items有过交互，或者和相似的items的交互；
2. 上次观看时间：自上次观看同channel视频的时间，原理类似“注意力机制" ；
3. 之前视频已经被曝光给该用户的次数：如果一个视频之前展示过，用户没有观看，那么用户在下次展示的时候依旧不会观看的概率比较大，其原理类似“exploration”。

**（2）离散特征Embedding**

和候选集生成一样生成，我们使用embedding来将稀疏离散特征映射到适合于神经网络的稠密矩阵形式。每个唯一的视频ID都对应一个单独学习出来的embedding，它的维度大小和整个视频集$V$的ID个数的对数呈正比增加，即**log(unique(values))**。这些视频是通过在训练之前扫描一次数据建立的简单查找表。**对视频集按照ID在点击展现中出现的频率进行倒序排列，仅保留频率最高的topN个ID，其他的ID就被从视频集中去掉了**。不在视频集中的值，简单的被embedding成值为0的向量。像在候选集生成阶段一样，==多值离散特征映射成embedding之后，在输入网络之前需要做一下加和平均==。

重要的是，离散特征对应的ID一样的时候，他们的底层embedding也是共享的。比如存在一个全局的embedding对应很多不同的特征（曝光的视频ID，用户最后一次浏览的视频ID，推荐系统的种子视频ID等等）。embedding虽然是共享的，但是每个特征在训练的时候是单独输入到网络的，以便上层网络可以学习到每个特征的特殊表示。==embedding共享对于提升泛化能力、加速训练、减小内存占用是非常重要的==。绝大多数参数模型是在这种高维embedding中的，例如，100万个ID映射成32维的空间，其参数是完全连接的2048个单元宽网络参数的7倍多。

**（3）连续特征归一化**

神经网络对其输入的缩放和分布非常敏感，而诸如融合了决策树的模型对于各个特征的缩放是不会受什么影响的。我们发现连续特征的正确归一化对于收敛是至关重要的。一个符合$f$分布的特征$x$，等价转化成 $\bar x$，用微积分使其均匀的分布在[0,1）区间上。在训练之前，扫描一次数据，用线性插值的方法，基于特征值的分位数近似的计算出该积分。

除了输入归一化的特征之外，我们还输入归一化特征 $\bar x$ 的平方、 $\bar x$的平方根，特征的超线性和子线性的函数使得网络有更强的表达能力。输入连续特征的幂值，被证明是能提高离线精度的。这样看似毫无逻辑的特征竟然也有用，可能真的是丰富了特征的表达吧，只能这么理解了。

**4.3 隐层实验**

<img src="https://pic3.zhimg.com/80/v2-0b36cb019a91ab32786d4805757cce02_720w.jpg" alt="img" style="zoom:80%;" />

表中显示了在保留数据集上用不同的隐层配置得到的结果。每个配置得到的值是通过在同一个页面上展示给一个用户的正负样本而来的。我们首先用我们的模型对两种样本进行打分。如果负样本的得分比正样本的得分高，就认为这个正样本的观看时长是错误预测。每个用户的损失值就是所有错误预测的数量。

对网络结构中隐层的宽度和深度方面都做了测试，从下图结果看增加隐层网络宽度和深度都能提升模型效果。而对于1024->512->256这个网络，测试的不包含归一化后根号和方式的版本，loss增加了0.2%。而如果把weighted LR替换成LR，效果下降达到4.1%之多。

