## 推荐系统——召回篇

文章地址：https://zhuanlan.zhihu.com/p/351716045

### 背景

新年开工第一周，孟子曰“兵马未动，粮草先行”，最近总结了一下推荐算法的相关知识，也算是为今年开个头。将推荐系统分成四个部分，召回、粗排、精排、重排，本文是第一个部分，主要讲讲推荐系统的召回。 

### 目录

**1.1 召回层的作用和意义**

**1.2 召回模型的演化关系图**

**1.3 召回模型前言——传统协同召回算法**

- 1.3.1 基于协同过滤的召回算法
- 1.3.2 基于矩阵分解的召回算法
- 1.3.3 传统召回算法总结

**1.4 Embedding召回基本框架**

- 1.4.1 Embedding产生过程
- 1.4.2 Embedding发展历史
- 1.4.3 i2i召回的基本框架
- 1.4.4 u2i召回的基本框架

**1.5 基于内容语义的i2i召回**

- 1.5.1 如何生成Item Embedding
- 1.5.2 Word2vec——经典的词向量方法
- 1.5.3 FastText——字符级别n-gram
- 1.5.4 Bert——动态词向量方法
- 1.5.5 内容语义模型扩展为行为序列模型

**1.6 基于Graph Embedding的i2i召回**

- 1.6.1 Deep Walk——随机游走图表征
- 1.6.2 EGES——阿里巴巴Graph Embedding方法
- 1.6.3 Node2vec——优化图结构的Graph Embedding
- 1.6.4 GCN——图卷积神经网络
- 1.6.5 GraphSAGE——基于邻节点聚合的图神经网络
- 1.6.6 Graph Embedding图表征召回总结

**1.7 基于深度学习的u2i召回**

- 1.7.1 DSSM——经典的双塔召回模型
- 1.7.2 YouTube深度学习召回方法
- 1.7.3 Airbnb基于用户短期和长期兴趣的Embedding召回
- 1.7.4 深度学习u2i召回总结

**1.8 召回技术总结**

#### 1.1 召回层的作用和意义

<img src="D:\Notes\raw_images\image-20210525180249564.png" alt="image-20210525180249564" style="zoom:67%;" />

​																									图1 推荐系统的四个阶段

图1显示了推荐系统中的4个主要阶段，其中召回阶段负责从海量数据中快速筛选出部分数据，供后面排序阶段使用。本质上，召回和后面的粗排、精排、重排都属于排序，之所以分成召回阶段和后面3个排序阶段，主要原因是基于工程上的考虑。在精排阶段，一般会使用复杂的模型和特征，比如模型使用深层神经网络，特征filed上千个，如果精排对上百万的候选集排序，耗时肯定扛不住。因此加入召回过程，利用少量的特征和简单的模型或规则对候选集快速筛选，减少后面排序阶段的时间开销。另外一个原因是出于业务上的考虑，排序阶段主要考虑单一目标，比如ctr，而有时候我们希望给用户多展现热点新闻或者时效性数据，这时候可以多加两路召回。总结起来，召回和排序有如下特点：

- **召回层**：候选集规模大、模型和特征简单、速度快，尽量保证用户感兴趣数据多召回。
- **排序层**：候选集不大，目标是保证排序的精准，一般使用复杂和模型和特征。

在设计召回层时，需要同时考虑召回率和计算速度，前面提到既要召回用户感兴趣数据，又要召回热点和时效性数据，如果一次同时召回，那么时间开销会是问题。这时候，一般考虑多路召回，图2显示了多路召回方法。

<img src="D:\Notes\raw_images\image-20210525180819357.png" alt="image-20210525180819357" style="zoom: 80%;" />

​																							图2 多路召回

### 1.2 召回模型的演化关系

召回模型经历了传统协同过滤到embedding模型化召回的演变，后面会主要介绍embedding模型化召回的相关内容。图3是召回技术的演变图谱。

![image-20210511203253461](D:\Notes\raw_images\image-20210511203253461.png)																								图3 召回技术演变

### 1.3 召回模型前言——传统召回算法

在介绍embedding召回技术之前，先回顾一下经典的协同过滤。协同过滤应该是大多数做推荐算法最开始接触的模型。按照维基百科的定义，**协同过滤**（collaborative filtering）是一种在推荐系统中广泛使用的技术，该技术通过分析用户或者事物之间的相似性（“协同”），来预测用户可能感兴趣的内容并将此内容推荐给用户。

#### 1.3.1 基于协同过滤的召回算法

协同过滤算法中，基于用户相似性推荐的叫UserCF，基于物品相似度推荐的叫ItemCF。图4是协同过滤的一个例子，其流程如下：

（1）构建共现矩阵。将图4中的用户评价转化为共现矩阵，用户作为行坐标，商品作为列坐标，喜欢设置为1，不喜欢设置为-1，则共线矩阵可以表示为：
$$
X= \begin{bmatrix}  1 & 1 & -1 & 1 \\  -1 & -1 & 1 & -1 \\ 1 & -1 & -1 & -1 \\ 0 & -1 & 1 & -1 \end{bmatrix}\tag1
$$
（2）计算用户之间的相似度。计算用户E和其它4个用户的相似度$sim\left( E,i \right)$，用户E的向量表示为共线矩阵中的行向量，即$\textbf{x}_{E}=\left( 0,-1,1,1 \right)$，从中选取相似度最高的n个用户。

（3）相似用户兴趣推荐给当前用户。找出相似度最高的用户是B，由于B不喜欢“书”这个商品，因此预测用户E也不喜欢“书”。

上述例子由于是基于用户相似度的推荐，因此是User-CF。

<img src="D:\Notes\raw_images\image-20210529114318164.png" alt="image-20210529114318164" style="zoom:80%;" />

**1. UserCF**

在UserCF中，最关键的步骤是用户相似性计算，共线矩阵中的行向量代表相应用户的用户向量，因此计算用户相似度，就是计算两个向量的相似度。相似度的计算主要有以下5种方法。

（1）Jaccard距离
$$
sim\left( \textbf{x},\textbf{y} \right)=\frac{\left| \textbf{x}\cap \textbf{y} \right|}{\left| \textbf{x}\cup\textbf{y} \right|}\tag2
$$
（2）欧式距离
$$
d\left( \textbf{x}, \textbf{y}\right)=\sqrt{\sum\nolimits_{i}\left( x_i-y_i \right)^2}\tag3
$$

$$
sim\left( \textbf{x},\textbf{y} \right)=\frac{1}{1+d\left( \textbf{x},\textbf{y} \right)}\tag4
$$

（3）皮尔逊相关系数
$$
sim\left( \textbf{x}, \textbf{y}\right)=\frac{\sum\nolimits_{i}\left( x_i-\overline{\textbf{x}} \right)\left( y_i-\overline{\textbf{y}} \right)}{\sqrt{\sum\nolimits_{i}\left( x_i-\overline{\textbf{x}} \right)^2}\sqrt{\sum\nolimits_{i}\left( y_i-\overline{\textbf{y}} \right)^2}}\tag5
$$
（4）余弦相似度
$$
sim\left( \textbf{x},\textbf{y} \right)=\frac{\textbf{x} \cdot \textbf{y}}{\left \| \textbf{x}\right \| \times \left \| \textbf{y}  \right \|}=\frac{\sum\nolimits_{i}x_iy_i}{\sqrt{\sum\nolimits_{i}x_i^2}\sqrt{\sum\nolimits_{i}y_i^2}}\tag6
$$
（5）Tanimoto系数
$$
sim\left( \textbf{x},\textbf{y} \right)=\frac{\textbf{x} \cdot \textbf{y}}{\left \| \textbf{x}\right \|^2+ \left \| \textbf{y}  \right \|^2-\textbf{x} \cdot \textbf{y}}=\frac{\sum\nolimits_{i}x_iy_i}{\sqrt{\sum\nolimits_{i}x_i^2}+\sqrt{\sum\nolimits_{i}y_i^2}-\sum\nolimits_{i}x_iy_i}\tag7
$$
在获得Top n 相似用户后，接下来是生成最终的排序结果。最常用方法是对Top n 用户评分做加权平均，加权系数是用户相似度。计算方式如下。
$$
r_{u,p}=\frac{\sum\nolimits_{i}w_{u,i}\cdot r_{i,p}}{\sum\nolimits_{i}w_{u,i}}   \tag8
$$
其中$ w_{u,i} $是用户 u 和用户 i 的相似度，$ r_{i,p} $是用户 i 对物品 p 的评分。

UserCF直观上比较好理解，就是找一群相似用户，把相似用户喜欢的物品推荐给目标用户。但是在工程和效果上都有一些缺陷，总结为以下两点。

（1）**计算和存储开销**。UserCF需要计算和存储用户之间的相似度，在互联网企业，用户规模一般都比较庞大，导致计算和存储的开销非常大。

（2）**稀疏用户效果不佳**。用户的历史行为一般都是比较稀疏的，比如电商场景，==有些用户可能一个月就几次购买行为，对于这些用户计算相似度一般都不太准确，因此UserCF不太适合用户行为稀疏的场景==。

既然通过用户协同面临诸多问题，那么换一个角度，是否可以通过物品协同，接下来介绍下ItemCF。

**2. ItemCF**

由于UserCF工程和效果上的缺陷，大多数互联网企业都选择ItemCF。ItemCF是基于物品相似度进行推荐的协同过滤算法。具体讲，通过计算Item之间的相似度，得到Item相似度矩阵，然后找到用户历史正反馈物品的相似物品进行排序和推荐，ItemCF的步骤总结如下：

> （1）构建共现矩阵。根据用户的行为，构建以用户为行坐标，物品为纵坐标的共现矩阵。
>
> （2）构建物品相似度矩阵。根据共现矩阵计算两两物品之间的相似度，得到物品相似度矩阵。
>
> （3）获取Top n 相似物品。根据用户历史正反馈物品，找出最相似的 n 个物品。
>
> （4）计算用户对Top n 物品的喜好度。用户对物品的喜好度定义为：当前物品和用户历史物品评分的加权和，加权系数是前面计算的物品相似度。计算方法如下所示。
> $$
> r_{u,p}=\sum\nolimits_{i}w_{p,i}\cdot r_{u,i}\tag{9}
> $$
> 其中$ r_{u,p}$ 表示预估的用户 u 对物品 p 的喜好度， $w_{p,i} $是加权系数，表示目标物品 p 和Top n 集合中物品 i 的相似度， $r_{u,i} $表示用户历史对物品 i 的评分。
>
> （5）按喜好度生成排序结果。

和UserCF相比，由于物品规模一般远远小于用户数，因此ItemCF的计算和存储开销都比UserCF小得多。除了技术实现上的区别，UserCF和ItemCF的应用场景也有所不同。总结为下面两点。

（1）==UserCF更适合新闻推荐场景==。在新闻推荐场景中，新闻的兴趣点一般比较分散，比如虎嗅的新闻一般受众群体是从事IT的人，而UserCF可以快速找到都是从事IT的人群，然后把虎嗅的新闻推荐给自己。

（2）==ItemCF更适合电商或视频推荐场景==。在电商和视频推荐场景中，都有一个共同点，用户的兴趣比较稳定。比如在电商场景，ItemCF可以推荐和兴趣点相似的商品，比如以前经常购买球鞋的人，可以推荐球衣球裤。

#### 1.3.2 基于矩阵分解的召回算法

协同过滤具有简单、可解释性强的优点，在推荐系统早期广泛应用。但是==协同过滤也有泛化能力弱、热门物品头部效应强的弱点==。为了解决上述问题，后来矩阵分解技术被提出来。矩阵分解的过程如图5所示。

<img src="C:\Users\j00496872\AppData\Roaming\Typora\typora-user-images\image-20210529114624204.png" alt="image-20210529114624204" style="zoom:67%;" />

​											                                                图5 矩阵分解过程

矩阵分解在推荐系统中的应用得益于2006年Netflix举办的推荐系统算法竞赛Netflix Prize Challenge，当时Netflix提出能在现有推荐系统基础上误差降低10%的人可以瓜分100万美元奖金，吸引了很多人参加。Netflix的比赛数据正是用户的评分数据，这次竞赛推动了无数推荐算法的产生，其中就包含了一系列矩阵分解模型，而最著名的便是SVD算法及其变种。

SVD是Singular Value Decomposition的简称，翻译过来就是奇异值分解。SVD的具体原理是假设矩阵$ \textbf{M}$ 的大小为 $n\times m $，则可以分解成3个矩阵的乘法$ \textbf{M}=\textbf{U}\Sigma\textbf{V}^T $，其中 $\textbf{U} $是 $m\times m $的正交矩阵， $\textbf{V} $是 $n\times n $的正交矩阵， $\Sigma $是$ m\times n $的对角矩阵。只取对角矩阵 $\Sigma $中的前 k 个较大元素，则SVD可以近似的表示为：
$$
\textbf{M}\approx\textbf{U}_{m\times k}\Sigma_{k\times k}\textbf{V}_{k\times n}^T\tag{10} 
$$
SVD应用在推荐系统中，用户矩阵表示为$ \textbf{U}_{m\times k}$，物品矩阵表示为 $ \textbf{V}_{n\times k}$。这样就可以计算用户和物品的相似度，但是在工程上会有问题，奇异值分解的计算复杂度达到了$ O\left( mn^2 \right) $，对于物品上百万的互联网行业是无法接受的。另外，SVD对于稀疏的共现矩阵也不友好，需要人为填充缺失值，而互联行业用户行为大多是比较稀疏的。
针对传统SVD计算复杂度以及用户稀疏行为效果不好的问题，后来提出了改进的版本。用户 $u$ 的向量表示为$ \textbf{p}_u $，物品 $i$ 的的向量表示为 $\textbf{q}_i $，则用户对物品的评分表示为两个向量的内积 $\textbf{p}_u\cdot \textbf{q}_i$，真实的评分假设为$ r_{u,i} $，目标函数表示为：
$$
\min_{\textbf{p}^{*},\textbf{q}^{*}}\sum_{\left( u,i \right)\in K}\left( r_{u,i}-\textbf{p}_u \cdot \textbf{q}_i \right)^2\tag{11}
$$
然后可以用梯度下降求解上面的目标函数。

> 谈谈矩阵分解的优缺点，相比基于近邻的协同过滤算法来说，有以下几个优点：
>
> - 泛化能力强。一定程度上解决了数据稀疏的问题 ；
> - 空间复杂度低 。只需存储用户和物品的隐向量，空间复杂度由 $n^2$级别降到 $( n + m ) ∗ k$ 级别；
> - 更好的扩展性与灵活性。
>
> 其局限性：
>
> - 缺乏用户历史行为时，无法进行有效的推荐；
> - 矩阵分解不方便加入用户、物品和上下文相关的特征。

#### 1.3.3 传统召回算法总结

传统召回算法有其简单、可解释性强的特点，但是也有自身的局限性。==协同过滤和矩阵分解都没有加入用户、物品和上下文相关的特征，也没有考虑用户行为之间的相关性==。随着embedding技术的发展，召回技术开始朝着模型化embedding的方向演化。

---

### 1.4 Embedding召回基本框架

在介绍embedding召回技术之前，先简单介绍下什么是embedding。embedding其实就是用一个低维稠密的向量表示一个对象，这里的对象可以是一个词、一个商品，也可以是一篇新闻、一部电影，等等。==直观上看embedding相当于是对one-hot做了平滑==。

#### 1.4.1 Embedding产生过程

one-hot表示一个对象时，往往高维稀疏，以词向量为例，牛津词典英语单词大概有10万个左右，因此one-hot就是10万维。经过某个语言模型后，我们可以把它转化为低维的稠密向量，这样就可以很方便的计算词之间的相似度，这个语言模型就是embedding的产生过程，比如接下来要讲的Word2Vec。图6显示了词向量经过embedding化后，man和king之间具有更高的相似度。

![image-20210512093419216](D:\Notes\raw_images\image-20210512093419216.png)																							图6 词向量embedding化

图6显示了embedding具有很好的表达能力，但是embedding究竟是怎么产生的，前面的SVD已经隐约有了embedding的影子。图7显示了embedding的产生过程，可以看出，==embedding是神经网络的中间产物，神经网络的隐藏层权重便是最终生成的embedding==，生成embedding表后，可以通过查表的方式获取具体的embedding向量。

![image-20210512093601169](D:\Notes\raw_images\image-20210512093601169.png)图7 embedding产生过程

#### 1.4.2 Embedding发展历史

embedding最早由Hinton在1986年提出，后来谷歌在2013年提出了Word2Vec，使得embedding这一研究话题迅速成为热点，并成功的应用在了推荐系统等多个领域。到2018年Google提出Bert，摧枯拉朽一般在11项NLP测试中获得SOTA（state-of-art, 最优），embedding研究话题达到新高潮。图8总结了embedding的发展历史。其中，有4个关键节点，包括1986年首次提出embedding概念、2013年Word2Vec诞生、2016年Google提出WDL、2018年Google提出Bert。

<img src="D:\Notes\raw_images\v2-15f4cb9bd12b69818d79cdb00e60474c_720w.jpg" alt="v2-15f4cb9bd12b69818d79cdb00e60474c_720w" style="zoom:50%;" />

​																								图8 embedding发展历史

#### 1.4.3 i2i召回的基本框架

在介绍完embedding的概念后，这部分介绍下基于embedding的召回框架，主要分为i2i召回和u2i召回。i2i召回是指我们可以得到item embedding，但是模型没有直接得到user embedding。u2i是指模型同时得到了user embedding和item embedding。先介绍下第一种情况，即i2i召回框架，如图9所示。离线根据用户历史行为训练召回模型，输出item embedding，存储到数据库中，线上用户请求，根据用户历史行为，从数据库从查找对应的embedding，然后检索相似度最高的n个item，最后将 Top n召回结果返回给后面的排序模块。

![v2-15f4cb9bd12b69818d79cdb00e60474c_r](D:\Notes\raw_images\v2-15f4cb9bd12b69818d79cdb00e60474c_4.jpg)																										图9 i2i召回框架

#### 1.4.3 u2i召回的基本框架

如果召回模型能够直接推断出user embedding和item embedding，那么就可以直接计算二者的相似度，然后进行召回。典型的应用是2016年YouTube的召回模型，可以直接输出user embedding。在u2i召回框架中，有时考虑到用户规模太大，不方便存储，可以在线上召回的时候，直接通过模型请求获取user embedding，然后再检索相似item。图10是u2i的召回框架，和图9的i2i相比，==主要区别在于u2i可以直接基于user embedding 进行检索==。![v2-24a820c9d2445380949dc8bb4366dad1_r](D:\Notes\raw_images\v2-24a820c9d2445380949dc8bb4366dad1_r.jpg)																								图10 u2i召回框架

### 召回部分【1】总结

本文主要总结了召回的基础知识，介绍了一些传统的基于协同过滤的召回算法，引出了embedding技术在召回中的应用。本文算是召回的开碟小菜，后续会重点总结下模型化语义召回、图召回以及深度学习u2i召回。

下一篇将介绍内容语义模型在召回中的应用。

---

### 1.5 基于内容语义的i2i召回

在上一篇文章中，介绍了i2i的召回框架，在获得item的Embedding后，可以基于相似度计算，召回用户感兴趣的item。在模型化召回中，有一类是最常用，也是最先能被想到的召回方法，那便是内容语义召回。在实际的推荐系统中，我们经常需要根据语义进行推荐，比如在新闻推荐场景，用户点击了娱乐新闻中有关范冰冰的新闻，那么自然而然的也想看到李晨相关的新闻；在视频推荐场景，用户观看了庆余年视频，那么自然也会想看到最新的赘婿相关的视频。无论是范冰冰和李晨的新闻，还是庆余年和赘婿的视频，都是内容语义相似的。因此，基于内容语义相似的i2i召回，在推荐系统中被广泛应用。后面会依次介绍经典的内容语义模型。

#### 1.5.1 如何生成Item Embedding

我们都知道，内容语义模型，一般都是基于词来做的，最后生成的都是词的Embedding，词是最小的单元，比如经典的词向量模型Word2Vec。但是在i2i召回中，我们需要整个Item的Embedding，比如新闻推荐场景，我们需要标题或正文的Embedding。

- 常用的方法是==对词的Embedding做某种加权平均，得到整个Item的Embedding==，比如按TF-IDF加权，或者直接求平均值。
- 另外还有一种方法，就是==将内容语义模型输入的词序列扩展为用户的行为序列==，最后可以直接得到行为序列Item的Embedding，这个会在后面的1.5.5节提到。

#### 1.5.2 Word2vec——经典的词向量方法

Word2vec是Google2013年在论文*Efficient Estimation of Word Representations in Vector Space*中提出的语言模型，用于生成词的Embedding。==Word2vec有两种模型结构，CBOW是给定周围词预测中心词，而Skip-gram是给定中心词预测周围词==。比如句子“we soon believe what he desire”，如果中心词是"believe"，窗口大小是2，则CBOW是用"we","soon","what","he"来预测"believe"；而Skip-gram是用中心词"believe"来预测"we","soon","what","he"。图1是CBOW和Skip-gram的模型结构。

<img src="D:\Notes\raw_images\image-20210529115600971.png" alt="image-20210529115600971" style="zoom:67%;" />

​																							图1 Word2vec的两种模型结构

图1是Word2vec的两种模型结构，可以看出==CBOW和Skip-gram都是包含1个隐藏层的神经网络，模型结构略有不同==。后面分别介绍下CBOW和Skip-gram，分别从训练语料构建、模型结构、模型训练、负采样4个方面介绍。

**1. Skip-gram**

**（1）Skip-gram语料构建**

还是以前面的“we soon believe what he desire”为例，滑动窗口大小 $C=2$ 。

![img](https://pic1.zhimg.com/80/v2-1777c2ab9a221367e635c8d9552e029c_720w.jpg)

​																				图2 Skip-gram训练语料生成

图2是Skip-gram训练语料的生成过程，Skip-gram是一个多分类的模型，当中心词是believe时，应该使得"We","soon","what","we"出现概率最大，即$p\left( We,soon,what,we \right|believe)$最大化。

**（2）Skip-gram模型结构**

图3是Skip-gram模型结构，输入词"believe"先经过一个隐藏层，得到其隐向量$\textbf{v}_{wI}$，然后再和矩阵$\textbf{W}_{N\times V}'$做乘法，最后套上一个softmax，得到所有词的输出概率。

<img src="D:\Notes\raw_images\image-20210529115653732.png" alt="image-20210529115653732" style="zoom:80%;" />

​																						图3 Skip-gram模型结构

根据图3的模型结构，我们可以得出，给定输入词$ w_I $的词向量 $\textbf{x}=\left[ 0,0,...,1,...,0 \right] $，输出其它有词的概率为：
$$
\begin{align} p\left( .|w_I \right)&=softmax\left( \textbf{x}\cdot \textbf{W}\cdot \textbf{W}' \right) \\ &=softmax\left( \textbf{v}_{w_I} \cdot \textbf{W}'  \right) \\ &=softmax\left(   \textbf{v}_{w_I} \cdot \left[ \textbf{v}'_{w_{1}} {}^T, \textbf{v}'_{w_{2}} {}^T,...,\textbf{v}'_{w_{V}} {}^T\right] \right)\\ &=softmax\left( \left[  \textbf{v}_{w_I} \cdot \textbf{v}'_{w_{1}} {}^T,  \textbf{v}_{w_I} \cdot \textbf{v}'_{w_{2}} {}^T,..., \textbf{v}_{w_I} \cdot \textbf{v}'_{w_{V}} {}^T\right] \right)\\ \end{align} \tag1
$$
于是，可以得到输出 $w_j $ 的概率为：
$$
p\left( w_j|w_I \right)=\frac{exp\left( \textbf{v}_{w_I}\textbf{v}'_{w_j} {}^T \right)}{\sum_{j'=1}^{V}{exp\left( \textbf{v}_{w_I} \textbf{v}'_{w_{j'}} {}^T \right)}}\tag2
$$
这里举一个例子说明图3模型结构是如何根据输入词$w_I $，预测输出词 $w_j $的概率。就拿计算 $p\left( soon\right|believe) $ 为例，如图4所示。

![img](https://pic1.zhimg.com/80/v2-c7093b9a90e5e7d604b2c31bdb59f584_720w.jpg)

​																				图4 Skip-gram根据输入词预测输出词概率

**（3）Skip-gram模型训练**

==Skip-gram的目标是给定中心词，最大化上下文出现的概率==，也就是最大化 $p\left( w_{O,1},w_{O,2},...,w_{O,C}|w_I \right) $，基于极大似然估计，损失函数可以表示为：
$$
\begin{align} E&=-\text{log }p\left( w_{O,1},w_{O,2},...,w_{O,C}|w_I \right)\\ &= -\text{log }\prod_{c=1}^{C}p\left( w_{O,c}|w_I  \right)\\ &=-\text{log }\prod_{c=1}^{C}\frac{exp\left( \textbf{v}_{w_I}\textbf{v}'_{w_{O,c}} {}^T \right)}{\sum_{j'=1}^{V}{exp\left( \textbf{v}_{w_I} \textbf{v}'_{w_{j'}} {}^T \right)}} \\ &=-\text{log }\sum_{c=1}^{C}{\textbf{v}_{w_I}\textbf{v}'_{w_{O,c}} {}^T} +C\cdot \text{log }\sum_{j'=1}^{V}{exp\left(  \textbf{v}_{w_I} \textbf{v}'_{w_{j'}} {}^T \right)}\\ \end{align} \tag3
$$
给定损失函数后，可以使用梯度下降进行模型训练。

**（4）负采样**

前面给出了Skip-gram的训练方法，但事实上，完全按照上面的方法进行训练并不可行。假设语料库中词的数量为10000，隐藏层神经元个数为300，则每次训练都需要对10000个词计算输出概率，并且对 $300\times10000+300$ 个参数计算梯度，在实际训练中计算开销太大。

为了减轻Skip-gram的训练压力，往往采用负采样的方法进行训练。相比于原来需要计算所有词的预测误差，负采样方法只需要对采样出的几个负样本计算预测误差。这样一来，==Skip-gram模型的优化目标从一个多分类问题转化为一个近似二分类问题==，模型结构变成如图5的形式。

<img src="https://pic3.zhimg.com/80/v2-0c013f5e63ec9e4b91c2611489376a5a_720w.jpg" alt="img" style="zoom:67%;" />

​																								图5 Skip-graim负采样模型结构

在图5中， $w_{I} $是中心词，经过隐藏层后输出隐向量为$ \textbf{v}_{w_I}  $； $w_{j} $是上下文词（即正样本）和负采样词构成的集合，经过隐藏层后输出隐向量为$ \textbf{v}'_{w_{j}} $。假设 $w_O\in \mathcal{W}_C $是正样本， $w_N\in \mathcal{W}_{neg} $是负样本，则损失函数可以表示为：

$$E=-\sum_{w_O\in\mathcal{W}_C}{}\text{log }\sigma\left( \textbf{v}_{w_I} \textbf{v}'_{w_{O}} {}^T \right)-\sum_{w_N\in\mathcal{W}_{neg}}{}\text{log }\sigma\left(- \textbf{v}_{w_I} \textbf{v}'_{w_{N}} {}^T \right)\tag4$$

其中$ \sigma $是sigmoid函数。

**2. CBOW**

前面提到Skip-gram是根据中心词预测周边词，而CBOW是根据周边词预测中心词，即最大化 ![[公式]](https://www.zhihu.com/equation?tex=p%5Cleft%28w_I%7C+w_%7BO%2C1%7D%2Cw_%7BO%2C2%7D%2C...%2Cw_%7BO%2CC%7D+%5Cright%29) 。前面介绍了Skip-gram的样本构建，CBOW与之差不多，只是输入的是周边词，输出的是中心词，后面主要介绍下CBOW的模型结构、模型训练以及负采样。

**（1）CBOW模型结构**

<img src="D:\Notes\raw_images\image-20210529115806282.png" alt="image-20210529115806282" style="zoom: 67%;" />

​																							图6 CBOW模型结构

图6是CBOW模型结构，显示了如何根据周围词得到中心词（"believe"）和其它词的输出概率。周围词（图中的"we","soon"等）先经过一个隐藏层，得到其隐向量$ \textbf{v}_{wI,1},\textbf{v}_{wI,2},...,\textbf{v}_{wI,C} $，然后求均值得到 $\textbf{h}=\frac{\textbf{v}_{wI,1}+\textbf{v}_{wI,2}+...+\textbf{v}_{wI,C}}{C} $，再和矩阵$ \textbf{W}_{N\times V}' $做乘法，最后套上一个softmax，得到所有词的输出概率。

根据图6的模型结构，我们可以得出，给定周围词，经过隐藏层和Avg pooling后，得到隐向量 $\textbf{h} $，输出其它有词的概率为：
$$
\begin{align} &p\left( .|w_{I,1},w_{I,2},...,w_{I,C} \right)\\ &=softmax\left( \textbf{h}\cdot \textbf{W}' \right) \\ &=softmax\left(   \textbf{h} \cdot \left[ \textbf{v}'_{w_{1}} {}^T, \textbf{v}'_{w_{2}} {}^T,...,\textbf{v}'_{w_{V}} {}^T\right] \right)\\ &=softmax\left( \left[  \textbf{h} \cdot \textbf{v}'_{w_{1}} {}^T,  \textbf{h} \cdot \textbf{v}'_{w_{2}} {}^T,..., \textbf{h}\cdot \textbf{v}'_{w_{V}} {}^T\right] \right)\\ \end{align} \tag5
$$
于是，可以得到输出 $w_j $的概率为：
$$
p\left( w_j|w_{I,1},w_{I,2},...,w_{I,C} \right)=\frac{exp\left( \textbf{h}\cdot \textbf{v}'_{w_j} {}^T \right)}{\sum_{j'=1}^{V}{exp\left( \textbf{h}\cdot  \textbf{v}'_{w_{j'}} {}^T \right)}}\tag6
$$
这里举一个例子说明图6模型结构是如何根据周围词，预测输出词 $w_j $的概率。就拿计算 $p\left( believe\right|we,soon,what,he,desire) $为例，如图7所示。

![img](https://pic4.zhimg.com/80/v2-e21f2bd8a826e796155316900626ae43_720w.jpg)

​																				图7 CBOW根据周围词预测输出词概率

**（2）CBOW模型训练**

CBOW的目标是给定周围词，最大化中心词出现的概率，也就是最大化 $p\left( w_{O}|w_{I,1},w_{I,2},...,w_{I,C} \right) $，基于极大似然估计，损失函数可以表示为：
$$
\begin{align} E&=-\text{log }p\left( w_{O}|w_{I,1},w_{I,2},...,w_{I,C} \right) \\ &=-\text{log }\frac{exp\left( \textbf{h}\cdot \textbf{v}'_{w_{O}} {}^T \right)}{\sum_{j'=1}^{V}{exp\left( \textbf{h}\cdot \textbf{v}'_{w_{j'}} {}^T \right)}} \\ &=-{\textbf{h}\cdot \textbf{v}'_{w_{O}} {}^T} +\text{log }\sum_{j'=1}^{V}{exp\left(  \textbf{h}\cdot \textbf{v}'_{w_{j'}} {}^T \right)}\\ \end{align} \tag7
$$
给定损失函数后，可以使用梯度下降进行模型训练。

**（3）负采样**

和前面的Skip-gram一样，CBOW也可以使用负采样降低计算开销，将多分类问题转化为近似的二分类问题。

<img src="D:\Notes\raw_images\image-20210529115940480.png" alt="image-20210529115940480" style="zoom: 67%;" />

​																			图8 CBOW负采样模型结构

在图8中， $w_{I,1},w_{I,1},...,w_{I,C} $ 是周围词，经过隐藏层和Avg pooling后输出隐向量为 $\textbf{h} $；$ w_{j} $是中心词（即正样本）和负采样词构成的集合，经过隐藏层后输出隐向量为 $\textbf{v}'_{w_{j}} $ 。假设$ w_N\in \mathcal{W}_{neg}$ 是负样本，则损失函数可以表示为：
$$
E=-\text{log }\sigma\left( \textbf{h}\cdot \textbf{v}'_{w_{O}} {}^T \right)-\sum_{w_N\in\mathcal{W}_{neg}}{}\text{log }\sigma\left(- \textbf{h}\cdot \textbf{v}'_{w_{N}} {}^T \right)\tag4
$$
由于负采样集合的大小非常有限，在每轮梯度下降过程中，计算复杂度将大大降低。
在获得输入向量矩阵$ \textbf{W}_{V\times N} $后，其中每一行对应的权重向量就是词的Embedding。由于所有词都是one-hot，因此向量矩阵可以转换成Word2vec的查找表，如图9所示。

<img src="https://pic1.zhimg.com/80/v2-ea85100cb166d1b5ac7bba9c3a831a0c_720w.jpg" alt="img" style="zoom:80%;" />

​																						图9 Word2vec查找表

图9是Word2vec词向量的获取方式，但是我们知道，Word2vec还有一个输出词向量矩阵，那为什么输入和输出不用同一个词向量矩阵？在斯坦福NLP课程CS224n第二讲有关Word2vec中有提到，主要原因是有两个向量的话数学上更简单，两个向量在优化时相互独立，优化的时候也更容易操作。最后可以使用其中一个向量或者取二者平均值使用。

Word2vec的提出使得Embedding这一研究话题成为热点，另一方面，在Word2vec的研究中提出的模型结果、目标函数、负采样方法，在后续的模型中被反复使用。

#### 1.5.3 FastText——字符级别n-gram

FastText是2016年Facebook提出的文本分类模型，Embedding是FastText分类的副产物。这里主要从输入特征表示、模型结构、词向量生成3个方面介绍一下FastText。

**（1）FastText输入特征表示**

FastText的输入是整个文本的词序列，同时在表示单个词Embedding的时候，引入了==单个词的n-gram特征==，最后词的Embedding就可以用==n-gram向量的均值表示==。这里介绍下n-gram特征是如何生成的。

==Word2vec把语料中的每个单词作为最小单元，最后每个单词生成一个Embedding。这忽略了单词内部的结构，比如"china" 和"chines"，两个单词有很多公共字符，它们的内部结构相似。但是在传统的Word2vec中，这种单词内部结构信息因为它们被转换成不同的id丢失了==。为了克服这个问题，FastText使用了字符级别的n-grams来表示一个单词。对于单词"where"，假设n的取值为3，则n-grams表示为：

<wh, whe, her, ere, re>

其中，<表示前缀，>表示后缀。于是，我们可以用这些字符来表示"where"，进一步，我们可以用这5个字符的向量叠加来表示"where"的词向量。同时，FastText保留了整个单词<where>，需要注意的是<her>表示的是单词"her"，而her表示的是where中的tri-gram。

有了单个词的n-gram特征表示，那么词的Embedding就可以用n-gram向量的均值表示。比如上面提到的词"where"，输入特征为"<wh","whe","her","ere","re>","<where>"，经过one-hot编码后，然后乘以权重矩阵得到$\textbf{v}_{1},\textbf{v}_{2},\textbf{v}_{3},\textbf{v}_{4},\textbf{v}_{5},\textbf{v}_{6}$，那么词"where"的Embedding就可以表示为$\frac{\textbf{v}_{1}+\textbf{v}_{2}+\textbf{v}_{3}+\textbf{v}_{4}+\textbf{v}_{5}+\textbf{v}_{6}}{6}$。

**（2）FastText模型结构**

上面介绍了FastText每个词的Embedding表示，由n-gram特征生成。FastText的模型结构和Word2vec的CBOW非常相似，如图10所示。

![v2-5a8cadb879034bdfc922084bf3d1ad3b_r](D:\Notes\raw_images\v2-5a8cadb879034bdfc922084bf3d1ad3b_r.jpg)

​																						图10 FastText模型结构

从图10可以看出，FastText和CBOW非常相似，都是由输入层、隐藏层、输出层构成，只是在输入层词向量的表示，以及输出层目标上不一样，总结成下表的内容。

|        | FastText模型             | CBOW模型              |
| ------ | ------------------------ | --------------------- |
| 输入层 | 文本所有词、词n-gram特征 | 中心词的上下文        |
| 隐藏层 | 词的Embedding加权平均    | 词的Embedding加权平均 |
| 输出层 | 输出是文本类别           | 输出是中心词          |

**（3）FastText词向量生成**

FastText的词向量由n-gram特征向量的均值得到，和Word2vec相比，这样做有两个优点：

- 对于低频词生成的词向量效果会更好，因为它们的n-gram可以和其它词共享。
- 对于训练词库之外的单词，仍然可以构建它们的词向量。我们可以叠加它们的字符级n-gram向量。

#### 1.5.4 Bert——动态词向量方法

在介绍Bert之前，先直观上解释下为什么Bert是动态词向量方法。假设有两个句子"I have an apple", "I have an apple phone"，把这两个句子分别作为Bert的input，==由于Bert的每个词会与上下文做复杂的Attention计算==，==那么两个句子中"apple"对应的Embedding是不一样的，即Bert是动态词向量方法==。而如果是Word2vec，由于训练好了lookup table，那么上面两个句子中"apple"的Embedding只需要查询lookup table就可以了，==因此Word2vec是静态词向量方法==。

Bert（Bidirectional Encoder Representations from Transformers）是2018年Google AI团队提出的**预训练语言模型**，在11项NLP任务上刷新了最好指标，可以说是近年来NLP领域取得的最重大的进展之一，BERT论文也斩获NLP领域顶会NAACL 2019的最佳论文奖。Bert提出的同年，业界还提出了包括基于RNN的ELMo和ULMFiT，基于Transformer的OpenAI GPT，当然这里面最引人注目的还是Bert。预训练语言模型的成功，证明了我们可以从海量的无标注文本中学到潜在的语义信息，而无需为每一项下游NLP任务单独标注大量训练数据。此外，预训练语言模型的成功也开创了NLP研究的新范式，即==首先使用大量无监督语料进行语言模型预训练（Pre-training），再使用少量标注语料进行微调（Fine-tuning）来完成具体NLP任务（比如分类、序列标注、句间关系判断和机器阅读理解等）==。目前，大部分NLP深度学习任务中都会使用预训练好的词向量（如Word2vec）进行网络初始化（而非随机初始化），从而加快网络的收敛速度。

前面提到的Word2vec也可以作为预训练语言模型，但是Word2vec生成的词向量对上下文信息考虑不足，无法处理一词多义问题。比如前面提到的"apple"一词，在不同语境中可能表示“苹果”，也可能表示“苹果手机”，但是Word2vec却对应相同的词向量。Bert的提出，极大程度上解决了这个问题，后面将从模型结构、输入表示、预训练目标、Fine-tuning、动态词向量获取5个方面介绍Bert。

**（1）Bert模型结构**

BERT是==基于Transformer的深度双向语言表征模型==，基本结构如图11所示，本质上是利用Transformer结构构造了一个多层双向的Encoder网络。

![img](https://pic4.zhimg.com/80/v2-7abd7fc552f7dbd696d69e1068415347_720w.jpg)

​																									图11 Bert模型结构

**（2）Bert输入表示**

针对不同的任务，BERT模型的输入可以是单句或者句对。对于每一个输入的Token，它的表征由其对应的词表征（Token Embedding）、段表征（Segment Embedding）和位置表征（Position Embedding）相加产生，如图12所示。

![img](https://pic1.zhimg.com/80/v2-f85716fc57fa8aa1fe2a9ff6d40fe54c_720w.jpg)

​																								图12 Bert模型输入表示

图12中的**[CLS]表示起始token，对应最终的Hidden State（即Transformer的输出），可以用来表征整个句子，用于下游的分类任务**，如果没有分类任务，可以忽略此向量。[SEP]用于区分两个句子。

**（3）Bert预训练目标**

BERT预训练过程包含两个不同的预训练任务，分别是**MLM**（Masked Language Model）和 **NSP**（Next Sentence Prediction）。

**MLM（Masked Language Model）**

==MLM是通过随机掩盖一些词（替换为统一标记符[MASK]），然后通过预测这些被遮盖的词来训练双向语言模型，并且使每个词的表征参考上下文信息==。这样做会产生两个缺点：

- 造成预训练和微调时的不一致，因为在微调时[MASK]总是不可见的。
- 由于每个Batch中只有15%的词会被预测，因此模型的收敛速度比起单向的语言模型会慢，训练花费的时间会更长。

对于第一个缺点的解决办法是：把80%需要被替换成[MASK]的词进行替换，10%的随机替换为其他词，10%保留原词。比如执行下面的过程：

80％的时间：用[MASK]标记替换单词，例如：I have an apple → I have an [MASK]

10％的时间：用一个随机的单词替换该单词，例如：I have an apple → I have an egg

10％的时间：保持单词不变，例如：I have an apple → I have an apple

==由于Transformer Encoder并不知道哪个词需要被预测，哪个词是被随机替换的，这样就强迫每个词的表达需要参照上下文信息==。

**NSP（Next Sentence Prediction）**

为了训练一个理解句子间关系的模型（如问答QA和自然语言推理NLI），引入一个下一句预测任务。这一任务的训练语料可以从语料库中抽取句子对，包括两个句子A和B来进行生成，其中50%的概率B是A的下一个句子，50%的概率B是语料中的一个随机句子。NSP任务预测B是否是A的下一句。

**（4）Bert Fine-tuning（微调）**

对于下游分类任务，Bert直接取第一个[CLS]最终的输出向量，作为下游分类任务的输入，最后经过softmax预测输出类别。需要强调的一点是，Bert中的NSP任务就是一个分类任务。图13显示了Bert Fine-tuning用于分类任务，[CLS]最终输出的向量$C$用于下游分类任务的输入。

<img src="D:\Notes\raw_images\image-20210529140934623.png" alt="image-20210529140934623" style="zoom: 67%;" />

​																							图13 Bert Fine-tuning

**（5）Bert动态词向量获取**

图11最后Transformer输出 $T_1,T_2,..,T_N$ 便是我们需要的词向量，针对不同语境，同一个词的输出Embedding也不一样。最后，我们可以根据所有词Embedding的均值作为整个文本的Embedding。

#### 1.5.5 内容语义模型扩展为行为序列模型

Word2vec和Bert可以对“词序列”中的词进行Embedding，那么对于用户点击“新闻序列”中的一条新闻，用户“观看序列”中的一部电影，也可以使用Word2vec或者Bert生成对应的Embedding。这里举两个例子，在新闻推荐场景，分别基于Word2vec和Bert生成新闻Embedding。

**（1）Item2vec：基于Word2vec生成新闻Embedding**

在Word2vec诞生之后，Embedding的思想迅速从自然语言处理领域扩展到推荐系统等其它领域。基于Word2vec的原理，微软于2016年提出了计算Item Embedding向量的方法Item2vec（这里的Item可以是新闻、物品、视频等）。

相比Word2vec利用“词序列”生成词的Embedding，Item2vec利用“新闻点击序列”生成新闻的Embedding。假设Word2vec中一个长度为 T 的句子为 $w_1,w_2,...,w_T $，则其优化目标为：
$$
\frac{1}{T}\sum_{t=1}^{T}{\sum_{-c\leq j\leq c,j\ne0}{\text{log }p\left( w_{t+j}|w_t \right)}}\tag5
$$
假设Item2vec中一个长度为 K 的用户历史点击序列为$ item_1,item_2,..,item_K$ ，则Item2vec的优化目标为：
$$
\frac{1}{K}\sum_{i=1}^{K}{\sum_{1\leq j\leq K,j\ne i}{\text{log }p\left( item_{j}|item_i \right)}}\tag6
$$
通过观察公式（5）和（6）的区别可以看出，Item2vec与Word2vec唯一区别在于，==Item2vec摒弃了时间窗口的概念，认为序列中任意两个Item都相关==。剩下的训练过程和最终Item Embedding的生成和Word2vec完全一样。

**（2）Bert生成新闻Embedding**

将Bert应用于新闻推荐场景，只考虑MLM任务，则输入可以表示为图14所示。

<img src="https://pic4.zhimg.com/80/v2-1d4095385051d970ed1054c869bbedcb_720w.jpg" alt="img" style="zoom: 33%;" />

​																				图14 Bert用户新闻推荐的输入表示

图14中的Item Embedding对应Bert中的Token Embedding，Session Embedding对应Segment Embedding。可以看出，有两点和标准Bert不一样：

- 图14中并没有Position Embedding**，**这主要是因为用户的点击序列不会有重复的Item，因此加上Position Embedding，效果上相当于重复加上了Item Embedding。
- 另外可以发现，没有像标准Bert那样，在第一个位置加上标记[CLS]，主要是因为仅考虑了MLM任务。

除了输入表示和标准的Bert有所不同，剩下的MLM采样方法、模型结构与标准的Bert完全一样。

### 召回部分【2】总结

本文主要介绍了内容语义模型在召回中的应用。重点介绍了Word2vec和Bert的基本原理，后面很多词向量模型都是基于Word2vec和Bert演化而来，而Bert预训练模型在很多场景都应用广泛。Word2vec和Bert针对词序列的Embedding，自然而然的可以扩展到推荐场景的点击序列，在我们自己的业务场景，基于Word2vec和Bert的召回在线上都取得了不错效果，最近上线的使用Bert生成点击新闻Embedding，作为召回服务在线上取得了不错的效果。

但是Word2vec和Bert都只考虑了序列关系，而没有考虑复杂的图结构信息。在推荐场景，数据对象之间呈现的更多是图结构信息，在后面的召回部分【3】中，将介绍Graph Embedding在召回中的应用。

https://zhuanlan.zhihu.com/p/354859728

### 1.7.1 DSSM——经典的双塔召回模型

DSSM（Deep Structured Semantic Models ，深度语义模型）是2013年微软发表的一篇论文，本用于语义匹配，后被移植到推荐系统等各个场景，成为经典的双塔模型。

<img src="https://pic4.zhimg.com/80/v2-a5dd690600b0cff04f30e6576fdad7b7_720w.jpg" alt="img" style="zoom: 33%;" />

​																						图1 DSSM变种（双塔模型）

DSSM双塔结构，两侧分别输入user特征和ad特征，经过DNN变换后分别产出user向量和ad向量。DSSM最大的特点是user侧和ad侧是独立的两个子网络，可以离线产出user embedding和ad embedding，召回时只需要计算二者的相似度。

### 1.7.2 YouTube深度学习召回方法

2016年，YouTube发表了深度学习推荐系统论文*Deep Neural Networks for YouTube Recommendations*，是一篇理论和实践俱佳的论文，论文提到了如何从百万量级视频中快速召回几百个视频，同时保证召回的效果。

<img src="https://pic2.zhimg.com/80/v2-44a7baacddada2a044bc3c13ee5ca5b1_720w.jpg" alt="img" style="zoom: 50%;" />

​																							图2 YouTube召回模型架构

图2是YouTube召回模型的架构，这里主要从特征输入、模型训练、如何获取用户/视频Embedding、线上召回服务四个方面介绍下实现细节。

**1）特征输入**

输入特征包括用户历史观看视频的Embedding、历史搜索词的Embedding、地理位置信息以及年龄和性别。为了生成视频Embedding和搜索词Embedding，利用==用户的视频观看序列和搜索序列，采用Word2vec方法==。

**2）模型训练**

YouTube的召回模型在预测下一次观看哪个视频的场景中，将其转化为一个多分类问题，每一个候选视频都会是一个分类，因此总共的分类就有几百万，使用softmax对其进行训练就变得非常低效。为了解决这个问题，==YouTube采用了Word2vec中的负采样方法，减少每次训练的分类数量，加快模型的收敛速度==。

**3）如何获取用户/视频Embedding**

如图2所示，用户的Embedding就是最后一层ReLU层的输出向量。视频的Embedding就是softmax之前的参数矩阵，假设用户Embedding维度为 $m$ ，视频数量为 $n$，那么参数矩阵的维度就是 $m\times n$ ，参数矩阵的列向量就是视频的Embedding。

**4）线上召回服务**

图2左上角是YouTube采用的线上召回服务方法，模型训练好后，将视频Embedding存储到redis等数据库中，线上召回时，根据用户Embedding进行相似度检索，获取Top n视频返回给用户。==这里，用户Embedding获取有两种方法，一种是输入用户特征调用召回模型，模型实时返回用户Embedding；另外一种是将用户Embedding也存储到redis中，用户行为发生改变再更新用户Embedding==。由于用户行为时刻在发生改变，因此一般会直接调用模型服务，实时返回用户Embedding。

这里讲一下，上面讲的Embedding放到redis，只是存储方法。线上相似度检索并不是直接从几百万Embedding中计算相似度，一般都是基于ANN检索（近似最邻近检索），比如使用Facebook的向量检索工具Faiss，离线构建索引，然后计算每个用户最邻居的K个Item，最后再把每个用户最邻近的K个Item存储到redis中，线上直接查找K个结果就可以。在我们自己业务场景，1000万的用户，50万条数据，Embedding维度为100，用20台机器使用Faiss构建索引，每台机器8核16G，最后生成1000万个用户100个候选结果一共只需要20分钟左右，最后存储到redis中，线上直接查找返回100个召回结果。

### 1.7.3 Airbnb基于用户短期和长期兴趣的Embedding召回

2018年8月，Airbnb在KDD发表了论文*Real-time Personalization using Embeddings for Search Ranking at Airbnb*，该论文获得了KDD2018最佳论文奖。论文中，Airbnb对于Embedding技术的应用非常巧妙，

- 比如在点击序列中加入用户预定的房源信息，使得搜索结果倾向于之前预定的房源；
- 为了捕捉长期兴趣，引入了时间更长的预定序列，
- 而为了解决预定序列稀疏问题，又对用户和房源进行分组。

这一系列优化方法，都是基于实际业务和数据特点，有很强的借鉴意义。后面主要从Airbnb业务场景、基于短期兴趣的房源Embedding、基于长期兴趣的房源Embedding、召回效果评估、Embedding特征在实时搜索中的应用，5个方面介绍Airbnb的具体实践。

**1）Airbnb业务场景**

Airbnb是全球知名的短租平台，包含400多万房源，分布在190多个国家，每天有超过200万用户访问Airbnb，其主要功能是连接房主和租客。Airbnb的典型应用是用户在搜索框输入地点、时间、价格等信息，Airbnb返回房源的推荐列表，如图1所示。

![img](https://pic1.zhimg.com/80/v2-0c00665cba13a3aba749c4df314e31e4_720w.jpg)

​																						图1 Airbnb搜索场景

在根据用户搜索返回推荐结果后，用户和房主之间的交互如图2所示。用户在点击房源后（Click），然后发出预定请求（Booking Request），房主有可能拒绝（Reject）、同意（Accept）或者不响应（No Response）。

![img](https://pic1.zhimg.com/80/v2-c17a69bbabb6c3cf4d7abba83fd7d570_720w.jpg)

​																					图2 Airbnb中用户的不同交互方式

Airbnb主要基于点击序列和预定序列，生成房源的Embedding，然后用于召回或排序。在Embedding方法上，Airbnb基于Word2vec原理，生成了两种不同的Embedding，==分别用于表达用户的短期兴趣和长期兴趣==。短期兴趣Embedding可以用于相似房源推荐，长期兴趣Embedding可以用于推荐用户之前预定的房源偏好。之所以要引入预定房源序列，是因为用户的预定是低频行为，而点击是高频行为，用户可能在一天就点击了很多房源，因此点击序列更多刻画的是短期兴趣，而用户在一年可能只预定过几次，因此预定行为反应的是长期兴趣。

**2）基于短期兴趣的房源Embedding**

Airbnb利用用户点击session对房源进行Embedding，捕捉用户在一次搜索过程中的短期兴趣。session的定义如图3所示，只保留点击后停留时间超过30秒的点击，30分钟没有行为就断开session。

<img src="https://pic1.zhimg.com/80/v2-b8c2ffd09fd513c142c467db51bc02d4_720w.jpg" alt="img" style="zoom:50%;" />

​																						图3 Airbnb session定义

有了session的定义，就可以使用Word2vec中的Skip-gram模型训练房源Embedding，优化目标定义如下：
$$
\mathop{\text{argmax}} \limits_{\theta}\sum_{\left( l,c \right)\in \mathcal{D}_p}{\text{log}\frac{1}{1+e^{-\text{v}'_{c}\text{v}_l}}}+\sum_{\left( l,c \right)\in \mathcal{D}_n}{\text{log}\frac{1}{1+e^{\text{v}'_{c}\text{v}_l}}}\tag1
$$
其中 $l $ 是session内的中心房源， $c$ 是滑动窗口内的房源， $\mathcal{D}_p $ 是正样本集合，$ \mathcal{D}_n $是负样本集合。

Airbnb为了使推荐结果倾向于用户预定的房源，在生成房源Embedding过程中引入了预定房源信息。将点击session分成两类：

- booked session：用户最终产生预定行为的点击session
- exploratory sessions：用户最终没有产生预定行为的点击session

==在booked session中，只有最后一个房源是被预定的，为了将这个预定行为引入目标函数，不管这个预定行为是否在Word2vec的滑动窗口内，都假设这个预定房源与滑动窗口的中心房源相关==，Skip-gram模型结构变成下图4的形式。

<img src="D:\Notes\raw_images\image-20210529141102642.png" alt="image-20210529141102642" style="zoom:67%;" />

​																	图4 skip-gram模型中引入预定房源信息

图4显示了将预定房源（booked listing）引入Skip-gram模型中，相当于增加了一个正样本，优化目标变成下面的形式：
$$
\mathop{\text{argmax}} \limits_{\theta}\sum_{\left( l,c \right)\in \mathcal{D}_p}{\text{log}\frac{1}{1+e^{-\text{v}'_{c}\text{v}_l}}}+\sum_{\left( l,c \right)\in \mathcal{D}_n}{\text{log}\frac{1}{1+e^{\text{v}'_{c}\text{v}_l}}}+\text{log}\frac{1}{1+e^{-\text{v}'_{l_b}\text{v}_l}}\tag2
$$
其中，最后一项 $l_b $代表被预定房源。
在Airbnb场景，用户总是喜欢搜索同一地区的房源，比如用户想去纽约，就会一直搜索纽约的房源，这样导致正样本集合 $\mathcal{D}_p $ 中的房源主要来自同一地区，而负样本集合  $\mathcal{D}_n$中的房源主要来自不同地区，这种不平衡导致一个地区内的相似性不是最优的，会出现并不相似的房源计算出的相似度也很高。为了解决这一问题，Airbnb新增了一组负样本，从与中心房源 $ l $ 同一地区的房源中进行随机采样。新的目标函数变成下面的形式：
$$
\mathop{\text{argmax}} \limits_{\theta}\sum_{\left( l,c \right)\in \mathcal{D}_p}{\text{log}\frac{1}{1+e^{-\text{v}'_{c}\text{v}_l}}}+\sum_{\left( l,c \right)\in \mathcal{D}_n}{\text{log}\frac{1}{1+e^{\text{v}'_{c}\text{v}_l}}}+\text{log}\frac{1}{1+e^{-\text{v}'_{l_b}\text{v}_l}}+\sum_{\left( l,m_n \right)\in \mathcal{D}_{m_n}}{\text{log}\frac{1}{1+e^{\text{v}'_{m_n}\text{v}_l}}}\tag3
$$
其中， $\mathcal{D}_{m_n} $指的是同一地区的负样本集合。

![img](https://pic1.zhimg.com/80/v2-4dc5f617d8f2dc7f1ba2d38cd0523bd8_720w.jpg)

​																							图5 相似房源结果

图5是根据得到的房源Embedding，计算出的相似房源。从图中可以看出，根据房源Embedding选出的相似房源在价格、类型、建筑风格上都很相似。

表1是不同房源类型的相似度，可以看到对角线上的相似度最大，也就是同类型房源的相似度更大。

![img](https://pic2.zhimg.com/80/v2-3f33db21a9af84945c45d53b051f6e8d_720w.jpg)

​																							表1 不同房源类型相似度

表2是不同价格区间房源的相似度，同样是对角线上的相似度最大，即相同价格区间的房源相似度更大。从表2还可以看出，价格区间越接近，相似度越大。

![img](https://pic1.zhimg.com/80/v2-401711414ea8d29e0d7a5b1776b3e378_720w.jpg)

​																					表2 不同价格区间房源相似度

**3）基于长期兴趣的房源Embedding**

前面基于点击session生成房源Embedding，虽然可以很好的找到相似房源，但是没有包含用户的长期兴趣。比如用户1个月之前预定的房源，在点击session里面就没有包含，从而丢失了用户的长期兴趣。

为了捕捉用户的长期兴趣，Airbnb使用了预定序列（booked session）。但是预定序列数据非常稀疏，大多数用户一年只有几次预定行为，甚至有些一年只预定过一个房源，这导致很多预定序列的长度为1。为了解决预定序列数据稀疏的问题，Airbnb对用户和房源进行分组，保证相同组的用户有相似的爱好，相同组的房源相似度高。

Airbnb主要基于规则对用户和房源进行分组，表3是对用户进行分组。从表3可以看出，主要基于用户的语言、设备类型、历史预定次数等信息进行分组。

![img](https://pic3.zhimg.com/80/v2-35c28dc0c97278ca27c86b6a2f772f5e_720w.jpg)

​																									表3 对用户分组

对房源分组也采用类似的规则，表4是对房源进行分组，从表4可以看出，主要基于房源所在国家、每晚价格等信息进行分组。

<img src="D:\Notes\raw_images\image-20210525180139022.png" alt="image-20210525180139022" style="zoom:80%;" />

​																										表4 对房源分组

有了用户分组和房源分组，就可以生成新的预定序列。使用$\left( \text{user_type},\text{listing_type} \right)$替换原来的listing，预定序列变成，$\left( \left( u_{type1} ,l_{type1} \right),\left( u_{type2} ,l_{type2} \right),...,\left( u_{typeM} ,l_{typeM} \right)\right)$ 由于用户的属性会发生变化，比如历史预定次数，因此$u_{type1},u_{type2}$不一定相同。

有了预定序列，接下来便可以使用Skip-gram训练Embedding，模型结果如图6所示。

<img src="D:\Notes\raw_images\image-20210525180114508.png" alt="image-20210525180114508" style="zoom:67%;" />

​															图6 Skip-gram生成user_type和listing_type Embedding

图6显示了中心词是user_type时的模型结构，从图中可以看出，Airbnb在训练user_type Embedding和listing_type Embedding时没有任何区分，把user_type和listing_type当作完全相同的词进行训练，这样可以保证用户Embedding和房源Embedding在相同空间，可以直接计算二者的相似度。

当中心词是$user\_type( \text{u}_t )$时，目标函数如下所示：
$$
\mathop{\text{argmax}} \limits_{\theta}\sum_{\left( \textbf{u}_t,\textbf{c} \right)\in \mathcal{D}_{\text{book}}}{\text{log}\frac{1}{1+e^{-\text{v}'_{c}\text{v}_{\textbf{u}_t}}}}+\sum_{\left( \textbf{u}_t,c \right)\in \mathcal{D}_{\text{neg}}}{\text{log}\frac{1}{1+e^{\text{v}'_{c}\text{v}_{\textbf{u}_t}}}}\tag4
$$
当中心词时$listing\_type( l_t )$时，目标函数如下：
$$
\mathop{\text{argmax}} \limits_{\theta}\sum_{\left( l_t,\textbf{c} \right)\in \mathcal{D}_{\text{book}}}{\text{log}\frac{1}{1+e^{-\text{v}'_{c}\text{v}_{l_t}}}}+\sum_{\left( l_t,c \right)\in \mathcal{D}_{\text{neg}}}{\text{log}\frac{1}{1+e^{\text{v}'_{c}\text{v}_{l_t}}}}\tag5
$$
其中 $ \mathcal{D}_{\text{book}} $ 是中心词附近用户分组（user_type）和房源分组（listing_type）的集合。由于预定序列中的房源大多来自不同地区，因此这里不再需要对相同地区房源进行负采样。

和点击行为只反映用户信息不同，预定行为还包含了房主的信息，有些房主在接到预定请求后，可能选择拒绝，原因可能是用户信息不全，或者用户信誉低。因此，在预定序列中，Airbnb引入了额外的房主拒绝的负样本集合 $ \mathcal{D}_{\text{reject}}$ ，模型结构如图7所示。

<img src="D:\Notes\raw_images\image-20210525180046473.png" alt="image-20210525180046473" style="zoom:67%;" />

​																				图7 预定序列引入房主拒绝负样本

图7显示了用户$user\_i$预定房源  $listing\_i$$, 被房主拒绝，模型结构中增加了该负样本: 

$\left( \text{user_type}_i,\text{listing_type}_i \right)$

引入额外负样本集合后，当中心词是$user\_type( \text{u}_t )$时，目标函数如下所示：
$$
\mathop{\text{argmax}} \limits_{\theta}\sum_{\left( \textbf{u}_t,\textbf{c} \right)\in \mathcal{D}_{\text{book}}}{\text{log}\frac{1}{1+e^{-\text{v}'_{c}\text{v}_{\textbf{u}_t}}}}+\sum_{\left( \textbf{u}_t,c \right)\in \mathcal{D}_{\text{neg}}}{\text{log}\frac{1}{1+e^{\text{v}'_{c}\text{v}_{\textbf{u}_t}}}}+\sum_{\left( \textbf{u}_t,l_t \right)\in \mathcal{D}_{\text{reject}}}{\text{log}\frac{1}{1+e^{\text{v}'_{l_t}\text{v}_{\textbf{u}_t}}}}\tag4
$$
当中心词时$listing\_type( l_t )$时，目标函数如下：
$$
\mathop{\text{argmax}} \limits_{\theta}\sum_{\left( l_t,\textbf{c} \right)\in \mathcal{D}_{\text{book}}}{\text{log}\frac{1}{1+e^{-\text{v}'_{c}\text{v}_{l_t}}}}+\sum_{\left( l_t,c \right)\in \mathcal{D}_{\text{neg}}}{\text{log}\frac{1}{1+e^{\text{v}'_{c}\text{v}_{l_t}}}}+\sum_{\left( l_t,\text{u}_t \right)\in \mathcal{D}_{\text{reject}}}{\text{log}\frac{1}{1+e^{\text{v}'_{\text{u}_t}\text{v}_{l_t}}}}\tag5
$$
**4）召回效果评估**

离线评估一直是召回的难题，Airbnb的方法是测试通过用户最近的点击来推荐房源，有多大可能最终会产生预定，使用这一方法来评估模型训练出的Embedding效果。

具体地，假设获得了用户最近点击的房源和候选房源集合，其中包含用户最终预定的房源，通过计算点击房源和候选房源的相似度，对候选房源进行排序，并观察预定房源在候选房源中的排序位置，如果排序位置越靠前，说明Embedding效果越好。

![img](https://pic2.zhimg.com/80/v2-5e0576979d4834ea1ce569b792f69641_720w.jpg)

​																		图8 离线评估房源Embedding效果

图8显示了评估结果，横坐标表示最近的17次点击，比如-2表示倒数第二次点击，纵坐标表示预定房源在排序中的位置，d32 reg表示序列中没有加预定房源，d32 book表示序列中引入预定房源，d32 book+neg表示引入了同一地区负样本集合。比如使用d32 book+neg计算倒数第2次点击（-2）和所有候选房源相似度，最终预定房源的平均位置在4.5，而d32 reg计算出的平均位置在4.9，因此book+neg的效果更好。

**5）Embedding特征在实时搜索中的应用**

在搜索排序阶段，Airbnb根据前面生成的房源Embedding、房源分组Embedding、用户分组Embedding，构建不同特征，然后输入搜索排序模型。主要构建了以下特征：

![img](https://pic1.zhimg.com/80/v2-0ce2212dd873afaa87f683493f2d1c2c_720w.jpg)

​																					表5 使用Embedding构建特征

为了评估上面特征的效果，Airbnb使用GBDT模型生成特征权重，用于评估特征重要度，如表6所示。

![img](https://pic2.zhimg.com/80/v2-d8a2b79f6c02f98c1e003fcba546d315_720w.jpg)

​																						表6 特征重要性排序

### 1.7.4 深度学习u2i召回总结

和前面几篇Embedding召回方法相比，本文主要总结了业界常用的深度学习召回方法，从经典的双塔模型，到YouTube生成用户Embedding和视频Embedding方法，最后是Airbnb结合自己业务改进的序列Embedding方法，这些方法都有很强的借鉴意义，这些实践经验都可以用于自己的业务场景。

