## The Performance and Energy Efficiency Potential of FPGAs in Scientific Computing

论文地址：https://ieeexplore.ieee.org/document/9307865

作者：Tan Nguyen; Samuel Williams; Marco Siracusa; Colin MacLean; Douglas Doerfler; Nicholas J. Wright

发表：2020 IEEE/ACM Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS)

日期：12-12 Nov. 2020

机构：Lawrence Berkeley National Laboratory



### 摘要

Hardware specialization硬件专业化是数字计算未来的一个有前途的方向。Reconfigurable technologies可重构技术使硬件专业化具有适度的非重复工程成本。在本文中, 我们使用FPGAs来评估在科学应用中建立（numerical kernels）数值内核的特殊硬件的优点。

为了适当地评价性能, 我们不仅将英特尔Arria和Xilinx U280的性能与英特尔Xeon、Intel Xeon Phi和NVIDIA V100 GPUs进行比较, 还将Empirical  Roofline Toolkit (ERT) 扩展到FPGAs, 以评估我们在Roofline Model上的结果。

虽然FPGA的性能比GPU要差得远得多, 但我们也基准（benchmark ）了每个平台的==能源效率==, 与微基准（microbenchmark ）和技术限制相比。结果表明, 尽管FPGAs很难在内存和计算密集型内核上与GPUs进行竞争,但它们需要的能量要小得多, 能够实现几乎相同的能源效率。

**总结：**ERT工具拓展到FPGAs, Roofline模型扩展到FPGAs，FPGAs在考虑能源效率的情况下，可以与GPUs相媲美。



### 介绍

**FPGA介绍：**与传统的冯·诺依曼（Von Neumann）指令处理器体系结构（包括CPU和GPU）不同，存储在存储器中的程序是指令序列，Field-Programmable Gate Arrays（FPGA）代表了独特的一类可重配置的空间体系结构，其中整个程序都是完整的在硬件中被实现为顺序逻辑电路（ sequential logic circuit）。因此，代替在时分复用功能单元上（time-multiplexed functional units）获取，解码和执行指令，可以通过在电路上发送操作以流水线方式在FPGA上执行操作。没有指令解码，注册文件或缓存。而是由称为LUTs（查找表）的可重配置逻辑块阵列构建FPGA，编译器对其进行配置和互连以形成顺序逻辑电路。较新的FPGA架构实例化了用于算术的强化功能单元，集成用于本地存储的寄存器和Block RAM（BRAM），包括集成的ARM内核和NIC，并使用了最新的HBM内存技术。最终，FPGA允许用户创建针对其程序中的每个计算内核优化的自定义体系结构（例如，使用FIFO代替高速缓存或3-element SIMD单元）。

在本文中，==我们从性能（performance），能效（energy efficiency）和可编程性（programmability）的角度探讨了HPC环境中FPGA的潜力==。为此，我们同时评估了==英特尔的Arria 10 GX1150==和==赛灵思（Xilinx’s ）的Alveo U280==。我们首先使用成熟的Roofline模型[3]研究峰值性能和效率与数据重用的关系，以建立对话框架。然后，我们检查==三个HPC内核==（SGEMM，SpMV和Smith-Waterman），它们涵盖了计算强度，并行性，同步要求和基础数据类型的范围。为了提供背景信息，我们将基础内存技术（理想的架构尽可能减少能源开销）与==现有的CPU和GPU架构== - 英特尔至强（Haswell），英特尔至强融核（Knights Landing）和 NVIDIA V100（Volta）GPU - 的能效进行了比较。 这凸显了DDR和MCDRAM / HBM内存技术以及多核，多核和GPU架构的价值。



### 实验设备

![1611714270765](D:\Notes\raw_images\1611714270765.png)

两个FPGA都集成了超过一百万个LUT和数千个硬化乘法器，但是由于其标称频率大大降低，因此其理论峰值性能低于GPU。

许多HPC应用程序的算术强度低（FLOP：字节比率），这对存储子系统有很高的要求。两种FPGA都包含50-66MiB的BRAM（远远超过典型的CPU或GPU缓存容量），从而允许软件定义的架构利用空间和时间局部性。  

Arria 10仅包含两个通道的DDR内存，==将其内存带宽限制为34GB/s==，这远远小于典型的CPU，并且比典型的GPU低约25倍。相反，U280包括DDR和HBM内存，后者提供的带宽比现代GPU的一半好。在本文中，我们将对这些架构进行基准测试，以确定其可达到的内存带宽和计算潜力。

为了提供背景信息并与现代架构进行比较，我们还在NERSC上对CPU和GPU系统进行了实验[25]。虽然CPU是基于DDR的，而GPU是基于HBM的，但我们也可以在NERSC的Knights Landing（KNL）系统上运行，以探索具有MCDRAM（类似HBM）存储技术的CPU内核。

V100（Volta）是NVIDIA的旗舰GPU。它包括5120个单精度FMA，它们以1.5GHz的频率运行，并具有超过800GB/s 的HBM内存带宽。尽管这提供了出色的峰值性能，但它是基于表达大量数据并行性的用户而设计的，并且==要求超过200W的功率。内存延迟通过大量的多线程隐藏。因此，性能和能效高度依赖于应用==。

NERSC的Cori系统包括两个分区。第一种使用双插槽Intel Xeon E5-2698 v3（Haswell）节点形式的常规CPU。每个节点包括32个以标称2.3GHz运行的内核，支持AVX2（总共512个FMA）。节点的8个DDR-3通道共同提供了理论上136GB/s的引脚带宽。

对于吞吐量密集型应用，英特尔至强融核7250（Knights Landing）可提供Cori的大部分性能。每个==多核处理器包括68个核==，每个核带有==两个以1.4GHz运行的AVX-512 vector units==（2176 32b FMA）。每个单个套接字节点都会实例化16GiB的MCDRAM内存，从而提供超过460GB/s的内存带宽。与V100 GPU以及在较小程度上与Haswell CPU一样，要在KNL处理器上达到最佳性能取决于大量的数据并行性。但是，与GPU不同，KNL和Haswell都利用==硬件流预取==器来隐藏内存延迟。

### FPGA MICROBENCHMARKS  

在本文中，我们沿两个轴对FPGA进行了表征：性能 随 时间局部性 temporal locality（算术强度）而变，带宽随空间局部性而变。前者使我们能够评估FPGA软件堆栈在流水线中识别和重用的程度，以及指令处理器可以通过硬件缓存重用的程度。后者告诉我们FPGA软件堆栈可以在不同的存储器访问模式下充分利用存储器子系统。

**A. Temporal Locality (Roofline)**

==仅展示在Arria 10上获得的结果==

在这项工作中，我们将Empirical  Roofline Tool(ERT) 移植到OpenCL，并允许用户选择数据类型(float, int32, int8，等等…)。ERT的前提是==对向量的每个元素求任意次多项式==。==随着多项式次数的增加，算术强度也随之增加(more FLOPs per byte)==。当一个改变向量大小，一个改变缓存工作集，并量化缓存层次结构中的带宽缩减。当改变数据类型时，就会改变内存访问的自然量程。

由于FPGA没有硬件高速缓存层次结构，因此除非将数据明确放置在BRAM中，否则我们不会观察到减小vector大小的任何好处。此外，尽管CPU高速缓存层次结构在quanta cache lines的数量上控制着DRAM和SRAM数据的移动，但FPGA提供了对DRAM的自然宽度访问，直至硬件控制器的极限，而BRAM控制器则通过多种配置选项进行综合。==尽管可以启动多个到DRAM的事务处理，但没有高速缓存，仅返回给定时钟周期内所需的数据（相邻的行内数据被丢弃）==。最终，对空间局部性的利用是 综合 frequency，数据并行性，内存访问模式的编译时知识， 编译器合并突发加载存储单元 等能力之间的良好平衡。因此，==必须对系统进行基准测试以量化可达到的带宽==。

图1（a）突出显示，与CPU和GPU不同，未经调整的==FPGA带宽可以在0.6（int8）到5GB / s（double / int64）之间变化-远远小于理论内存带宽34GB/s==。通过将测得的带宽除以大约312MHz的内核时钟频率，可以看出这是一个管道瓶颈。 int8的大约2个字节，double和int64的大约16个字节恰好是每个时钟周期读取和写入的数据量。与其中硬件专用于带宽或计算的指令处理器不同，==FPGA上的资源是可互换的==。结果，我们根据可用的FPGA资源和算术强度定义了一组==编译器衍生的理论Roofline上限==。

优化的两种主要方法可以提高带宽。首先，与以相对恒定的频率运行的CPU和GPU不同，==FPGA频率高度依赖于编译器==。通过使用==__fpga_reg（）内在函数，我们可以将带宽增加34％==，以便利用多项式内的重用。其次，可以通过==#pragma unroll指令来提高数据并行性（复制管道）==。

图1（b）显示，==调整后，无论数据类型如何，带宽始终保持30GB/s不变==。但是，==与CPU和GPU看到的性能在架构的峰值性能达到饱和不同，我们看不到FPGA性能达到饱和==。相反，==该工具集会在性能饱和之前耗尽资源（趋势线的突然结束）==。这是因为增加ERT操作的数量会与使用的计算资源成比例地增加数据路径。在离开内核之前多次通过同一管道传递数据的内核可能会遇到管道宽度瓶颈或数据摄取/写入瓶颈，具体取决于数据流。多次通过较小的多项式计算出的ERT多项式将在较小的多项式的点处看到Roofline平稳段。未来的工作将研究利用graph相似性来最大化性能同时降低硬件利用率的转换。  

![1611716502286](D:\Notes\raw_images\1611716502286.png)

图1：Arria 10 ERT Roofline float, double, int8, int32, and int64（虚线）图 。调整之前（a），Roofline带宽取决于数据类型。相反，在调整（b）之后，Arria 10的Roofline带宽类似传统的Roofline。

随着polynomial degree increases，流水线合成增加了部分计算和数据移动的次数，ERT多项式在FPGA架构上创建了直线。 FPGA受到可用资源的限制，无法合成这样的数据流管道，==资源受限的Roofline天花板以纯色显示==。最佳unrolling 可填充512位宽的DRAM传输。

![1611718880962](D:\Notes\raw_images\1611718880962.png)

图2 显示了==手动展开以提高数据并行性和性能的好处==。总体而言，展开32次可将强度小于16的带宽的带宽提高约16倍至约30GB/s。但是，对于高强度，展开可能会导致合成的代码无法运行。在这种情况下，必须减少展开才能成功编译并达到约930GFLOP/s的性能。

在整篇论文中，我们将使用Roofline数据来评估内核基准测试的结果。对于给定的体系结构，我们可以根据内核的算术强度对它的Roofline进行内省。这告诉我们内核的实现可以充分利用目标计算机，而Roofline相对于带宽和FMA的数量则告诉我们架构可以很好地利用基础（underlying ）技术。

![1611724712599](D:\Notes\raw_images\1611724712599.png)

尽管Arria 10的持续带宽和峰值性能低于CPU或GPU，但通常所需的功率要少得多。图3绘制了ERT性能和功效之间的关系，该关系是==数据类型（彩色趋势线）和算术强度（趋势线内的点）== 的函数。如人们所见，==无论数据类型如何，Arria 10在低性能（但最大带宽）下消耗约24W的功率，而在最大性能下则增加到约32W的最大值==。==随着算术强度的提高，性能会迅速提高，但功耗却会缓慢提高==。结果是，能效趋向于运行能效渐近线（对角线）。

但是，与CPU或GPU不同，

- FPGA明显激励了float和int8数据类型的节能计算（Performance最高），
- 同时为double和int64数据类型提供了低8-16倍的能源效率。
- 此外，==接近垂直的趋势线意味着Arria 10 FPGA无法实现与功率成比例的性能==。

**B. Spatial Locality**

尽管ERT在向量接近GBytes的情况下获得了最大的DRAM内存带宽，但许多应用却没有表现出如此高的空间局限性。而是，许多应用程序可能会在跳转到内存中的另一个位置之前访问几个连续的元素。这样的内存访问模式可能会对流预取器造成严重破坏，并要求采用高级方法来隐藏延迟。为此，我们==创建了一系列类似于GPU和FPGA的Stanza Triad [29]基准测试，以了解内存带宽如何随空间局部性扩展==。

 ![1611725417881](D:\Notes\raw_images\1611725417881.png)

图4显示了在不同程度的空间局部性下访问数据时的持久内存带宽（左侧代理为 random1 64B访问，右侧代理为STREAM基准）。不出所料，所有架构都以较高的空间局部性接近其ERT或STREAM带宽。但是，对于CPU（流预取器），GPU（多线程）和FPGA（流水线），我们观察到非常不同的属性。==在512B以下时，CPU带宽急剧下降==，==而在256B以下时，GPU带宽仅出现中等程度的下降==。相反，==基于DDR的Arria 10在32KiB以下的带宽明显降低==，而==基于HBM的Xilinx U280需要至少64KiB的空间局部性才能饱和带宽==。实际上，对于不到2KiB的空间局部性，基于DDR的Haswell提供了出色的带宽。显然，取决于应用程序中缺乏空间局部性，架构之间的差异可能会大大夸大。

Haswell，U280和Arria 10的平行对角线表示，通过顺序访问带宽时间（time per byte）和“启动损失”时间（time for first byte）的简单α-β模型，可以很好地代理所有三种架构的带宽。渐近地，对于低空间局部性，接近locality/α的带宽；对于高空间局部性，接近于1/β的带宽。急剧的过渡意味着重叠（时间是这两项的最大值），而平稳的过渡则意味着序列化（这两项的总和）。由于该基准测试是并行运行的，因此将带宽合并在一起，并且启动惩罚时间（大约64B除以64B处的带宽）应按workers的数量进行缩放。

![1611726096366](D:\Notes\raw_images\1611726096366.png)

图5：Arria 10存储器访问效率：存储器传输时，载存储单元（LSU）的宽度为量子字节。连续内存访问的空间位置字节。两个DRAM控制器并行使用，以从两个单独的存储器缓冲区进行读取和写入。==有效的DRAM访问需要较高的空间局部性和合并的数据传输==。  

CPU和GPU将DRAM访问量化为32B，64B或128B事务，而FPGA则将其留给程序员以使存储器访问量子和数据类型正交。==图5显示了小的内存访问量如何严重限制带宽==。尽管访问包含4B数据的数据结构的程序员可能会被激励使用4B内存量子，但很显然，这样做==将使带宽降低一个数量级==。相反，==FPGA应该使用64B量子（反映了潜在的DRAM技术）访问存储器==，提取相关的字，并对64B量子进行缓存/缓冲以备后用。

### HPC KERNELS

我们研究了三个HPC内核：密集矩阵矩阵乘法，稀疏矩阵矢量乘法SpMV和Banded Smith-Waterman,BSW。 这些内核展现出高度变化的算术强度，并行度以及对细粒度同步的要求。

我们将自身限于==单精度矩阵乘法（SGEMM）和SpMV==，以确保==缺乏强化的双精度功能单元==不会歪曲我们对FPGA潜力的评估。与许多其他Genomics代码相似，==BSW使用16位整数变量==来计算比对得分。在所有情况下，我们都假定数据驻留在FPGA加速器上，从而避免了任何PCIe数据移动。

**A. Dense Matrix-Matrix Multiplication**

> FPU: floating-point functional units (FPUs)

  ![1611726850327](D:\Notes\raw_images\1611726850327.png)

图6绘制了==CPU，GPU和FPGA上SGEMM的性能与硬件资源（FPU数量）的关系==。我们使用大型矩阵来运行（在CPU和GPU上最大为32K×32K，在FPGA上最大为13K×13K）。我们通过控制MPI进程数（即内核数）来改变CPU上的硬件利用率，通过控制线程块数（即SM数）来改变GPU上的硬件利用率，并通过综合多个设计来逐渐增加DSP模块的数量，从而在FPGA上改变硬件利用率。 

当硬件用完（CPU / GPU）或综合synthesis失败（FPGA）时，趋势线终止。该图中的直线表示完美的缩放比例。当使用全芯片时，==大多数体系结构都可以很好地扩展，但超过16核的HSW和U280的性能会有所损失==。使用这种形式，我们可以定义“有效频率”的等曲线，它们表示FPU的平均频率为 $\frac{GFLOP / s }{2*\#FPUs}$。一般而言，架构趋向于频率等效曲线略低于其标称频率（标称V100频率为1.5GHz，有效频率为1.0GHz），这表明峰值比例很高。但是，==使用完整芯片时，U280明显显示出频率上的重大损失==。==该FPGA由3个超级逻辑区域组成，跨越这些区域边界的设计需要以低时钟频率运行==。

![1611727417440](D:\Notes\raw_images\1611727417440.png)

图7绘制了SGEMM性能与功率的函数关系，以提高硬件资源利用率（并发性）。尽管功率比例架构沿等曲线线是有限的，但我们看到==所有架构仅显示出适度的功率增加，而并发性和性能却得到极大提高==。尽管GPU和CPU所需的功耗相当，但==FPGA的功耗仅适度增加，而功耗不到GPU的七分之一==。和以前一样，我们可以绘制==恒定能量效率的等曲线==。 GPU的能源效率最终超过32GFLOP/J（约31pJ / FLOP），而Arria 10和U280 FPGA分别达到16和8GFLOP/J。

  ![1611727428679](D:\Notes\raw_images\1611727428679.png)

图8：==ERT趋势线确定了数据流量变化时的性能上限==。这些趋势线上具有相同性能的数据点表示相同的硬件使用情况。 Arria 10上的SGEMM性能由大符号表示，该符号非常接近Roofline极限。

**B. Sparse Matrix-Vector Multiplication (SpMV)**

![1611728288718](D:\Notes\raw_images\1611728288718.png)

图12绘制了功率dot product（实心符号）和SpMV（不同矩阵的空心符号）带宽。我们还显示了恒定能效的等曲线，以突出显示每种架构的能效。还标出了HBM（橙色）和DDR（红色）能效和带宽的极限。

对于streaming dot product，U280 FPGA的能源效率比V100 GPU和KNL CPU高1.2倍和4倍，同时达到接近峰值带宽。此外，由于HBM的下限为7pJ/bit或17.8GB/s/J，我们可以看到==U280的能效约为技术极限的2倍==。相反，Arria 10所提供的带宽却远远小于可比较的功率，因此能源效率降低了近一个数量级。

**C. Banded Smith-Waterman (BSW)**

![1611728847874](D:\Notes\raw_images\1611728847874.png)

图14绘制了BSW到解决方案的时间与功率的随着并发性增加函数关系。与以前的HPC内核不同，大多数架构在插座中的功耗仅略有增加，==而Arria 10消耗约30W==。但是，==尽管GPU吞吐量增加了大约80倍，但其功率却增加了大约4倍==。因此，与GPU或单插槽CPU相比，Arria 10最终所需的 energy-to-solution 降低了10％和40％。




