## 矩阵乘模式

实际的许多工程应用都涉及或可转化为矩阵乘法，因此常常成为衡量硬件实际性能的标准。如NVIDIA和AMD发布新的GPU处理器时，经常就宣称其sgemm（单精度稠密矩阵乘法）或dgemm（双精度稠密矩阵乘法）性能达到多少TFLOPS。如何高效的将矩阵乘法映射到硬件上并且获得非常高效率的实现就异乎寻常的重要，矩阵乘性能成为衡量硬件实际性能的现实标准之一，同时也应当成为衡量软件开发人员性能优化水平的标准之一，能够掌握矩阵乘模式的高性能实现的性能优化人员必定会成为市场上的宠儿。

矩阵乘的计算方式大致可矩阵A的每一行和矩阵B的每一列做向量内积，以伪代码表示如代码清单1-1 所示。

代码清单1-1 矩阵乘伪代码

```python
forall(row of A) {
	forall(col of B) {
		temp = dot(row, col);
	}
}
```

如果将dot运算改成其它运算，如将row向量和col向量之间的运算改成差的平方和，那么结果可以表示一组数据与另一组数据两两之间的距离；如果将其改成其它运算，就可以映射成某个领域具体的意义；同样对于矩阵A和矩阵B的行列，也可自己选择；这样简单的矩阵乘变为通用的一类运算，我们称这类通用的运算为矩阵乘模式，如代码清单1-2 所示。

代码清单1-2 矩阵乘模式伪代码

```
forall(row/col of A as sa) {
    forall(col/row of B as sb) {
        temp0 = fun0(sa, sb);
        temp1 = fun1(sa, sb);
        temp = fun2(temp0, temp1);
        out[sa, sb] = temp;
    }
}
```

函数fun0, fun1表示对矩阵的行列向量做点对点的操作，fun2函数可能引入一些点之间的相关性。

矩阵乘模式还有一种类似于域分解的表示方式：将结果矩阵划分成小矩阵，每个小矩阵是矩阵A的多行与矩阵B的多列的做矩阵乘的结果。如果将A的多行和B的多列进一步划分，那么又会形成一个层次的矩阵乘模式。这是一个递归的、层次化的定义方式，这个定义方式使得只要矩阵足够大，矩阵乘模式的并行度就足够大；矩阵乘模式的优化就可以立足于小矩阵和小矩阵之间的通信，而这又是一个递归问题。

在计算机视觉中，经常需要比较两个数据特征的相似度，数据特征可表示为固定长度的向量，在待比较数据量和数据库中的数据量都比较大的时候，这些运算就可以表示成矩阵乘模式；此类应用有特征比对，k-means/KNN算法等。

### 1.1 矩阵乘模式的适用场景

矩阵乘模式常可用来表示一组数据和另外一组数据的差异，本质上可表示为数据并行的一种特例。以代码清单1-1 为例，矩阵乘模式存在三个层次的并行性：遍历矩阵A的行，遍历矩阵B的列，矩阵A的一行和矩阵B的一列的运算。对于递归方式的矩阵乘定义来说，只要矩阵足够大，矩阵乘模式的并行度是无限大的，因此能够方便的映射到各种硬件平台上。由于小矩阵乘依旧满足矩阵乘模式存在的三个层次的并行性，因此在并行化和性能优化时，通常立足于在小矩阵上获得最大的性能，然后将其扩展到大矩阵上。

矩阵乘模式比较适合向量化，最内层的向量运算能够非常方便的映射到处理器的向量寄存器上。例外的情况在于：某些操作可能不被处理器指令集支持，比如一些处理器不支持超越函数的向量操作。为了达到最优的性能，通常在小矩阵上获得向量化的最优配置，然后在不同硬件上将其扩展到大矩阵上。要在单核心上获得矩阵乘的最优性能，则需要同时在向量指令的吞吐量，核心缓存的大小上综合考虑，才能获得最佳性能。

矩阵乘模式能够方便的支持线程化操作，对于遍历矩阵A的行和遍历矩阵B的列都可以线程化。对于矩阵乘模式的递归定义来说，每个线程可以处理不同的小矩阵，由于对小矩阵的处理没有相关性，因此能够获得接近线性扩展的小矩阵尺寸。

在集群中，矩阵乘模式也很适用，映射也比较直接。潜在的风险在于：即使在集群的一个节点上矩阵乘获得了最优性能，也不能保证整体获得最优性能，一方面存在集群中节点数据传输的开销，另一方面集群内机器可能存在细微的性能差异，这种性能差异在一些系统环境、配置的影响下可能会被集群系统扩大。

### 1.2 串行代码

对于一个大小为 M ×K 的矩阵与一个大小为 K×N 的矩阵相乘，其结果是一个大小为 M × N 的矩阵。由于现代处理器的缓存大小是有限的，因此计算访存比上限通常受限于寄存器数量和缓存容量。以具有32KB一级缓存的Intel X86处理器为例，计算得到n大小为48~64，在实际情况中，也经常选择M K N 大小为48或64。

对于AB来说，A的每一个元素$a[i][k]$需要和B的第 $k$ 行所有元素相乘，这样可以向量加载、运算B的一行。伪代码表示如下。代码清单1-3 串行矩阵乘实现

```python
for(int i = 0; i < BM; i++){
    for(int k=0; k < BK; k++){
        float a_v=a[i*BK+k];
        for(int j = 0; j < BN; j++){
            c[i*BN+j] += a_v*b[k*BN+j];
        }
    }
}
```

此实现可以看成一个固定大小的小块矩阵乘法实现，通过解决一个固定大小的矩阵乘法的性能进而解决一个大的矩阵乘法的性能是一种常见的方式，因为矩阵乘模式具有递归的特征。

### 1.3 矩阵乘模式优化要点

在现代多核向量处理器上实现矩阵乘法时，需要注意以下几点：

q 对全局存储器（DRAM）的访问是否高效。比如在X86、ARM 架构上实现时是否能够使用向量加载、存储指令，地址是否对齐，向量的读写是否高效；在GPU上实现时是否满足合并访问的要求。NVIDIA GPU和AMD GPU都支持向量加载和存储指令，使用向量加载存储指令一方面能够更好的在NVIDIA GPU和AMD GPU上发挥局部存储器的带宽、减少延迟，同时也能够减少加载、存储指令的数量进而改善指令效率。而ARM 移动GPU和高通移动GPU则只有使用向量读写指令才能发挥存储器系统带宽。

q 是否很好的使用处理器核心上的缓存。比如在 X86、ARM 架构上是否有效的利用一级缓存，在GPU上实现时是否很好的利用了局部存储器带宽。很明显，可以每次计算a的 X行和b的X列的乘积，这种方式可以使用局部存储器存储对应的数据。但是目前在AMD GCN GPU上每个工作组能够使用的局部存储器的最大大小为32KB；在NVIDIA GPU上每个工作组可使用的最大局部存储器容量为16 KB、32KB或48 KB。这意味着只要X稍微大点就有可能超过局部存储器容量的大小限制。此问题可以通过将a的X行和b的X列再次进行划分解决。

q 是否很好的使用了寄存器分块算法。无论是在X86、ARM 处理器上，还是在 GPU 上，如果不考虑寄存器数据重用的话，一级缓存或局部存储器的带宽和延迟无法满足矩阵乘法计算的要求。为了使用更多的寄存器优化性能，可以每次计算更多的数据，每次计算16个结果（每次计算出4x4个结果值），实际上也许可以计算更多，比如4x8或者8x8个结果。使用寄存器分块要特别留意使用的寄存器数量，因为一旦寄存器使用超标，编译器会将数据放到缓存中。

q 是否考虑到生成的指令是否足够好。在支持乘加指令的处理器就不应当生成乘或加指令。在矩阵乘法中，主要有全局存储器加载、存储，局部存储器加载、存储，整数加法、乘加和浮点乘加指令。需要注意这些指令的分布和比例，尽量去掉浮点乘加指令以外的指令。

q 是否考虑到如何掩盖指令和访存延迟。需要有足够的计算和访存并行度以利用处理器计算单元和访存单元的带宽。提高指令级并行能够更好的隐藏延迟，减少指令数量，必要时要使用循环展开、合理的安排指令顺序，更好的使用预取技术等。矩阵乘模式的延迟主要来自：读写数据（包括缓存）；计算浮点乘加时的延迟。要让计算和读取数据重叠，只需要在等待数据写入寄存器时，其它的寄存器中数据可以进行计算，这样一部分计算就掩盖了另外一部分访问存储器的延迟。

本章将会通过NEON/AVX/CUDA/OpenCL来实现矩阵乘法计算，读者会看到上面的几条的具体使用。

### 1.5 AVX实现矩阵乘模式

在使用AVX加速矩阵乘法时，有AB, ATB 和 BTA多种计算模式，本节以AB为例介绍如何使用AVX指令优化。

对代码清单1-3 的代码来说，直接的实现需要先从内存中读出c[i*BN+j]位置的向量，进行乘加运算，然后再将结果保存回内存，这意味着最内层循环里面有两次向量读、一次向量写和一次乘加运算，计算访存比很低，性能不会太理想。考虑到如果能够将 j 循环完全展开的话，那么对数组c的中间操作都可以在向量寄存器中完成，这意味着只要BK够大，计算访存比接近2:1。

在Intel处理器上，一级数据缓存大小大约为32KB，即大约能够保存8K个32位浮点数，考虑到同时保存a和b，故分块大小最多只能4k个32位浮点数，即分块大小最多只能是64×64。考虑到AVX指令集具有16个向量寄存器，而分块大小为64×64时，每行保存c中间运算结果最多需要8个向量寄存器，故有可能同时计算多行。

基于以上考虑，笔者设计了如下的实现算法，如代码清单1-6 所示。

代码清单1-6 AVX矩阵乘法

```python
template<int BM, int BK, int BN>
void sgemm_kernel(float *a, float *b, float *c) {
    #define B_REG_M 2
    //12
	__m256 c_vec[B_REG_M*(BN/8)];

    for(int i = 0; i < BM; i += B_REG_M) {
        for(int k = 0; k < B_REG_M*(BN/8); k++){
            c_vec[k] = _mm256_setzero_ps();
        }

        for(int k = 0; k < BK; k++) {
            __m256 b_vec[BN/8];
            for(int jj = 0; jj < BN/8; jj++){
                b_vec[jj] = _mm256_load_ps(b+k*BN+jj*8);
            }

            for(int ii = 0; ii < B_REG_M; ii++){
                __m256 a_vec = _mm256_broadcast_ss(a+(i+ii)*BK + k);

                for(int jj = 0; jj < BN/8; jj++) {//6
                    __m256 temp = _mm256_mul_ps(a_vec, b_vec[jj]);
                    c_vec[ii*(BN/8)+jj] = _mm256_add_ps(temp , c_vec[ii*(BN/8)+jj]);
                }
            }
        }

        for(int ii = 0; ii < B_REG_M; ii++){
            for(int jj = 0; jj < BN/8; jj++){
                _mm256_store_ps(c+(i+ii)*BN+jj*8, c_vec[ii*(BN/8)+jj]);
            }
        }
    }
	#undef B_REG_M
}
```

在BM=BK=BN=48的情况下，对a行展开2次，对b列完全展开，此算法在ivy bridge架构上获得了超过80%的峰值性能。在Intel Haswell架构的处理器上，由于其支持FMA指令，因此实际性能应当会进一步提升。