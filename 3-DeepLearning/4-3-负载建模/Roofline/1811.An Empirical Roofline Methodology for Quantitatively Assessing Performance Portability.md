## An Empirical Roofline Methodology for Quantitatively Assessing Performance Portability

地址：https://sc18.supercomputing.org/proceedings/workshops/workshop_files/ws_p3hpc107s2-file1.pdf

发表：2018 IEEE/ACM International Workshop on Performance, Portability and Productivity in HPC (P3HPC)

作者：Charlene Yang

机构：Lawrence Berkeley National Laboratory



### 摘要

系统和节点架构持续多样化, 以更好地平衡节点计算、内存容量、内存带宽、互连带宽、功率和成本。

对于许多应用程序开发人员来说,实现性能可移植性 performance portability (有效地利用多个架构的功能)是一个理想的目标。不幸的是, ==不同的节点性能和机器平衡的差异，会导致开发人员无法确定他们是否获得了性能的可移植性, 或者仅仅是编写了可移植的(portable)代码==。Roofline模型提供了一种方法,可以定量评估给定应用程序使用目标平台的计算能力。

在本文中,我们扩展了Roofline模型,使其在计算FLOPs、3)在计算不同的内存访问模式和4)的代码性能和Roofline上限的同时,促进了性能可移植性分析。

在本文中,我们扩展了Roofline模型, 

1. 捕获==真实成本作为CPUs和GPUs的性能限制== （it empirically captures a more realistic set of performance bounds for CPUs and GPUs）
2. 使其在计算FLOPs时, 对不同浮点指令计算True Cost （factors in the true cost of different floating-point instructions when counting FLOPs）
3. 结合了==不同内存访问模式的影响==（incorporates the effects of different memory access patterns）
4. 并==对代码性能和Roofline上限进行了适当的配对==, 促进了性能可移植性分析。（with appropriate pairing of code performance and Roofline ceiling, facilitates the performance portability analysis）

**总结：**采用Empirical Roofline模型，可以分析在不同架构上的代码的性能的可移植性。



### 介绍

相对于屋顶模型这样的性能模型，==定义性能可移植性==有一些细微差别。首先，==必须通过使用基准（而不是供应商自己的规格编号）来建立准确的上限==，而规格编号可能是理论上的也可能是过于乐观的。其次，需要选择一个合适的上限，以便进行有意义的比较。这需要一些有关代码的指令组合和主内存级别的知识。最后，人们不再能够仅依靠规范的FLOP（手工计数）由于FMA这样的存在，而且除法，指数和对数都会使这些计数产生偏差。例如，浮点除法应计为多个FLOP，因为它们通常是在大多数现代CPU和GPU架构上由多个指令实现的。

本文做出了一些贡献。

- 首先，它量化了Intel Knights Landing CPU（KNL）[12]和NVIDIA V100 Volta GPU [13]的逼真的Roofline上限。
- 其次，我们针对KNL和V100开发并部署了一种方法，该方法可以在Roofline模型的上下文中准确说明不同指令组合的真实成本（以浮点除法演示）。
- 第三，我们解决了一个长期的问题，即Roofline如何考虑不同的内存访问模式-通过==适度的跨步内存==访问来演示。非如此会夸大体系结构的计算上限，又会低估应用程序的算术强度和计算性能，并导致观测值和性能界限之间存在巨大（错误的）差异。
- 最后，我们证明只有通过适当的应用程序检测和系统基准测试，Roofline才能用作定量评估跨CPU和GPU架构的性能可移植性的基础。

### METHODOLOGY

**A. 性能可移植性度量**

应用程序a的性能可移植性P在给定的平台H上解决问题p是，

![1611833628771](D:\Notes\raw_images\1611833628771.png)

其中$e_i(a,p)$是应用a在架构i上的架构效率，可以使用Roofline性能模型获得：

![1611833753289](D:\Notes\raw_images\1611833753289.png)

其中$P_i(a,p)$ 是每秒在浮点运算中观察到的代码性能（FLOP/s），$F_i$ 是架构i的峰值 FLOP/s性能，$B_i$ 是架构i的峰值带宽，而$I_i(a,p) $ 是在架构i上应用a的算术强度（AI）。分母$min(F_i,B_i \cdot I_i(a,p))$ 解释应用程序何时受计算限制（即F_i限制）以及带宽限制（即 $B_i \cdot I_i(a,p)$）限制，并增加了应用程序架构效率 $e_i(a,p)$ 的计算精度。

本文的大部分内容都集中在提高 $e_i(a,p)$ 的准确性，即$P_i(a,p)$ ，$F_i$，$B_i$ 和 $I_i(a,p)$。为此，我们为$F_i$，$B_i$ 部署了经验基准测试，并考虑了==不同指令混合==和==内存访问模式==的真实成本，以更准确地测量$P_i(a,p)$ 和 $I_i(a,p)$。

我们在本文中研究了==两种当代的HPC架构==- Intel Knights Landing （KNL）CPU [12]和NVIDIA V100 Volta GPU [13]。

ERT==运行各种“微内核”，遍历一系列参数，例如CPU上的进程和线程数，GPU上的线程块和线程数，问题大小和试验次数==。在本文中，我们==部署了四个“微内核”。即，它们是“ FMA”，“无FMA”，“除 with FMA” 和 “除 without FMA”内核==。所有这些都是基于双精度的，并且还尽可能地利用了vectorization机会和指令级并行性  instruction level parallelism（ILP）。这里与除法相关的内核是专门设计用于研究复杂操作（例如除法）对给定体系结构可达到的性能的影响（请参阅第III-B节）。

我们还使用ERT获取Roofline模型的内存上限。特别是，我们专注于内存层次结构的HBM级别，因为大型应用程序通常将其数据集分解为适合节点级别的HBM，以便充分利用HBM的高带宽。本文==选择的GPP内核（及其问题大小）也符合该描述==，即，它执行单个节点的工作，数据集适合HBM，但在两个架构KNL和V100上都超过L2。

关于使用ERT的一个警告是，即使它为可实现的性能提供了更为现实的界限，也绝不能得出结论，即每个应用程序甚至每个HPC应用程序都可以达到相同的性能水平。==ERT中的内核通常经过精心设计和调整，以匹配目标体系结构的特性==，以充分利用其潜力，而在现实世界的大规模应用中，这是不可能的。

**D. 等离子极 General Plasmon Pole (GPP) Kernel**

GPP内核：使用通用等离子极点近似计算电子自能[18]，并且用C ++编写并与OpenMP并行化。此内核中的计算代表的工作通常是单个MPI任务将在更大的计算中执行，跨越数百或数千个节点。计算类似于张量收缩，其中将一些预先计算的复杂双精度数组相乘并求和成一定维，然后折叠成一个小的矩阵。本文选择的问题大小是512个电子和32768个平面波基础元素，并且在材料科学的现实世界中是中等大小的问题。

```python
#pragma omp parallel
do band = 1, nbands
	do igp = 1, ngpown
		do ig = 1, ncouls #vectorization
			do iw = 1, nw #typically nw=3; unrolled
				load wtilde_array(ig,igp)
				load aqsntemp(ig,band)
				load eps(ig,igp)
				compute wdiff, delw, sch_array
			update achtemp(iw)

# and on the V100 as...
#threadblock grid: (nbands,ngpown)
do band = 1, nbands
	do igp = 1, ngpown
		do ig = 1, ncouls #threads
			do iw = 1, nw #typically nw=3; unrolled
				load wtilde_array(ig,igp)
				load aqsntemp(ig,band)
				load eps(ig,igp)
				compute wdiff, delw, sch_array
			update achtemp(iw)
```

我们选择GPP的原因是GPP==不仅提供了丰富的并行度（线程和向量）==，而且还包括一些我们可以改变的参数，以便任意增加算术强度（增加iw循环中的nw或更改数据类型）从复杂到实际-两者都与整个应用程序中的不同实际问题配置相关），==启用跨步内存访问模式==（ig循环的修改-与完整代码中的不同索引相关），并量化浮点的影响在Roofline模型的上下文中进行除法（用乘法代替calculate delw语句中的除法）。 

运行时长，我们可以通过以下方式计算GPP内核的持续性能（GFLOP / s）：

![1611835671226](D:\Notes\raw_images\1611835671226.png)


![1611835684030](D:\Notes\raw_images\1611835684030.png)

![1611835914403](D:\Notes\raw_images\1611835914403.png)

![1611835928679](D:\Notes\raw_images\1611835928679.png)

尽管对编译器为每个除法生成的所有FLOP进行计数是朝着正确方向迈出的一步，但这可能还不够。在KNL上，编译器会生成多个与尾数和指数相关的提取和插入指令，以及尽管SDE在矢量单元中执行并替换其他浮点指令的事实，但SDE不算作FLOPs。

![1611836813962](D:\Notes\raw_images\1611836813962.png)

GPP中包含除法，除法的理论Flops和实际的Flops数有差距，包含了其他操作，并随着问题的增大，增大的Flops比重更大。

![1611836918564](D:\Notes\raw_images\1611836918564.png)

![1611837516750](D:\Notes\raw_images\1611837516750.png)

![1611837650902](D:\Notes\raw_images\1611837650902.png)

![1611837668770](D:\Notes\raw_images\1611837668770.png)

**C. Capturing Changes in Performance Bottleneck**

为此，我们只需将nw从1更改为6，即可简单地增加GPP中iw回路的trip次数。理论上，算术强度应随nw的增加线性增加（几乎）。为了在动态混合中最大化浮点指令的数量并简化分析，我们首先在KNL和V100上No-FMA的生成。在V100上，图6显示了一个强烈的过渡（空心符号），因为从内存绑定性能到无FMA上限限制的性能线性增加了算术强度（请注意对数刻度），图5显示了KNL命中即使具有很高的算术强度并适当考虑了浮点运算，也很难hitting No-FMA上限。如果要利用FMA（实心符号），那么两种架构都不会看到理论上的2x加速。

![1611837964660](D:\Notes\raw_images\1611837964660.png)

![1611837159505](D:\Notes\raw_images\1611837159505.png)

