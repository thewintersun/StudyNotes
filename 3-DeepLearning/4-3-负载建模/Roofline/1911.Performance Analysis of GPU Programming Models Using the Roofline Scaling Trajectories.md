## Performance Analysis of GPU Programming Models Using the Roofline Scaling Trajectories

地址：https://www.researchgate.net/publication/342019818_Performance_Analysis_of_GPU_Programming_Models_Using_the_Roofline_Scaling_Trajectories

作者：Khaled Ibrahim, Samuel Williams, Leonid Oliker

发表：International Symposium on Benchmarking, Measuring and Optimizing (Bench), BEST PAPER AWARD

时间：November 2019

机构:  Lawrence Berkeley National Laboratory



### 摘要

性能分析是一项令人生畏的工作, 尤其对于快速发展的加速器（accelerator ）技术。==Roofline Scaling Trajectories 技术的目的是通过直观的Roofline图来诊断GPU编程模型的各种性能瓶颈==。

在这项工作中,我们介绍了在NVIDIA Volta GPU架构中捕获主要性能瓶颈的Roofline Scaling Trajectories，比如warp efficiency、occupancy和locality。利用该分析技术, 我们解释了 NAS Parallel Benchmarks(NPB) 的性能特征, NPB采用两个编程模型CUDA和OpenACC编写。

本文介绍了编程模型对性能和缩放特性的影响。我们还利用Roofline Scaling Trajectories分析的见解来调整一些NAS Parallel Benchmarks, 达到了2倍的加速。

**总结：**通过缩放的方式（比如Kernel的SM数量），找到Roofline的轨迹，对==NPB进行调整==，得到2倍的加速。



### 介绍

为此，本文介绍了使用Roofline缩放轨迹[10]来分析GPU架构的性能。具体来说，我们展示了==在更改计算中涉及的GPU SM数量时，Roofline缩放轨迹方法如何强调各种体系结构特征==。这样，这些轨迹揭示了更改GPU SM计数时的GPU Warps执行的效率，occupancy 级别，共享的最后一级缓存的时间局部性的效率。缩放轨迹显示了上述低效率对GPU性能限制的影响，该影响是算法算术强度的函数。我们使用CUDA编程模型和基于编译指示的OpenACC模型对移植到GPU架构的多个NAS Parallel Benchmarks[2]进行了分析。

我们提出使用Roofline缩放轨迹来揭示各种性能低下的根本原因。我们展示了==如何在视觉上将缩放轨迹与各种Warps效率低下相关联，包括那些由于branch divergence和那些与latency divergence有关的Warps，以及由于缺少线程块并行性而导致的占用率下降==。此外，Roofline缩放轨迹已显示[10]，以揭示我们扩展应用程序时==缓存的抖动影响及其对时间局部性的影响==。确定了应用程序的性能瓶颈后，我们利用这些见解来调整两个NAS基准测试的性能，从而将性能提高了2倍。

### Motivation

第一个基于OpenACC指令[15]，另一个基于CUDA编程模型[6]。我们针对每种实现运行可能适合GPU内存的问题大小或类。我们展示了强大的扩展行为，同时针对多个问题大小更改了计算中涉及的SM数量。为了进行这些实验，我们利用了Volta架构的新功能来启用多进程服务（MPS）[14]。对MPS的硬件支持允许将计算资源的子集专用于特定的进程或应用程序，从而实现性能隔离（或QoS）和地址空间隔离。

我们可以轻松地识别出各种次优suboptimal的性能趋势。对于NPB的OpenACC端口[19]，我们观察到==LU和FT不会随着SM计数的增加而显着改善==，而==MG则在SM计数较高时先提高，然后性能下降==。随着==问题大小从A类增加到B或C，性能并没有显着提高==。对于NPB的CUDA版本[9]，LU的随着SM数量显着提高。将问题的大小从A类增加到B类可以进一步改善性能，但是当从B类更改为C类时，趋势会相反。

不幸的是，强大的缩放曲线表明仍有改进的余地，但它们==并未阐明观察到的行为的原因==或应将这些GPU应用程序的调整工作控制在何处。几种配置工具包括nvprof [13]，TAU [16]等。本文==扩展了Roofline缩放轨迹技术[10]，以揭开一些观察到的性能趋势的神秘面纱==。

![1611749997519](D:\Notes\raw_images\1611749997519.png)

图1：使用两种编程模型CUDA和OpenACC，随着GPU SM的数量增加，对各种NPB应用程序进行了强大的扩展。我们观察到各种次优的缩放趋势，没有明确的原因。可以==在单个GPU上容纳的最大类取决于编程模型==。

### GPU架构上的性能

GPU的性能通常与利用各种架构功能的效率相关。NVIDIA GPU架构具有多个并行级别：Warp内的线程，Warp共享一个SM 以及多个GPU内的多个SM，Warp内的线程使用SIMT执行模型。

当所有线程选择相同的执行路径并创建合并的内存访问时，线程Warp的执行通常会更高效。

较早的GPU使用单个程序计数器来处理线程Warp，因此分支差异对性能的影响非常严重。 Volta GPU提供了一个程序计数器和每个线程的堆栈，从而减轻了frequent re-convergence的需求，但是当branch divergence最小时，执行效率仍然更高。 

CUDA线程块是一种软件抽象，它使用多个Warp并提供低开销的同步原语和通过共享内存进行的通信。线程块的所有Warp都在同一SM上调度和执行。假设有足够的硬件资源（包括寄存器，共享内存等）可用，则硬件调度程序可以调度SM内的多个线程Warp（可能还有块）。SM内协同调度的所有Warp都使用同一组功能单元，对于Volta使用 64 FP32 。Volta中的SM数量为80。Volta可以为每个SM调度多达64个Warps。 Volta提供了多进程服务（MPS）功能，以允许多个内核在GPU上同时运行，并控制分配给内核的SM数量。在这项研究中，我们利用MPS支持来控制同时使用的SM数量。

在NVIDIA GPU上执行计算的效率需要仔细考虑以下方面。

- ==一组线程内的Warp效率==或SIMT效率，即所有线程遵循相同的执行路径或具有相同的延迟来执行一条指令。
- ==SM Occupancy占用率==，即为每个SM调度尽可能多的Warp的能力，以隐藏访问内存系统的长时间延迟。这还涉及使GPU大部分时间处于繁忙状态。
- ==数据局部性==，即有效内存请求合并和temporal L2缓存访问。

这些目标可能是矛盾的。例如，GPU占用率需要对小型内核进行流水线处理，但必须提供足够的并行度以使SM饱和。有效使用缓存层次结构也可能与提供足够的独立线程Warp发生冲突。理想地，性能工具或技术将识别性能次优并将其链接到上述性能维度之一。

### GPU Roofline Scaling Trajectories

在本文中，我们利用Roofline缩放轨迹技术来分析GPU加速的并行应用程序的性能和可伸缩性。我们旨在利用这种分析技术来确定应用程序可能在GPU架构上遇到的各种性能瓶颈。 Roofline扩展轨迹可视化了扩展行为，并确定了缓存和内存访问位置，Warp效率以及SM和GPU Occupancy 对应用程序性能的影响。

图2显示了Roofline缩放轨迹曲线的示例。标称的机器专用==屋顶线是针对最低和最高并发级别（在Volta上为2 SM和80 SM）构建的==。轨迹是每个并发级别上应用程序性能和算术强度的趋势线。

可以将这一分析应用于完整的应用程序或单个内核。我们使用早期GPU Roofline研究[4,12]中提出的经验测量技术来表征机器特性。我们的重点主要放在DRAM Roofline模型上，该模型必须通过NVIDIA nvprof配置工具测量dram读取事务和dram写入事务指标。我们将应用程序报告的FLOP用于所有并发级别，以确保吞吐量反映了应用程序的性能。我们计算应用程序的算法强度如下：

![1611754001930](D:\Notes\raw_images\1611754001930.png)

该模型可以轻松扩展到[4]中介绍的其他层次结构。我们==依赖于应用程序开发人员估算的应用程序规范FLOP计数，而不是依赖于分析工具的测量结果==。这允许进行一致的性能比较，因为由于在线程块内使用数据复制和精简操作（尤其是在CUDA版本中），==FLOP的数量可能随运行配置而变化==。这种方法允许在编程模型之间进行公平的比较。

理想情况下，性能应随计算资源的增加而线性提高，而不会降低算术强度。在Roofline图上，==这将转化为吞吐量与计算资源的增加成比例的垂直变化==，同时并行性改变。实际上，在缩放（例如，挂在左边的缩放曲线）时，应用可能遇到==吞吐量的次优变化==或==算术强度的变化==。

当强烈扩展应用程序时，主要瓶颈通常会改变。例如，在低并行性下，通常==很难将带宽饱和到内存层次结构的共享级别==。在这种情况下，Warp效率成为应用达到屋顶线的主要限制因素。尽管较低的占用率可能会在低并发率下产生类似的效果，但是对于大小不小的内核，这不太可能。另外，Occupancy的缺乏是很容易区分的。当我们大规模扩展应用程序运行时，==观察到算术强度的损失意味着在共享缓存级别存在一些缓存颠簸==。吞吐量提高的潜力通常受memory-bound应用程序算术强度损失的影响。缩放时==占用率的降低==进一步降低了观察到的性能提升。

找出性能瓶颈通常是将优化工作引导到正确问题的第一步。例如，会发生Warp效率问题，是由于divergent branches或数据索引效率低下而导致跨线程Warp的非分批non-coalesced内存访问。这将需要特定的代码重构技术，而占用率的下降将需要不同的补救措施。

占用问题涉及到两个相互矛盾的约束。首先，要改善内核中的并行性，就需要使用粗粒度coarse-grained的内核来提高GPU的占用率。其次，减少启动内核的停顿 reducing stalls 需要通过为每个内核分配一个较小的任务来流水线化内核调用。==要平衡这两个相互矛盾的要求，就需要对每个目标体系结构进行一些调整==。理想情况下，代码的结构应能够处理不同的任务粒度。![1611753582548](D:\Notes\raw_images\1611753582548.png)

图2：GPU架构上的Roofline缩放轨迹。每个点代表某个SM并发级别的吞吐量。我们使用Roofline扩展轨迹来诊断各种性能扩展瓶颈，包括Warp执行效率低下，扩展时占用率降低，过多数据移动到缓存层次结构等。

### Experimental Setup

NAS Parallel Benchmarks (NPB)该套件使用FT表示频谱方法，使用CG表示稀疏线性代数，使用LU表示规则稀疏的上下三角系统，使用MG表示使用网格层次结构的多网格PDE求解器。此外，该套件包含两个微型应用程序SP和BT，它们在结构化网格上进行关键的计算流体动力学（CFD）计算。

### Scaling Trajectory Analysis for Computational Kernels

在介绍Roofline缩放轨迹之前，我们展示了所研究基准集的各种内核的Warp效率。==Warp效率定义为每根Warp的平均活动线程数与每根Warp的最大线程数之比==[13]。该指标捕获了线程执行的差异，并且通常不会随着跨SM的工作负载分布而变化。==具有不规则内存访问的应用程序的Latency divergence[5]对性能具有相同的影响==。

![1611755895847](D:\Notes\raw_images\1611755895847.png)

图3：NPB基准的各种内核的Warp效率。通常，==与CUDA版本相比，OpenACC版本的翘曲效率更高==，而BT和LU基准测试则尤为如此。更改SM计数时，warp效率保持恒定，并且可能随问题大小而变化。 更改数据集的大小（即，类）可能会导致扭曲效率的提高（例如在CUDA LU的情况下），或在OpenACC BT的情况下降低效率，或者可能会像CUDA SP。我们将显示Warp效率以及数组的索引方案会影响低SM并发性能。

![1611756478440](D:\Notes\raw_images\1611756478440.png)



图4显示了NPB应用程序的Roofline线缩放行为。首先，我们观察到==所有应用程序都基于它们的算术强度而受到内存的限制==。因此，我们通过删除计算范围区域的融合乘加上限来简化Roofline体系结构限制，因为这是无法实现的。==无论是强缩放还是弱缩放，CUDA LU和OpenACC MG都显示出AI最明显的变化==。对于BT，由于Warp效率更高，因此与CUDA版本相比，我们注意到OpenACC实现的低并发效率更高，但是CUDA版本在缩放时具有更好的占用率。除了CG和LU，OpenACC实施具有较高的启动效率，因为它具有更好的Warp效率。

由于延迟差异latency divergence[5]，OpenACC CG扭曲在低并发时效率较低[5]，从而降低了Warp效率。重要的是，这种Warp效率低下通常不会由nvprof指标捕获，而是由Roofline标度轨迹捕获。==CUDA BT的低Warp效率体现在Roofline比例图中的SM数量低==。对于同时具有OpenACC和CUDA实现的应用程序，CUDA实现的算法强度更高，即，涉及到L2缓存的数据移动更少，这对于实现Memory-Bound应用程序的更高性能至关重要。

对于CUDA LU和SP，当我们更改问题类别（大小）时，我们观察到算术强度的损失。==对于LU，增加问题的大小可以提高GPU的占用率，因为从A类更改为B类可以提高性能==。对于C类，由于过度的数据移动而导致的算术强度降低，降低了总体收益。我们观察到SP的趋势类似但不太深刻。对于OpenACC MG，由于L2数据的有效重用，我们发现在低并发下性能超过了内存限制。这导致算术强度的提高，并且在低并发时缩放较弱。作为一个强尺度，算术强度会降低，尤其是在高并发时。对于OpenACC LU，我们注意到小规模的低效率和次优的占用率改善都是一个强有力的规模。 SM计数的变化略有改善。此应用程序有一个GPU占用问题，表现为SM计数较低，这将在后面讨论。对于CUDA LU，我们注意到在低并发性的同时增加了问题的大小，这与Warp效率的提高相关。

![1611757387088](D:\Notes\raw_images\1611757387088.png)

occupancy的变化会影响应用程序的强大扩展行为。应用程序需要足够的并行度，以使所有可用SM饱和，以实现最佳性能。如第5节所述，occupancy term会影响潜在的改善，同时增加SM数量。在图5中，我们考虑了CUDA BT和CUDA LU的详细SM occupancy 行为。对于BT，我们观察到在弱缩放和强缩放期间都有恒定的占用率。缩放轨迹显示与测得的SM占用行为高度相关。

![1611757596215](D:\Notes\raw_images\1611757596215.png)


CUDA LU的占用率随着强缩放而降低。问题大小的增加会提高占用率，并且影响在内核之间并不均匀。 jacld_blts内核受占用率损失的影响最大。如图6所示，单个内核的Roofline缩放轨迹精确地捕获了这种行为，既损失了jacld blts内核的占用率，又保持了rhs_kernel_*的良好占用率。我们注意到，与rhs_kernel_x相比，在弱缩放过程中rhs_kernel_y,z的算术强度变化很小；检查代码，我们发现一个单元在x方向上跨步访问，在y和z方向上跨步跳跃。应用数据转置通常可用于解决此类瓶颈，但需要一种高效的转置，其开销比嵌入跨步访问的开销要低。  

