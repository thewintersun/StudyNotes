## Hierarchical Roofline Performance Analysis for Deep Learning Applications

地址: https://arxiv.org/abs/2009.05257

作者：Charlene Yang, Yunsong Wang, Steven Farrell, Thorsten Kurth, Samuel Williams

机构：Lawrence Berkeley National Laboratory， Berkeley, CA USA



### 摘要

本文提出了一种收集性能数据的实用方法, 对NVIDIA GPUs进行分层的Roofline分析。

讨论了Empirical Roofline Toolkit的扩展, 以更广泛地支持一系列数据精确和张量核心支持, 并引入了一种基于Nsight Compute的方法来准确收集应用程序性能信息。

该方法允许在NVIDIA GPUs上对整个内存层次的Roofline分析进行自动化的机器特性和应用特性, 并通过一个复杂的深度学习应用程序（气候图像分割）来验证。

我们分别使用了两种版本的代码, 分别在TensorFlow 和PyTorch中, 以证明该方法的使用和有效性。我们强调了应用程序如何利用GPU的计算和内存功能, 以及在两个深度学习框架中实现和性能的不同。

**总结：**用ERT的扩展收集Roofline实际模型，用Nsight Compute构建两种框架下Tensorflow PyTorch的实际应用程序性能点。 

### 介绍

该方法有两个组成部分：==使用Empirical Roofline Toolkit（ERT）[2]进行机器表征==和==使用Nsight Compute [23]进行应用表征==。

我们将讨论==ERT的扩展以支持多种数据精度和Tensor Core操作==，以及用于衡量应用程序性能的Nsight Compute指标（例如运行时间，持续吞吐量和整个内存层次结构中的数据移动）。

然后，该方法将通过最新的==深度学习应用程序DeepCAM== [21]在气候图像分割中得到验证，以证明其在应用程序分析中的有效性。

我们将==分别在TensorFlow和PyTorch中==检查两个版本的代码，并将重点介绍深度学习应用程序通常如何利用NVIDIA GPU上的计算/内存功能以及TensorFlow和PyTorch这两个深度学习框架的一些见解。 在实现和性能上可能有所不同。

### METHODOLOGIES

**A. ERT Extensions for Machine Characterization**

为了支持多种数据精度（例如FP16）和NVIDIA GPU上的Tensor Core操作而在Empirical Roofline Toolkit（ERT）上进行的扩展工作。

![1611798455741](D:\Notes\raw_images\1611798455741.png)

![1611798408469](D:\Notes\raw_images\1611798408469.png)

**1) Single-Precision (FP32) and Half-Precision (FP16)**

只将half 的数据类型传递给模板函数，只可以实现与FP32精度的15.4 TFLOP/s类似的性能。这是==因为V100不直接在CUDA Core 上支持FP16，并且每个FP16操作本质上都作为FP32操作执行==（即通过同一管道）。向量类型half2 可用于==将两个FP16值打包在一起到一个FP32寄存器中==，并在一条FP32指令中执行。

实践证明，用uint32_t数据类型替换uint64_t索引变量可以带来最大的性能提升，从20.1 TFLOP/s到28.2 TFLOP/s。

**2) Tensor Core**

NVIDIA Tensor Core旨在加速矩阵矩阵乘法运算，这代表了许多深度学习工作负载的数学性质，例如卷积神经网络（CNN）。它们==对4×4矩阵进行运算==，并且可以非常高效地执行以下矩阵乘法和累加运算

![1611798951462](C:\Users\j00496872\AppData\Roaming\Typora\typora-user-images\1611798951462.png)

其中A和B是FP16中的矩阵，而C和D是FP16或FP32中的矩阵。 V100具有80个SM和每个SM 8个张量核心，并且在1.312 GHz时钟频率下，其理论Tensor Core峰值可计算为:

![1611799006235](D:\Notes\raw_images\1611799006235.png)

通常，有两种方法可以在Tensor Core上进行编程，即使用CUDA中的WMMA（Warp矩阵乘法累加）API或诸如cuBLAS和cuDNN之类的库。对于图2中给定的矩阵大小，我们可以使用运行时间t将内核的FLOP/s性能估计为$（M^3×2）/ t$。

![1611799169867](D:\Notes\raw_images\1611799169867.png)

我们从cuBLAS方法获得了理论峰值的96.5％的103.7 TFLOP/s，从wmma方法获得了54％的58 TFLOP/s。这主要是由于cuBLAS中的优化，例如==共享内存的使用，数据填充（以避免共享内存中的库冲突），高度调整的线程块大小，切片大小和其他参数==。

**B. Nsight Compute Metrics for Application Characterization**



![1611799342549](D:\Notes\raw_images\1611799342549.png)

**1) Kernel Run Time:**  如表II所示，我们使用指标 sm__cycles_elapsed.avg 获取经过的周期总数，并使用其次指标per_second 获取速率（每秒的周期数），以便计算内核执行时间：time = cycles / rate

```python
dfmetric['Time'] = dfmetric['sm__cycles_elapsed.avg'] / (dfmetric['sm__cycles_elapsed.avg.per_second'] / dfmetric['Count'])
```

**2) FLOPs:** 为了计算内核中执行的FLOP的数量，Nsight Compute没有提供像nvprof中的flop_count_dp这样的统一指标。但是对于每个浮点精度（FP64，FP32和FP16），它会根据指令类型，加法，乘法和融合乘加（FMA）将测量结果分为三个指标。请注意，每个FMA被视为两个FLOP，并且对于每个数据精度，FLOP的总数可以计算为add + 2 x fma + mul。

```python
dfmetric['CC FLOPs'] = 
2 * dfmetric['sm__sass_thread_inst_executed_op_dfma_pred_on.sum'] 
+ dfmetric['sm__sass_thread_inst_executed_op_dmul_pred_on.sum'] \
+ dfmetric['sm__sass_thread_inst_executed_op_dadd_pred_on.sum'] \
+ 2 * dfmetric['sm__sass_thread_inst_executed_op_ffma_pred_on.sum'] \
+ dfmetric['sm__sass_thread_inst_executed_op_fmul_pred_on.sum'] \
+ dfmetric['sm__sass_thread_inst_executed_op_fadd_pred_on.sum'] \
+ 2 * dfmetric['sm__sass_thread_inst_executed_op_hfma_pred_on.sum'] \
+ dfmetric['sm__sass_thread_inst_executed_op_hmul_pred_on.sum'] \
+ dfmetric['sm__sass_thread_inst_executed_op_hadd_pred_on.sum']

dfmetric['TC FLOPs'] = 512 * dfmetric['sm__inst_executed_pipe_tensor.sum']

dfmetric['all FLOPs'] = dfmetric['CC FLOPs'] + dfmetric['TC FLOPs']
```


  **3) Bytes:**  表II中列出了度量标准，用于测量存储器层次结构每个级别上的数据移动。

```python
dfmetric['AI HBM'] = dfmetric['all FLOPs'].div(dfmetric['dram__bytes.sum'])

dfmetric['AI L2'] = dfmetric['all FLOPs'].div(dfmetric['lts__t_bytes.sum'])

dfmetric['AI L1'] = dfmetric['all FLOPs'].div(dfmetric['l1tex__t_bytes.sum'])
```

### RESULTS

在下面的Roofline图表上，每个内核都由一个三元组空心圆表示（L1表示蓝色，L2表示红色，HBM表示绿色），==圆圈的大小与内核的运行时间成比例==。  

The main computational kernel represented by the three large circles under the Tensor Core ceiling, indicates that it has very high Tensor Core utilization, whereas many of the other circles either do not use Tensor Core or are bandwidth bound. This major kernel’s L1 circle (in blue) slightly overlaps with its L2 circle (in red) indicating a relatively low L1 cache locality; however, the large gap between its L2 and HBM circles demonstrates that L2 cache misses rarely happened and that the kernel benefits from high L2 data locality. As for the rest of the kernels, their L1, L2, and HBM kernels are generally close to each other, implying a poor data locality across all levels of memory hierarchies (“streaming” operations).

由Tensor Core上限下的==三个大圆圈表示的主要计算内核表明，它具有很高的Tensor Core利用率，而其他许多圆圈要么不使用Tensor Core，要么受带宽限制==。这个主要内核的L1圆圈（蓝色）与其L2圆圈（红色）略有重叠，表示L1缓存位置相对较低；但是，其L2和HBM圈之间的巨大差距表明==L2高速缓存未命中的情况很少发生==，并且内核受益于较高的L2数据局部性。至于其余的内核，==它们的L1，L2和HBM内核通常彼此靠近，这意味着在所有级别的内存层次结构（“流”操作）中，数据局部性很差==。

![1611800018138](D:\Notes\raw_images\1611800018138.png)

图3：使用默认配置的正向传播 TensorFlow DeepCAM的分层Roofline。占主导地位的内核（具有三个大圆圈）具有很高的Tensor Core利用率，并消耗了33％的总体运行时间。  

和向前传播中不同，在向前传递中只出现一个主要内核，而在反向传递计算中找到了两个非常耗时的内核。显然，这两个内核都比正向传递中的主要内核需要更长的运行时间（注意大小），==这意味着向后传递比正向传递具有更多的计算密集型内核，并且通常更耗时==。另一个观察结果是，与前向传递相比，后向传递涉及的内核调用更多。

总的来说，我们可以得出结论，==无论是向前还是向后，主要的计算内核都是受计算限制的，并且针对基础架构进行了高度优化==。 TensorCore利用率很高。    

  ![1611799953405](D:\Notes\raw_images\1611799953405.png)

图4：使用默认配置的向后传递中的TensorFlow DeepCAM的分层屋顶线与前向传递相比，计算密集型内核更多。它们合计占运行时间的41.9％，并达到接近Tensor Core峰值性能。


The number one kernel is位于单精度性能峰值的下方，并且根据不同内存层次结构之间的符号距离，==其缓存利用率比TensorFlow中的主导内核更好（即使它在CUDA Core上运行）==。此外，类似于TensorFlow，DeepCAM的PyTorch实现中大量琐碎的内核都 HBM-bound。

![1611800809419](D:\Notes\raw_images\1611800809419.png)

图5：PyTorch DeepCAM的分层屋顶线在默认配置下的正向传递。没有哪个内核比其他内核需要更长的运行时间（没有很大的圆圈）。

![1611800857064](D:\Notes\raw_images\1611800857064.png)

图6：使用默认配置的PyTorch DeepCAM的向后传递的层次屋顶线。人们可以观察到高度计算密集型但性能低下的内核。

令人惊讶的是，==排在第一位的耗时内核没有使用Tensor Core，而仅提供了大约1 TFLOP/s的性能==。但是，由于其他内核的优化或内核的整体执行，从圆圈的大小来看，此==实现的总体运行时间仍低于TensorFlow情况==。

与TensorFlow相比，==PyTorch在分析模型时具有更大的灵活性，并且可以在反向传播中轻松地将“优化器”步骤与梯度计算步骤分开==。优化步骤主要是用新计算的梯度来更新模型参数，并且通常算术强度较低。图7证实了这一点，其中所有“优化器”内核都受内存限制，并且其FLOP/s性能比图5或图6中的某些内核低得多。应该注意，有2709个内核调用即使只有几个圆圈可见，也参与了此过程。这些内核调用具有非常相似的算术强度和性能，因此是重叠的。

![1611801235611](D:\Notes\raw_images\1611801235611.png)

图7：PyTorch DeepCAM的“optimizer”步骤中的分层屋顶线。梯度更新步骤包括许多流操作，并且算法强度和FLOP/s性能较差。