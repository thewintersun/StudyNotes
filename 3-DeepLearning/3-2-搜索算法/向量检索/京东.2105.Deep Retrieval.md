## Joint Learning of Deep Retrieval Model and Product Quantization based Embedding Index

论文地址：https://arxiv.org/abs/2105.03933

作者：Han Zhang, Hongwei Shen, Yiming Qiu, Yunjiang Jiang, Songlin Wang, Sulong Xu, Yun Xiao, Bo Long, Wen-Yun Yang

机构：京东搜索

发表：SIGIR2021

项目地址：https://github.com/jdcomsearch/poeem



### 摘要

Embedding 索引可实现快速近似最近邻 (ANN) 搜索，是最先进的深度检索系统不可或缺的组成部分。传统方法通常将Embedding 学习和索引构建这两个步骤分开，会导致额外的索引时间和衰减的检索精度。

在本文中，我们提出了一种称为 Poeem 的新方法，它代表基于（product quantization）乘积量化的 embedding 索引与深度检索模型联合训练，利用gradient straight-through estimator、热启动策略、（ optimal space decomposition）最优空间分解、（Givens rotation）吉文斯旋转等技术，统一端到端训练中的两个独立步骤。

大量的实验结果表明，该方法==不仅显着提高了检索精度，而且将索引时间减少到几乎没有==。为了比较和可重复性，我们已经开源了我们的方法。



### 介绍

Embedding 索引具有几个吸引人的优势：a) 可以学习Embedding 以优化下游感兴趣的检索任务，以及 b) 用于最大内积搜索 (MIPS) 或近似最近邻 (ANN) 的有效算法，例如 LSH [7]， Annoy [3] 和基于最新product quantization (PQ) 的方法 [11, 17, 18] 可用于在几毫秒内检索Items。

然而，Embedding 索引也有一些缺点。主要问题在于模型训练和索引构建之间的分离，这导致额外的索引构建时间和检索精度下降。因此，最近出现了一种新趋势，即放弃单独构建的Embedding 索引，而采用==联合学习的结构索引== (embracing jointly learned structural indexes)，其性能比前者有所提高。一般来说，具有联合学习结构的方法可以概括为两种类型，基于树的方法 [27, 28] 和基于 PQ 的方法 [4, 19, 25]。

- 这些基于树的方法通常需要特殊的近似训练技术，其复杂性减缓了它们的广泛采用。
- 那些现有的基于 PQ 的方法仅设计用于小型计算机视觉任务，例如从数万张图像中检索，因此不适用于具有至少数百万个item的大规模信息检索任务，例如我们在现实世界中的工业检索系统。

在本文中，我们提出了基于product quantization的 embedding indexes 与 deep retrieval model 联合训练的方法。这不是微不足道的，我们必须通过适当的技术克服一些障碍：

1）量化步骤，作为基于 PQ 的embedding indexes 的核心，具有不可微的操作，例如 arg min，这会 disable 标准的训练反向传播。因此，我们利用梯度直通估计器(gradient straight-through estimator [2] ) 绕过不可微性，以实现端到端训练。 

2)  随机初始化的量化质心(centroids ) 导致非常稀疏的质心分配、低参数利用率和随之而来的更高量化失真（distortion）。因此，我们引入了一种热启动策略来实现more uniform distributed centroid assigments。 

3）标准优化乘积量化（OPQ）[9]算法，通过正交矩阵旋转空间以进一步减少PQ失真，不能与联合模型一起迭代运行。因此，我们开发了一种最陡的块坐标下降 (steepest block coordinate descent) 算法，使用 Givens 旋转 [10] 来学习这种端到端训练中的正交矩阵（orthonormal matrix）。

因此，我们提出的 Poeem 方法代表基于乘积量化的嵌入索引与深度检索模型联合训练，具有几乎没有索引构建时间和没有衰减检索精度的优点。我们的方法被封装在一个独立的索引层中，可以很容易地插入到任何嵌入检索模型中。



### Embedding Indexing Layer

Formally, 索引层定义了一个完整的量化函数 $\tau : R^𝑑 → R^𝑑$ that maps an input embedding $x$ to an output embedding $T(x)$ , 它可以分解为四个函数： a coarse quantization function $𝜓$, a product quantization function $𝜙$ and a decoder function $𝜌$,  and a rotation function with an orthonormal matrix $𝑅$.  粗量化函数$𝜓$，乘积量化函数$𝜙$,  解码器函数$𝜌$，以及具有正交矩阵$𝑅$的旋转函数。

请注意，我们交替使用正交矩阵和旋转矩阵。现在让我们在下面的部分中详细解释这些操作。

<img src="D:\Notes\raw_images\image-20220919144856730.png" alt="image-20220919144856730" style="zoom:80%;" />

最后，有了以上四个函数，我们现在可以定义完整的量化函数T如下:
$$
\tau (x) = 𝑅^T𝜌 (𝜓 (x^′), 𝜙(x^′ − v_𝜓 (x^′)))
$$
where $x′ = 𝑅x$.

### Experiment

<img src="D:\Notes\raw_images\image-20220919154015635.png" alt="image-20220919154015635" style="zoom:80%;" />

![image-20220919154218576](D:\Notes\raw_images\image-20220919154218576.png)