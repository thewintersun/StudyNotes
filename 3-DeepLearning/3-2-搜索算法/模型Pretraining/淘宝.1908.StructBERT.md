## StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding

论文地址：https://arxiv.org/abs/1908.04577

作者：Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Jiangnan Xia, Liwei Peng, Luo Si

机构：阿里巴巴

文章地址：https://adaning.github.io/posts/51848.html



### 摘要

Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman [8], we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks. The StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.



### Basic Idea

虽然BERT和RoBERTa将注意力放在了NLU问题上, 并大幅度的化简NLU相关的任务难度, 但仍然没有把**语言结构信息**集成进去.

> 如”==研表究明, 汉字序顺并不定一影阅响读==. 比如当你看完这句话后, 才发这现里的字全是都乱的”. 尤其在使用了Self Attention后, Token之间的最短距离恒为1, 自然语言中的单词顺序可能对BERT没那么重要了, 语言的结构可能也就得不到关注.

因此, 作者希望改进BERT的训练方式, 使其能够注意到句子中的结构信息的变化.



### StructBERT

作者在保留BERT现有的两个训练任务的基础上, 额外引入两个**辅助任务**, 分别是Word Structural Objective 和 Sentence Structural Objective, 即从**单词**和**句子**的两个角度来提升BERT对语言结构的理解.

#### Input Representation and Transformer Encoder

和BERT一样, 对于输入序列 (可能是单句子也可能是句子对) 的每个Token $t_i$, 将它的Word Embedding, Segment Embedding, Position Embedding相加作为其初始表示$x_i$, 然后能通过对 $L$ 层 Transformer Encoder 的堆叠获得其在当前语境下的表示 $h_i$.

<img src="D:\Notes\raw_images\image-20220928153831112.png" alt="image-20220928153831112" style="zoom:50%;" />

同理, 输入句子时, 句向量$X=\{x_i\}_{i=1}^ N$ 会被 $L$ 层 Transformer Encoder编码为 $H^l$:
$$
\mathbf{H}^{l}=\text { Transformer }_{l}\left(\mathbf{H}^{l-1}\right)
$$
其中$l \in [1, L]$,  $\mathbf{H}^0 = X$,  $\mathbf{H}^{L}=\left[\mathbf{h}_{1}^{L}, \cdots, \mathbf{h}_{N}^{L}\right]$.

#### Word Structural Objective

作者认为, 一个好的语言模型必须能够通过句子中打乱顺序的单词组 **恢复** 出单词的原来顺序, 而BERT仍不能正确的做到这一点. 针对这点, 作者直接将这种想法作为训练任务补充到BERT的训练当中.

该任务的目标为:
$$
\arg \max _{\theta} \sum \log P\left(\operatorname{pos}_{1}=t_{1}, \operatorname{pos}_{2}=t_{2}, \ldots, \operatorname{pos}_{K}=t_{K} \mid t_{1}, t_{2}, \ldots, t_{K}, \theta\right)
$$
$θ$ 为参数, $K$ 为打乱顺序的子序列长度.

==如果 $K$ 比较大, 意味着模型需要对比较长的序列重建, 噪声比较多, $K$ 比较小, 模型只需要对比较短的子序列重建, 噪声相应的也比较少.== 为达到模型鲁棒性和模型重建能力的平衡, 作者设定K=3.

该任务与原始MLM训练目标是不冲突的, 能够**联合训练**:

<img src="D:\Notes\raw_images\structbert1.jpg" alt="img" style="zoom: 50%;" />

在输入为`[MASK]`的地方应该能被预测出正确被Mask掉的Token, 对于输入打乱的地方应该能根据 $h_i^L$ 恢复出正确顺序的Token.

> Word Structural Objective主要针对单句子任务.

#### Sentence Structural Objective

除去单词级别的结构, 还需要关注句子和句子之间的结构.

在BERT训练中, 使用的是**NSP任务**, 而在RoBERTa中提出NSP任务由于过于**简单**, 有害于模型性能.

其实NSP任务也不是不能用, 必须增加它的任务难度. 在StructBERT中, 沿着组装句子的思路, NSP被**改进**成一个**三分类问题**, 分别令当前句子S1 与另一个句子S2 组合, S2 可能是以下的其中一种:

1. S2 为S1 的**下**一句, 此时任务为原始的NSP任务.
2. S2 为S1 的**上**一句.
3. S2 为**其他文档**的**随机**一句.

这三种情况均为**等概率**发生, 即发生概率均为13, `[SEP]` 的添加与BERT相同, 采用`[CLS]`处的输出做三分类结果. 示意图如下:

<img src="D:\Notes\raw_images\structbert2.jpg" alt="img" style="zoom:50%;" />

> Sentence Structural Objective主要针对句子对任务.

### Experiments

详细的实验设置请参照原论文.

#### General Language Understanding

##### GLUE benchmark

在GLUE上的结果如下:

![img](D:\Notes\raw_images\structbert3.jpg)

StructBERT在GLUE上的平均表现超过了其他的PLM.

#### SNLI

在自然语言推理的数据集SNLI上结果如下:

<img src="D:\Notes\raw_images\structbert4.jpg" alt="img" style="zoom:50%;" />

#### Extractive Question Answering

在抽SQuAD1.1上的结果如下:

<img src="D:\Notes\raw_images\structbert5.jpg" alt="img" style="zoom:50%;" />

即使没有使用任何的数据增强和额外数据, StructBERT还是仅次于使用了数据增强和额外数据的XLNet.

> 其实打乱顺序这种处理也可以看做是一种数据增强.

#### Effect of Different Structural Objectives

消融实验便是针对StructBERT的两个额外任务做的:

<img src="D:\Notes\raw_images\structbert6.jpg" alt="img" style="zoom:50%;" />

前三个数据集是单句任务, ==在去掉Word Structural Objective后, 前三个任务的性能有退化. 后三个是句子对任务, 去掉Sentence Structural Objective后对后三个任务影响也比较大. 证明了这两种新增的任务的有效性==.

下图分别是Word Prediction Loss, Word Prediction Acc, Sentence Prediction Loss, Sentence Prediction Acc 随着训练步长的增长的变化曲线:

<img src="D:\Notes\raw_images\structbert7.jpg" alt="img" style="zoom:67%;" />

红色代表原始BERT, 蓝色为StructBERT, 绿色为StructBERT在Masked Token任务上的表现.

作者认为, 加入Word Structural Objective使得MLM任务做的更好, 加入Sentence Structural Objective明显使得任务变得更难.



### Summary

StructBERT通过在原有任务的基础上从Word Level和Sentence Level添加了两个新的辅助训练任务, 使得BERT能够关注一些语言结构, 想法非常非常简单, 但效果却出乎意料的好.

StructBERT属于对BERT的改进, 没有太多值得进一步的建议.