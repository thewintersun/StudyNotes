## Pre-training Tasks for User Intent Detection and Embedding Retrieval in E-commerce Search

论文地址：https://arxiv.org/abs/2208.06150

作者：Yiming Qiu, Chenyu Zhao, Han Zhang, Jingwei Zhuo, Tianhao Li, Xiaowei Zhang, Songlin Wang, Sulong Xu, Bo Long, Wen-Yun Yang

发表：CIKM 2022

机构：京东搜索

项目地址：https://github.com/jdcomsearch/jd-pretrain-data



### 摘要

BERT类型的与训练模型在很多NLP任务上具有很好的效果，比如问答、文本分类、语句标注等。

但这个方案并不是处处通用的，比如语料差距很大、Embedding特殊分布（比如 ANN Search）

这篇论文采用BERT的与训练思想，在两种商用搜索系统的两个任务（用户意图预测、语义Embedding检索）中进行尝试。预训练的模型比BERT-base小，仅为其10%左右，但相比于没有预训练的模型，和采用通用语料训练的模型效果均有明显提升。



### 介绍

<img src="D:\Notes\raw_images\image-20220914101605291.png" alt="image-20220914101605291" style="zoom:80%;" />

电商场景NLP的挑战：

1. 用户输入是完全自由的，不遵循语法，而BERT类的与训练语料基本上都是语法严格的。
2. 长尾查询：长尾查询的总量是很大的，并且长尾词很多为新的商品或品牌的冷启动词，出现的次数很少，模型很难预测出它的用户品类意图或者相似商品。

### 用户意图预测

用户意图预测可以转换为一个多分类任务，labels_num = 3000,  对于查询 𝑥，我们的模型学习一个向量值输出函数 $𝑓 (𝑥) ∈ R^𝐿$，它产生每个标签的概率。然后我们==获得具有 top-𝑘 概率的标签==，或者==设置一个概率阈值来获得预测标签的动态数量==。

<img src="D:\Notes\raw_images\image-20220914103331886.png" alt="image-20220914103331886" style="zoom:80%;" />

**预训练任务**

![image-20220914104348662](D:\Notes\raw_images\image-20220914104348662.png)

预训练任务由两个连续的任务组成：

1）**随机子字符串分类（RSC）**是指我们从 item 标题中获取随机子字符串作为查询语句，并将查询语句的类别预测为item的类目的任务。形式上，我们根据 [0, 标题长度之间] 的均匀随机选择item标题的起始位置 start，并从 [1 , 最大长度参数] （在我们的模型中为 5）的均匀采样随机长度 𝑙 的子串。  

2) **掩蔽语言模型 (MLM)** 是指标准 BERT 预训练任务 [4]，它随机掩蔽Token并学习恢复被掩蔽的Token。我们不采用不适合我们场景的下一句预测（NSP）任务，因为 MLM 更适合学习上下文信息。我们遵循标准 MLM 设置，该设置随机屏蔽 15% 的令牌，并用 [MASK] 令牌替换其中的 80%，用随机令牌替换 10%。

**Fine-tuning 任务**

fine-tuning 步骤与上述pre-training非常相似，除了以下两个不同之处：

1）我们通过==聚合用户点击日志数据==来收集fine-tuning数据，我们收集用户点击最多的商品类别占总点击次数的 90%。因此，微调步骤中的训练实例可能包含一个和几个查询类别，这使得 ==fine-tuning任务成为多标签分类问题，而不是预训练步骤中的单标签分类问题==。 

2）我们应用==softmax温度策略== softmax temperature strategy [6] to maximize the margin between positive and negative categories。具体来说，我们在模型中使用**温度 1/3**。

### 向量检索

同京东的上一篇论文，双塔结构的模型，Loss采用Triplet Loss

![图片](https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPhdrfyQIIjlXhHGmia3QXs8b8U4ecwVCLr1YOGb9ZXibFaPX59s7OFngfYgwvmCF82sESj5QbPhq0mQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

Pre-training 和 FineTune 与上面的用户意图预测一致。

### **Experiment**

表 2 显示了我们的训练和评估数据的统计数据，这些数据都是从用户在 ==60 天内==点击登录==单个“Level-1”类别==的Item收集的。

<img src="D:\Notes\raw_images\image-20220914110030573.png" alt="image-20220914110030573" style="zoom:80%;" />

Note that all models are optimized by ==AdamW [16] optimizer==, and trained with weighted decayed ==learning rate from 1e-4== and ==batch size of 1024==.

<img src="D:\Notes\raw_images\image-20220914110937415.png" alt="image-20220914110937415" style="zoom:80%;" />

<img src="D:\Notes\raw_images\image-20220914111024805.png" alt="image-20220914111024805" style="zoom:80%;" />



