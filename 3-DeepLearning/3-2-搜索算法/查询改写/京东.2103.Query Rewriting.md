## Query Rewriting via Cycle-Consistent Translation for E-Commerce Search

论文地址：https://arxiv.org/abs/2103.00800

作者：Yiming Qiu, Kang Zhang, Han Zhang, Songlin Wang, Sulong Xu, Yun Xiao, Bo Long, Wen-Yun Yang

机构：京东搜索

发表：ICDE2021



### 摘要

如今，电子商务搜索已成为许多人购物习惯中不可或缺的一部分。当今电子商务搜索中的一个关键挑战是语义匹配问题，其中相关items可能不包含用户查询中的确切术语。在本文中，我们提出了一种新颖的基于深度神经网络的查询改写方法，以解决这个问题。

具体来说，我们将查询改写制定为==循环机器翻译问题==，以利用丰富的点击日志数据。然后，我们结合最先进的机器翻译模型引入了一种新颖的循环一致训练算法（a novel cyclic consistent training algorithm），在查询改写准确性方面实现最佳性能。为了使其在工业场景中实用，我们优化了语法树的构建，以降低计算成本和在线服务延迟。

离线实验表明，该方法能够将硬用户查询改写为更适合倒排索引检索的标准查询。与人工策划的基于规则的方法相比，所提出的模型显着==提高了查询改写的多样性，同时保持了良好的相关性==。在线 A/B 实验表明，它显着提高了核心电子商务业务指标。自 2020 年夏天以来，我们的搜索引擎生产中推出了上述的模型，为数亿用户提供服务。



### 介绍

然而，许多电子商务搜索查询并没有从传统搜索引擎获得令人满意的结果。这是由于电子商务搜索的性质：

- 商品标题通常很短，因此倒排索引很难检索，
- 大量新的互联网用户倾向于使用自然语言相似的搜索查询，例如“手机给爷爷”、“给女朋友的礼物”、
- 多义查询在电子商务搜索中更常见，例如，“apple”可能表示 Apple 公司的产品或水果苹果。

传统的网络搜索技术采用基于规则的查询改写，将原始查询转换为类似但更标准的查询语句，以解决上述困难查询。图 1 举例说明了这种查询重写在实践中是如何工作的。这些查询重写规则通常来源于：human compilation 人工编译、数据聚合等。然而，这些基于规则的方法需要大量的人力，成本高，耗时长，并且无法涵盖更微妙的情况和长尾情况。因此，非常需要能够解决这个问题的、可扩展、先进且强大的系统。

<img src="D:\Notes\raw_images\image-20220916155936342.png" alt="image-20220916155936342" style="zoom: 50%;" />

​										Fig. 1. Illustration of query rewriting process that retrieves more relevant results.

最近，还有另一种趋势是学习 embedding 表示来解决这个术语不匹配问题 [1] 和推荐问题 [2]、[3]。基本思想是将查询语句、用户特征以及item标题映射到语义 embedding 空间，查询其中相关项目。因此，可以通过embedding 空间中的最近邻搜索来检索到，不包含确切术语，但与Query语义相关的项目。然而，在实践中，我们发现这种方法存在缺点：

1）难以平衡语义匹配能力和过多的泛化，可能会检索到查询不相关的商品。例如， 具有特定型号或样式的项链，非常具体意图的搜索查询，会召回其他型号或样式的项链。 

2）很难决定从最近邻搜索中检索多少个商品（卡阈值不行么？）。对于一些具有非常特定意图的长尾查询，可用和相关项目的数量可能远小于设定的超参数值，即要检索的最近邻近的数量。因此，额外检索到的项目可能会给后面的相关评分阶段带来负担。

在本文中，我们从另一个角度开发一种新的语义匹配问题方法，我们将查询重写问题表述为循环机器翻译问题，==首先将查询转换为商品标题，然后再转换回查询==。为了引导循环翻译更好地适应查询重写任务，我们还引入了一个新的优化Term来鼓励循环翻译“翻译回”原始查询。

![image-20220919101349962](D:\Notes\raw_images\image-20220919101349962.png)

Given a click log data  , where x denotes query, y denotes item title, and N denotes the number of training samples, the standard training objective in most translation models is to maximize the log likelihood of the training data

给定一个点击日志数据 $D = \{ x^n, y^n\}_{n=1}^N$，其中 x 表示查询，y 表示项目标题，N 表示训练样本的数量，大多数翻译模型中的标准训练目标是最大化训练数据的对数似然（ log likelihood ）：
$$
L_f(\theta_f) = \sum_{n=1}^N log P(y^n|x^n; \theta_f) \\
L_b(\theta_b) = \sum_{n=1}^N log P(x^n|y^n; \theta_b)
$$
where$P(y^n|x^n; \theta_f)$ and $P(x^n|y^n; \theta_b)$ are query-to-title (forward) and title-to-query (backward) neural translation models, parameterized by $\theta_f$ and $\theta_b$, respectively.  请注意，下标 f 和 b 分别是forward 和 backward 的简写。两个目标函数 $L_f$ 和 $L_b$ 相互独立。因此，可以单独训练模型而不会损失准确性。

在实践中，我们发现查询到标题模型需要更多的记忆能力才能生成足够好的商品标题，这可能是因为目标商品标题通常比源查询长得多，并且商品标题空间可能比查询空间大得多。另一方面，我们发现标题到查询模型更像是文本摘要模型。因此，它不需要很大的模型来记忆。因此，我们为查询到标题模型选择 4 层 Transformer ，为标题到查询模型选择 1 层 Transformer 。

![image-20220919110013107](D:\Notes\raw_images\image-20220919110013107.png)

为了获得更好的查询重写模型，直觉是鼓励两个翻译模型可以协作“翻译回”到原始查询。因此，应该学习模型参数以最大化“翻译回”原始查询的可能性。形式上，我们==引入循环一致似然== $ L_c(\theta_f,\theta_b) $ 来鼓励两个模型协作“翻译回”原始查询，如下所示: 

<img src="D:\Notes\raw_images\image-20220919114403878.png" alt="image-20220919114403878" style="zoom: 80%;" />

最终的似然函数是等式（1）、（2）和（3）中前向、后向和循环一致性似然的线性组合，如下所示:
$$
L(\theta_f,\theta_b) = L_f(\theta_f) + L_b(\theta_b) +L_c(\theta_f,\theta_b)
$$
训练过程：考虑循环一致性仅在两个模型都经过良好训练时才有意义。因此，我们只在一定数量的预热步骤后执行等式（5）中的循环一致性项。

<img src="D:\Notes\raw_images\image-20220919115118777.png" alt="image-20220919115118777" style="zoom:80%;" />

解码过程：

<img src="D:\Notes\raw_images\image-20220919142208522.png" alt="image-20220919142208522" style="zoom:80%;" />

线上部署：1. Transformer 改 RNN。 2. 采用同义词Pair。

### 实验

<img src="D:\Notes\raw_images\image-20220919142657934.png" alt="image-20220919142657934" style="zoom:80%;" />![image-20220919142742781](D:\Notes\raw_images\image-20220919142742781.png)

<img src="D:\Notes\raw_images\image-20220919142749066.png" alt="image-20220919142749066" style="zoom: 67%;" />

<img src="D:\Notes\raw_images\image-20220919143002140.png" alt="image-20220919143002140" style="zoom:80%;" />

<img src="D:\Notes\raw_images\image-20220919143028716.png" alt="image-20220919143028716" style="zoom: 80%;" />