# 基于深度学习的视频目标检测综述

文摘来源：https://zhuanlan.zhihu.com/p/67608224

最近对深度学习在视频任务中的应用做了个简单调研，切入点是视频目标检测，刚开始调研的时候很乐观，本想着作为研究课题继续研究，但是随着调研深入，到最后发现这个领域还是慎入。。。在这里把调研报告放出来吧。

**摘要：**近些年来，深度卷积神经网络在图像目标检测领域迅速普及，而且相较于传统方法取得了很好的效果，基于深度学习的图像目标检测也逐渐合称为一个统一的深度网络框架。在图像目标检测任务取得了不错的效果后，深度学习又迁移到基于视频的目标检测任务上。本文系统总结基于深度学习的视频目标检测方法，==归为2类：基于检测和跟踪的深度学习视频目标检测方法以及基于光流等动态信息的深度学习目标检测算法。== 本文通过细致探究这些方法，并进行横向的对比，结合在ImageNet VID数据集上的实验，相近分析每个方法的优势和劣势以及他们之间的联系。

**关键词：**深度卷积网络，视频目标检测，光流信息，循环神经网络

1. **绪论**（东拼西凑）

目标检测是计算机视觉领域的一个经典的任务，是进行场景内容分析和理解等高级视觉任务的基本前提。视频中的目标检测任务更是和现实生活的需求贴近，现实生活中的智能视频监控、机器人导航等应用场景都需要对视频进行处理，对视频中的目标进行检测。==视频中的目标检测需要在静态图像目标检测的基础上对目标因运动产生的各种变化进行处理==，这是其中的难点。

基于视频的目标检测任务相比于静态图像的目标检测任务，目标的外观、形状、尺度等属性会随着目标的运动发生变化，在检测过程中如何保持时间顺序上目标的一致性从而不会使目标在中间某帧丢失，这是视频目标检测任务的主要难点。由于视频比静态图像多了一个时间维度上的信息，所以很多视频目标检测算法利用该信息来增强检测性能。本文对近几年深度学习在视频目标检测任务上的工作进行了总结，将其分为两个不同的技术流派。

- 第一种是对视频中每一帧进行目标检测，然后在使用跟踪算法对目标框进行跟踪，使用跟踪的结果对之前的检测结果进行修正。

- 另一种是利用目标在视频中因为运动产生变化的信息直接在视频上进行目标检测，比如光流就是其中一种比较有效地运动信息，会随着时间的推移，成为目标运动有效地判别信息，利用这些信息算法可以将发生变化的目标也高效的检测出来，保证了检测的准确性和鲁棒性。



**2. 基于深度学习的视频目标检测**

**2.1 视频目标检测任务中的挑战**

视频目标检测任务中最大的困难就是如何保持视频中目标的时空一致性。众所周知，视频是由多个有序列关系的图像组成的，该序列关系是一种时间的序列关系，因此其中的目标如果在运动，那么空间位置以及其自身的属性会发生变化，而且这种变化与时间存在紧密的关系，但即使发生了变化，每一帧的发生变化的目标还是属于同一个目标，这就叫做==时空一致性==。

具体的挑战分为以下五类：

(a). 运动模糊：目标在现实中的运动实际上是一个连续的过程，但是摄像机是按照一幅一幅图的取景，每次捕捉图像的时候，会用快门控制外景进入相机内部成像组件，而快门闭合的速度就决定每次取景有多长时间外景被捕捉到，所以就会出现目标一段运动中多个被合成为一个的情况，也就产生了运动模糊。这个时候，目标的外观并不容易被分辨，如图1所示。

![img](https://pic4.zhimg.com/80/v2-3d567367b2ee54b849b841d15b4cc6bb_hd.jpg)

(b). 虚焦：这是因为摄像设备并没有一直准确的对目标进行对焦产生的问题。这种情况可能会使视频一种一些包含目标的帧中目标出现模糊或者使视频一些帧的图像产生整体的模糊，如图2所示。

![img](https://pic4.zhimg.com/80/v2-db58f64c1664180cfbf4d906eb0c2e9b_hd.jpg)

(c). 遮挡：目标在运动过程中，可能会出现运动到一些物体的后方，而被该物体遮挡，如图3所示。

![img](https://pic3.zhimg.com/80/v2-fd2493b6ce0975b4c4a6e1968adb42ea_hd.jpg)

(d). 外观变化：视频中目标可能会出现外观上的变化，例如图4中所示变色龙出现罕见的姿势。

![img](https://pic1.zhimg.com/80/v2-fde596c4ad282fdc3741e231f0907050_hd.jpg)

(e). 尺度变化：目标在运动过程中，从镜头中远的地方运动到近的地方，其外观的尺寸可能会发生变化。

**2.2 检测和跟踪结合的视频目标检测算法**

保持目标时空一致性的一个直接的思路就是，对目标进行跟踪，在视频中跟踪检测出来的目标。代表工作是T-CNN[1] ，2016。接下来我们将对该方法进行简要介绍。

本文的发现是==视频目标追踪任务学习到的时间信息与静态目标检测任务学习到的空间特征相结合可以提升视频目标检测任务的性能==。

T-CNN提出了一个基于深度学习的目标检测和跟踪的多阶段的框架，具体由==管道提取==模块和==管道分类==以及==重打分==模块构成，如图5所示：

![img](https://pic1.zhimg.com/80/v2-23bbb1896d909fd439bb7962286cdae4_hd.jpg)

​																				图5 T-CNN算法架构

管道指的是视频对每个目标在每一帧提取的检测框按照时间序列顺序连接在一起，就构成了一个管道。这里管道提取模块主要由三个步骤完成：静态图像的目标检测，先用Selective Search方法提取很多目标候选框，然后使用CNN对这些框进行分类，去除背景框；第二，使用R-CNN对目标框进行分类别的打分；第三，对高置信度的框进行跟踪。

重打分模块，利用一个Temporal Convolutional Network(TCN)网络进行时域卷积。TCN的输入是每个管道的各种属性，例如检测得分，跟踪得分，坐标以及时间区段。最后会输出重打分的结果。

本文利用跟踪算法来与目标检测算法相结合，两者相辅相成，增强了视频目标检测的性能。但是该方法也有一定局限性：1、依赖视频目标追踪算法的性能；2、算法过程较为复杂，实时性不够高；3、并没有直接的针对模糊、运动等视频信息的进行利用并加以解决视频中为出现类似的实际情况。

**2.3 利用运动信息的视频目标检测算法**

考虑到上述方法的局限性，研究者希望直接利用视频中的运动信息进行视频目标检测。目前使用的比较多的运动信息是==光流。是空间运动物体在观察成像平面上的像素运动的瞬时速度，是利用图像序列中像素在时间域上的变化以及相邻帧之间的相关性来找到上一帧跟当前帧之间存在的对应关系，从而计算出相邻帧之间物体的运动信息的一种方法==。一般而言，==光流是由于场景中前景目标本身的移动、相机的运动，或者两者的共同运动所产生的==。举例如图6所示：

![img](https://pic3.zhimg.com/80/v2-ebe0ed077d24abdf85eadc499109a622_hd.jpg)

​																				图6 光流信息示例

（1）Flow-Guided Feature Aggregation, 2017

这方面比较早的尝试是Flow-Guided Feature Aggregation[2]. 2017。本文的发现是视频中的光流信息和时间信息可以提升视频目标检测任务的性能。

FGFA主要由==光流提取==和==特征融合==两个模块构成。==光流利用FlowNet[flownet]网络进行提取。每次提取当前帧到相邻帧的光流，并将相邻帧的特征按照提取到的光流和当前帧的特征组合在一起。组合完成后，就进入特征融合阶段，将当前特征和与其相邻的多个特征进行融合==。本文中使用了==元素直接记性权值求和==的方式进行融合。==这里的权值衡量的是光流提取阶段得到的组合特征与相邻帧的特征的相似程度==，文中利用余弦相似度进行描述。算法的具体架构如图7所示。

本文利用了光流信息，并用特征融合增强了特征的判别度，因此在视频目标检测中取得了比较好的效果。但是仍然有一些不足：1、计算多帧的光流然后进行特征融合的==计算量非常大==；2、特征融合的权重是一个==cosine权重==，该方法比较简单粗暴，有提升的空间。

![img](https://pic3.zhimg.com/80/v2-0f87ed792a2df4c794504e90b44deb7e_hd.jpg)

​																			图7 FGFA算法架构

（2）Association LSTM, 2017

FGFA离线提取光流导致一方面计算量非常大，这和视频应用所要求的实时性不符；另一方面，==离线的光流提取网络并不能在训练阶段进行反向传播修正==。因此有研究者提出了==在线利用运动信息==的视频目标检测算法，Association LSTM[3], 2017。

本文的研究基础是==认为LSTM可以充分学习视频序列中的时间信息==。本文提出的方法主要由静态图像目标检测算法SSD和LSTM组成。SSD在视频的每一帧进行目标检测，按照SSD检测结果提取目标的特征，然后进行堆叠，送入LSTM，LSTM可以在同一时间处理多个目标框，同时也有记忆特性，这一特性十分适合基于时序的视频目标检测任务。本文的LSTM对每一帧处理完后，除了边框的Regression Error，还会在相邻两帧LSTM的输出结果上额外计算一个Association Error。Association Error就体现了两帧在时序上的一致性差异，最小化该Loss就能保持目标的时空一致性。算法流程如图8所示。

![img](https://pic1.zhimg.com/80/v2-77915a754d5f65db0bafcb3ee8ad4324_hd.jpg)图8 Association LSTM算法架构

本文的想法很直观，而且很有效，充分的利用了LSTM的优势，增强了视频目标检测算法的鲁棒性。但是本方法存在一些局限性：1、严重依赖==与进行定位检测跟踪算法的性能==；2、==相邻帧所反映的目标变化信息有局限性==，只能反映出短期的运动信息，而==对长时间的运动信息无能为力==。

（3）Aligned Spatial-Temporal Memory, 2018

相比Association LSTM方法，本文对多帧体现的运动信息进行学习。本文提出的算法结构叫做Spatial-Temporal Memory Network(STMM)[4], 2018，实际上是一种==循环卷积网络==。对于当前帧，通过在相邻多个连续帧上进行卷积堆栈以获得保留了空间特性的特征，然后将其送入STMM模块。然后将来自当前帧的STMM输出送到分类和回归子网络中。

STMM是一个==双向的循环卷积网络==，所以该模块既可以利用少数几帧相邻帧之间目标的运动信息，也可以学习到目标长时间段的运动和外观变化信息。具体算法结构如图9所示。

![img](https://pic3.zhimg.com/80/v2-2529401654fc4ada7019756f1049a5ce_hd.jpg)图9 STMM算法架构

本文在运动信息的利用方面更进一步，==可以学习到长期的运动信息==，从而提升视频目标检测算法的性能。但是本文的方法仍然有一些缺陷：1、LSTM算法==并行程度不高，训练效率受限==；2、==相邻帧所反映的目标变化信息有局限性==，因为本文虽然学习了长期的运动信息，但是从相距比较远的帧的运动信息还是通过相邻帧一步一步从STMM单元传送到当前帧的，这样是一种间接的方式，效率并不算太高。

（4）MANet, 2018

本文也是试图学习多个相邻帧所以先出的目标运动信息。相比较于STMM方法，本文还使用了==多帧特征融合的方法，可以直接学习到长时间段的运动信息==。

本文的研究发现是==全局目标和光流特征==与==局部目标和光流特征==的==结合可以增强检测器的性能==。MANet[5]先在图像提取全局特征，同时计算帧之间的光流信息，然后将图像提取的特征和光流信息一起融合，这两个步骤完成了特征和光流提取以及像素级别的特征校准。接下来在全局特征中针对目标实例利用运动信息继续进行特征校准。==最后将像素级别校准后的特征和实例级别校准后的特征进行融合，用来训练和测试。==整个算法结构如图10所示。

![img](https://pic2.zhimg.com/80/v2-d2a304134e6ad5e2beb44528971514a5_hd.jpg)图10 MANet算法架构

本文的想法集合了前人研究中的优势，取得了足够好的性能。可能存在的不足之处就是==特征融合使用了直接相加的方法，该方法比较简单粗暴，可能成为性能进一步提升的瓶颈==。



**3. 实验及结果分析**

视频目标检测任务中主要由两个数据集。第一个是ImageNet VID数据集，该数据集由三个不同的分割组成。 1）训练集包含1952个完全标记的视频片段，每个片段从6帧到5213帧不等。 2）验证集包含281个完全标记的视频片段，每个片段从11帧到2898帧。 3）测试集包含458个片段，实际上真实的标注尚未公开，所有的算法都将验证集上进行验证。第二个数据集是YTO数据集，该数据集包含10个类别，它们是ImageNet VID数据集的子集。与包含所有视频帧上的完整注释的VID数据集不同，YTO数据集被弱注释，即每个视频仅被确保包含相应类的一个对象，并且每个视频仅注释非常少的帧。

![img](https://pic4.zhimg.com/80/v2-b25f1ed7196cdc3240c23851709c290b_hd.jpg)

表1体现了上述五种算法在两个数据集上取得的性能。在YTO数据集上Association LSTM的效果比T-CNN差一些，这是因为YTO并不是强标注的，没有强标注数据对于深度神经网络来说训练比较困难，而T-CNN中使用了跟踪的方法，实际上也算是对弱标注的一种加强型标注，所以效果会好一些。而ImageNet VID这种严格标注的数据集上，T-CNN方法的效果就是最差的，利用光流学习的FGFA会好一些，最好的两个方法是利用了多帧的更加复杂有效的光流运动信息学习的STMM和MANet方法。



**4. 结语**

近几年基于深度学习的视频目标检测算法发展迅猛，具有如下研究热点：==学习帧之间的光流等运动信息==，因为运动信息是保持视频目标时空一致性的重要判别信息，所以算法利用离线或者在线的方式进行学习，离线学习方法不能方向传播就修正，而在线光流学习的方法可以在训练期间进行修正，因此会有更好的性能；另一个研究热点是==对多帧特征进行融合==，这种方式会更好的利用视频序列中帧之间的运动所产生的变化信息。目前阶段视频目标检测算法的性能已经不错，而计算量和实时性是一个制约其应用的亟需解决的问题。



**参考文献：**

[[1\] T-CNN—K. Kang, W. Ouyang, H. Li, X. Wang. Object Detection from Video Tubelets with Convolutional Neural Networks. CVPR2016](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1604.04053)

[[2\] FGFA — XZ. Zhu, YJ. Wang, JF. Dai, Y. Lu, YC. Wei. Flow-Guided Feature Aggregation for Video Object Detection. ICCV2017](https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_Flow-Guided_Feature_Aggregation_ICCV_2017_paper.pdf)

[[3\] Association-LSTM — YY. Lu, CW. Lu, CK. Tang. Online Video Object Detection using Association LSTM. ICCV2017](https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ICCV_2017/papers/Lu__Online_Video_ICCV_2017_paper.pdf)

[[4\] STMN — FY. Xiao, YJ. Lee. Video Object Detection with an Aligned Spatial-Temporal Memory. ECCV2018](https://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1712.06317v2.pdf)

[[5\] MANet — SY.Wang, YC. Zhou, YJ.Yan, ZD. Deng. Fully Motion-Aware Network for Video Object Detection. ECCV2018](https://link.zhihu.com/?target=https%3A//link.springer.com/chapter/10.1007/978-3-030-01261-8_33)

