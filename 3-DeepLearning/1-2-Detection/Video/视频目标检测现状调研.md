## 视频目标检测 - 目前最佳方法调研

Papers With Code：

https://paperswithcode.com/task/video-object-detection

ImageNet VID Dataset: 

![1574323025652](D:\Notes\raw_images\1574323025652.png)

### Sequence Level Semantics Aggregation for Video Object Detection

论文地址：https://arxiv.org/abs/1907.06390v2

作者：Haiping Wu, Yuntao Chen, Naiyan Wang, Zhaoxiang Zhang

发表：ICCV2019 Oral

机构：麦吉尔大学（加拿大），中科院，图森

代码地址：https://github.com/happywu/Sequence-Level-Semantics-Aggregation

 MXNet implementation，基础网络Resnet101，。。。。。。实时性很差。。。

摘要：

视频目标检测主要问题在由==快速运动引起的视频帧的外观退化==，这种问题在单帧中更为显现。因此,从其他帧融合特性成为自然的选择，现有的方法严重依赖于==光流==或==递归神经网络==的特征融合。然而, 这些方法更强调时间相邻帧的作用。在这项工作中,==我们认为,在全序列的特性融合在视频对象检测中更 discriminative and robust==。为了实现这个目标,我们设计了一个新颖的==序列层次语义融合==(SELSA)模块。

我们进一步证明了所提出的方法与经典的光谱聚类方法之间的密切关系,为理解VID问题提供了一种新颖的观点。

我们在ImageNet VID和 EPIC KITCHENS数据集测试提出的方法,并实现新的最先进的结果。我们的方法不需要复杂的后处理方法,如Seq-NMS或Tubelet rescoring, 这使得pipeline 保持简单。

![1574326874651](D:\Notes\raw_images\1574326874651.png)

![1574392601034](D:\Notes\raw_images\1574392601034.png)

### Integrated Object Detection and Tracking with Tracklet-Conditioned Detection

论文地址：https://arxiv.org/abs/1811.11167v1

作者：Zheng Zhang, Dazhi Cheng, Xizhou Zhu, Stephen Lin, Jifeng Dai

机构：MSRA, 中科院，北京理工大学

发表： CVPR2019

摘要：

准确的对象检测和跟踪对于视频理解至关重要。在以前的工作中,这两项任务经常被结合在一起, 跟踪效果大量地依赖于检测,但检测从跟踪中获益较少。为了增加协同作用,  我们建议==通过在前帧中计算的tracklets来调节对象检测==。通过这种方法,==对象检测结果不仅具有较高的检测响应,而且提高了轨迹的一致性==。这种更连贯性导致了估计的物体轨迹,比没有跟踪条件的检测的抖动路径更平稳、更稳定。在广泛的实验中, 这种方法被证明在检测和跟踪精度方面取得了先进的性能,以及跟踪稳定性的显著改进。

![1574392861859](D:\Notes\raw_images\1574392861859.png)

### Flow-Guided Feature Aggregation for Video Object Detection

论文地址:https://arxiv.org/abs/1703.10025v2

作者：Xizhou Zhu, Yujie Wang, Jifeng Dai, Lu Yuan, Yichen Wei

机构：中国科技大学，微软

代码地址：https://github.com/msracver/Flow-Guided-Feature-Aggregation

摘要：

FGFA主要由==光流提取==和==特征融合==两个模块构成。==光流利用FlowNet[flownet]网络进行提取。每次提取当前帧到相邻帧的光流，并将相邻帧的特征按照提取到的光流和当前帧的特征组合在一起。组合完成后，就进入特征融合阶段，将当前特征和与其相邻的多个特征进行融合==。本文中使用了==元素直接记性权值求和==的方式进行融合。==这里的权值衡量的是光流提取阶段得到的组合特征与相邻帧的特征的相似程度==，文中利用余弦相似度进行描述。算法的具体架构如图7所示。

本文利用了光流信息，并用特征融合增强了特征的判别度，因此在视频目标检测中取得了比较好的效果。但是仍然有一些不足：1、计算多帧的光流然后进行特征融合的==计算量非常大==；2、特征融合的权重是一个==cosine权重==，该方法比较简单粗暴，有提升的空间。![1574393370681](D:\Notes\raw_images\1574393370681.png)

Figure 1. Illustration of FGFA (flow-guided feature aggregation). For each input frame, a feature map sensitive to “cat” is visualized. The feature activations are low at the reference frame t, resulting in
detection failure in the reference frame. The nearby frames t − 10 and t + 10 have high activations. After FGFA, the feature map at the reference frame is improved and detection on it succeeds.