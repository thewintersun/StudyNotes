#### Joint Discriminative and Generative Learning for Person Re-identification

论文地址：https://arxiv.org/abs/1904.07223 

作者：Zhedong Zheng, Xiaodong Yang, Zhiding Yu, Liang Zheng, Yi Yang, Jan Kautz

出版：CVPR 2019 (Oral) 

机构：NVIDIA CAI, University of Technology Sydney， Australian National University 

代码地址：https://github.com/NVlabs/DG-Net

代码地址：https://github.com/layumi/Person_reID_baseline_pytorch

#### 摘要

行人Re-ID因为在不同摄像头下类内差异（intra-class variations)很大，至今仍然是很具有挑战的工作。

最近，用生成模型来做数据增强变得流行起来，这种方法可以增强网络对输入数据的不变形。然而，现有的生成方法，还没有达到可以辨别Re-ID的阶段。因此，Re-ID模型一般都是直接在生成的数据上直接进行训练。

本文提出一个联合学习的框架，couples Re-ID学习和数据生成 end-to-end。

生成模块分别 encodes 行人的appearance code 和structure code. 一个辨别模块和生成模块共享行人的appearance code。通过switching the appearance or structure codes，生成模块能够生成高质量的跨id组合图像，在线反馈到appearance encoder并用于改进判别模块。



#### 介绍

不同摄像头捕获的图片通常包含巨大的类内差异，由背景，观察角度（viewpoint），人体姿态等因素导致。

Current state-of-theart re-id methods widely formulate the tasks as ==deep metric learning problems== [13, 55], 

> Alexander Hermans, Lucas Beyer, and Bastian Leibe. In defense of the triplet loss for person re-identification. arXiv:1703.07737, 2017.
>
> Zhedong Zheng, Liang Zheng, and Yi Yang. A discriminatively learned CNN embedding for person reidentification. TOMM, 2017.

or use classification losses as the proxy targets to learn deep embeddings [23, 39, 41, 49, 54, 57]. 

> Wei Li, Xiatian Zhu, and Shaogang Gong. Person re-identification by deep joint learning of multi-loss classification. In IJCAI, 2017.
>
> Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin Wang. Beyond part models: Person retrieval with refined part pooling. In ECCV, 2018.
>
> Zheng Tang, Milind Naphade, Ming-Yu Liu, Xiaodong Yang, Stan Birchfield, Shuo Wang, Ratnesh Kumar, David Anastasiu, and Jenq-Neng Hwang. CityFlow: A city-scale benchmark for multi-target multi-camera vehicle tracking and re-identification. In CVPR, 2019.
>
> Yu Wu, Yutian Lin, Xuanyi Dong, Yan Yan, Wei Bian, and Yi Yang. Progressive learning for person re-identification with one example. TIP, 2019.
>
> Liang Zheng, Yi Yang, and Alexander Hauptmann. Person reidentification: Past, present and future. arXiv:1610.02984, 2016.
>
> Zhedong Zheng, Liang Zheng, and Yi Yang. Pedestrian alignment network for large-scale person re-identification. TCSVT, 2018.

To further ==reduce the influence from intra-class variations==, a number of existing methods adopt ==part-based matching== or ensemble to ==explicitly align and compensate the variations== [35, 37, 47, 52, 57].

> Chi Su, Jianing Li, Shiliang Zhang, Junliang Xing, Wen Gao, and Qi Tian. Pose-driven deep convolutional model for person reidentification. In ICCV, 2017.
>
> Yumin Suh, Jingdong Wang, Siyu Tang, Tao Mei, and Kyoung Mu Lee. Part-aligned bilinear representations for person reidentification. In ECCV, 2018.
>
> Longhui Wei, Shiliang Zhang, Hantao Yao, Wen Gao, and Qi Tian. Glad: global-local-alignment descriptor for pedestrian retrieval. In ACM MM, 2017.
>
> Haiyu Zhao, Maoqing Tian, Shuyang Sun, Jing Shao, Junjie Yan, Shuai Yi, XiaogangWang, and Xiaoou Tang. Spindle net: Person reidentification with human body region guided feature decomposition and fusion. In CVPR, 2017.
>
> Zhedong Zheng, Liang Zheng, and Yi Yang. Pedestrian alignment network for large-scale person re-identification. TCSVT, 2018.

生成模型尽管形式不同，但总的考虑因素这些方法背后是“现实主义”：生成的图像应该具备良好的品质来缩小合成场景与真实场景之间的领域差距; 和“多样性”：生成的图像应包含足够的多样性充分覆盖看不见的变化。 在这种背景下，一些先前的作品探索了==无条件的GAN和人体姿势条件GAN== [10,17,27,31,56]生成行人图像，以改善重新学习。

> Yixiao Ge, Zhuowan Li, Haiyu Zhao, Guojun Yin, Xiaogang Wang, and Hongsheng Li. FD-GAN: Pose-guided feature distilling GAN for robust person re-identification. In NeurIPS, 2018.
>
> Yan Huang, Jinsong Xu, Qiang Wu, Zhedong Zheng, Zhaoxiang Zhang, and Jian Zhang. Multi-pseudo regularized label for generated samples in person re-identification. TIP, 2018.
>
> Jinxian Liu, Bingbing Ni, Yichao Yan, Peng Zhou, Shuo Cheng, and Jianguo Hu. Pose transferrable person re-identification. In CVPR, 2018.
>
> Xuelin Qian, Yanwei Fu, Tao Xiang, Wenxuan Wang, Jie Qiu, Yang Wu, Yu-Gang Jiang, and Xiangyang Xue. Pose-normalized image generation for person re-identification. In ECCV, 2018.
>
> Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled samples generated by GAN improve the person re-identification baseline in vitro. In ICCV, 2017.

现阶段的GAN方法中，生成模型和判别模型基本上是分开的，这限制了生成数据的利用程度。因此，作者提出了一个新的学习的框架称为DG-Net，在统一的网络结构中联合学习判别和生成。 首先引入生成模块，编码器分解每个行人图像到两个潜在的空间：

- 一个外观空间，主要是编码外观和其他identity 相关的语义; 
- 一个结构空间，主要是编码几何和位置相关的结构信息，以及其他额外变化。 

我们将空间中的编码特征作为“代码”（codes）。两个空间的定义如下表：

 ![1563939459436](D:\Notes\raw_images\1563939459436.png)

外观空间是生成模块和判别模块共享的。

图像生成方法：通过交换两个图像的外观或结构代码。

![1563939718258](D:\Notes\raw_images\1563939718258.png)

对于一个行人图像，通过保持其外观代码并结合不同的结构代码，我们可以生成多个图像，这些图像保留衣服和鞋子，但改变姿势，视点，背景等。

#### 方法

![1564627068149](D:\Notes\raw_images\1564627196328.png)

图2：DG-Net的示意图。 （a）我们的判别性重新学习模块嵌入在生成中模块通过共享外观编码器Ea。==黑色虚线表示结构编码器Es的输入图像灰度化==，红线表示生成的图像在线反馈给Ea。 在生成中强制执行两个目标模块。

![1564627276527](D:\Notes\raw_images\1564627276527.png)

（b）通过相同的输入标识生成自身标识

生成图片$x_i^t$ , 上标t, 表示appearance code的来源图片ID, 下标i，表示structure code的来源图片ID。

所有Structure Encoder的输入图片进行灰度化处理，为了让$E_S$ 更专注于结构信息。

首先，同一个人的同一张照片，$x_i$重构出$x_i^i$ ， 这种简单的自我重建任务可以作为生成器的重要正规化角色，Loss为简单的像素级L1Loss 。

![1564713747457](D:\Notes\raw_images\1564713747457.png)

同一个人的不同照片，Loss 也是像素级的L1 Loss。

![1564714659313](D:\Notes\raw_images\1564714659313.png)

同时，为了使不同图片的appearance code 相互分隔，采用身份鉴别Loss来区分不同的人，

![1564714912900](D:\Notes\raw_images\1564714912900.png)

where $p(y_i|x_i)$ is the predicted probability that $x_i$ belongs to the ground-truth class $y_i$ based on its appearance code.

![1564627297970](D:\Notes\raw_images\1564627297970.png)

（c）通过不同的输入标识生成交叉标识。

不同身份的两张图片，进行重构，不能再用像素级的Loss，$x_i$与$x_j$ 对应的 $y_i \neq y_j$ 

![1564715318501](D:\Notes\raw_images\1564715318501.png)

同样身份鉴别Loss：

![1564715628296](D:\Notes\raw_images\1564715628296.png)

同时，引入GAN的判别器Loss：

![1564715744760](D:\Notes\raw_images\1564715744760.png)

![1564627317288](D:\Notes\raw_images\1564627317288.png)

（d）为了更好地利用生成的数据，重新学习涉及主要特征学习和细粒度特征挖掘。

鉴别模块，是与生成模块相嵌套的，与它共享Appearance Encoder架构，在$E_a$的基础上，增加两个分支，主要特征学习，和细粒度特征挖掘。（primary feature learning and fine-grained feature mining ）

**主要特征学习**

对于交差ID的生成图像，我们==采用教师模型==，来给它打动态软标签。这个教师模型只是一个CNN结构的用来做身份分类的模型。==最小化预测的概率分布$p(x_i|j)$和教师模型推理的概率分布$q(x_i|j)$ 之间的KL散度==。 ![1564716551952](D:\Notes\raw_images\1564716551952.png)

K是类别数。为什么要用教师模型：In comparison with the fixed one-hot label [31, 62] or static smoothing label [56], this dynamic soft labeling fits better in our case, as each synthetic image is formed by the visual contents from two real images.

> [31] Xuelin Qian, Yanwei Fu, Tao Xiang, Wenxuan Wang, Jie Qiu, Yang Wu, Yu-Gang Jiang, and Xiangyang Xue. Pose-normalized image generation for person re-identification. In ECCV, 2018. 2, 3, 4, 5, 6, 7, 8
>
> [62] Yang Zou, Zhiding Yu, Vijaya Kumar, and Jinsong Wang. Unsupervised domain adaptation for semantic segmentation via classbalanced self-training. In ECCV, 2018. 5
>
> [56] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled samples generated by GAN improve the person re-identification baseline in vitro.In ICCV, 2017. 2, 3, 4, 5, 8

**细粒度特征挖掘**

为了模拟同一个人穿着不同的衣服，判别器模型，被强迫学习，细粒度的，身份相关的特征 (比如，头昏，帽子，包，身体大小，等等)，这些都是与衣服无关的。这里呢，我们把提供Structure Code的图片的ID作为，同一个Structure Code与不同Appearance Code生成图片的target。（这里与最开始的$L_{id}^c$ 相反 ）

![1564717428153](D:\Notes\raw_images\1564717428153.png)

总结：我们 treat 生成的图像 into two differenct perspectives:主要特征学习和细粒度特征挖掘，前者关注的地方structure-invariant clothing information和后者关注 appearance-invariant structurual clues。

#### 优化方法

Loss的组成：

![1564727784542](C:\Users\j00496872\AppData\Roaming\Typora\typora-user-images\1564727784542.png)

where $L_{recon}^{img} = L_{recon}^{img1}+L_{recon}^{img2}$ is the image reconstruction loss in self-identity generation, 

$L_{recon }^{code} = L_{recon }^{code1} +L_{recon}^{code2}$ ist he latent code reconstruction loss in cross-identity generation,
$\lambda_{img}, \lambda_{id}, \lambda_{prim},$ and $ \lambda_{fine} $are weights to control the importance of related loss terms.

$\lambda_{img} = 5$ 设置，$\lambda_{id} = 0.5$ 。

We do not involve the discriminative feature learning losses $L_{prim}$ and $L_{fine}$until the generation quality
is stable. As an example, we add in the two losses ==after 30K iterations on Market-1501==, then linearly increase  $\lambda_{prim}$ from 0 to 2 in 4K iterations and set $\lambda_{fine}=0.2*\lambda_{prim}$

Similar to the alternative updating policy for GANs, in the cross-identity generation as shown in Figure 2(a), we alternatively ==train Ea, Es and G before the generated image== and ==Ea, Es and D after the generated image==.

### 实验效果

![1564627836240](D:\Notes\raw_images\1564627836240.png)

Figure 3: Comparison of the generated and real images on Market-1501 across the different methods including LSGAN [29],PG2-GAN [28], FD-GAN [10], PN-GAN [31], and our approach. This figure is best viewed when zoom in. Please attention to both foreground and background of the images.

> [29] Xudong Mao, Qing Li, Haoran Xie, Raymond Lau, Zhen Wang, and Stephen Smolley. Least squares generative adversarial networks. In ICCV, 2017. 6, 7
>
> [28] Liqian Ma, Xu Jia, Qianru Sun, Bernt Schiele, Tinne Tuytelaars, and Luc Van Gool. Pose guided person image generation. In NeurIPS, 2017. 3, 6, 7
>
> [10] Yixiao Ge, Zhuowan Li, Haiyu Zhao, Guojun Yin, Xiaogang Wang, and Hongsheng Li. FD-GAN: Pose-guided feature distilling GAN for robust person re-identification. In NeurIPS, 2018. 2, 3, 4, 6, 7, 8
>
> [31] Xuelin Qian, Yanwei Fu, Tao Xiang, Wenxuan Wang, Jie Qiu, Yang Wu, Yu-Gang Jiang, and Xiangyang Xue. Pose-normalized image generation for person re-identification. In ECCV, 2018. 2, 3, 4, 5, 6, 7, 8

**对比实验，去除online-feeding, 去除teacher model supervision**

![1564728973454](D:\Notes\raw_images\1564728973454.png)

GAN生成效果评价：

![1564729278963](D:\Notes\raw_images\1564729278963.png)

不同数据集实验结果：

![1564729233034](C:\Users\j00496872\AppData\Roaming\Typora\typora-user-images\1564729233034.png)

![1564729343816](D:\Notes\raw_images\1564729343816.png)

![1564729359473](D:\Notes\raw_images\1564729359473.png)