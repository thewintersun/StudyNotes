## XGBoost 三种特征重要性计算方法对比

文章地址：https://zhuanlan.zhihu.com/p/355884348



### 作用与来源

特征重要性，我们一般用来观察不同特征的贡献度。排名靠前的，我们自然而然的认为，它是重要的。

这一思路，通常被用来做**特征筛选**。剔除贡献度不高的尾部特征，增强模型的鲁棒性的同时，起到特征降维的作用。

另一个方面，则是用来做**模型的可解释性**。我们期望的结果是：重要的特征是符合业务直觉的；符合业务直觉的特征排名靠前。

在实际操作中，我们一般用树模型的分类节点做文章。常用的就是 XGBoost 和其他一般树模型。



### XGBoost 内置的三种特征重要性计算方法

XGBoost 中常用的三种特征重要性计算方法，以及它的使用场景。除此之外，再看两个第三方的特征重要性计算方法，跳出内置函数，思考其中的差异。

最后回到类似的树模型特征计算方法，进行特征重要性的一般方法总结。以下场景非特殊说明，均针对 python 包体下的 xgboost 和sklearn。

### weight

`xgb.plot_importance` 这是我们常用的绘制特征重要性的函数方法。其背后用到的贡献度计算方法为`weight`。

- ‘weight’ - the number of times a feature is used to split the data across all trees.

简单来说，就是在子树模型分裂时，用到的特征次数。这里计算的是所有的树。这个指标在R包里也被称为`frequency_2`

### gain

`model.feature_importances_` 这是我们调用特征重要性数值时，用到的默认函数方法。其背后用到的贡献度计算方法为`gain`。

- ‘gain’ - the average gain across all splits the feature is used in.

gain 是信息增益的泛化概念。这里是指，节点分裂时，该特征带来信息增益（目标函数）优化的平均值。

### cover

`model = XGBRFClassifier(importance_type = 'cover')` 这个计算方法，需要==在定义模型时定义==。之后再调用`model.feature_importances_` 得到的便是基于`cover`得到的贡献度。

- ‘cover’ - the average coverage across all splits the feature is used in.

cover 形象来说，就是树模型在分裂时，特征下的叶子结点涵盖的样本数除以特征用来分裂的次数。分裂越靠近根部，cover 值越大。



### 使用场景

weight 将==给予数值特征更高的值，因为它的变数越多，树分裂时可切割的空间越大==。所以这个指标，==会掩盖掉重要的枚举特征==。

gain 用到了熵增的概念，它可以方便的找出最直接的特征。即如果某个特征的下的0，在label下全是0，则这个特征一定会排得靠前。

cover 对于枚举特征，会更友好。同时，它也没有过度拟合目标函数，不受目标函数的量纲影响。

调用它们的方式如下

```python
# Available importance_types = ['weight', 'gain', 'cover', 'total_gain', 'total_cover']
f = 'gain'
XGBClassifier.get_booster().get_score(importance_type= f)
```

举个例子，我们来做西瓜分类任务。西瓜有颜色、重量、声音、品种、产地等各类特征。其中，生活（业务）经验告诉我们，声音响的瓜甜。我们构建了这么一个模型，判断西瓜甜不甜。输出三类特征重要性。这时会看到一些矛盾的现象。

在 `weight` 解释下，==声音这类枚举值特征并不会很靠前。反而是重量这类连续特征会靠前。==为什么会这样子，是因为连续特征提供了**更多的切分状态空间**，这样的结果，势必导致它在树模型的分裂点上占据多个位置。与此同时，**重要的枚举值特征靠后**了。而且我们还能预见的是，同一个量纲下，上下界越大的特征，更有可能靠前。

如何解决这个矛盾点，那就是采用 `gain` 或者 `cover` 方法。因为声音这个特征，必然能带来更多的信息增益，减少系统的熵，所以它在信息增益数值上，一定是一个大值。在树模型的分类节点上，也一定是优先作为分裂点，靠近根部的。

在实践中，也会发现，`gain` 排出来的顺序的**头尾部值差距较大**，这是因为信息增益计算时，后续的优化可能都不是一个量级。类似于神经网络在优化损失函数时，后续的量纲可能是十倍、百倍的差异。所以，综上而言，如果**有下游业务方**，更建议用 `cover` 的特征重要性计算方法。当然，如果是单纯的模型调优，`gain` 能指出最重要的特征。这些特征，某些场景下还能总结成硬规则。