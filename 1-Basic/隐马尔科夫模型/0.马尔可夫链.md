## 马尔可夫链

**马尔可夫链**（英语：Markov chain），又称**离散时间马尔可夫链**（discrete-time Markov chain，缩写为**DTMC**），因俄国数学家[安德烈·马尔可夫](https://zh.wikipedia.org/wiki/马尔可夫)得名，为==状态空间==中经过从一个状态到另一个状态的转换的==随机过程==。

该过程要求具备==“无记忆”==的性质：==下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关==。这种特定类型的“无记忆性”称作马尔可夫性质。马尔科夫链作为实际过程的统计模型具有许多应用。

在马尔可夫链的每一步，系统根据概率分布，可以从一个状态变到另一个状态，也可以保持当前状态。状态的改变叫做转移，与不同的状态改变相关的概率叫做转移概率。随机漫步就是马尔可夫链的例子。随机漫步中每一步的状态是在图形中的点，每一步可以移动到任何一个相邻的点，在这里移动到每一个点的概率都是相同的（无论之前漫步路径是如何的）。

### 一个经典的马尔科夫链实例

用一句话来概括马尔科夫链的话，那就是某一时刻状态转移的概率只依赖于它的前一个状态。举个简单的例子，假如每天的天气是一个状态的话，那个今天是不是晴天只依赖于昨天的天气，而和前天的天气没有任何关系。这么说可能有些不严谨，但是这样做可以大大简化模型的复杂度，==因此马尔科夫链在很多时间序列模型中得到广泛的应用，比如循环神经网络RNN，隐式马尔科夫模型HMM等==。
假设状态序列为 $\cdots x_{t-2}, x_{t-1}, x_t, x_{t+1}, x_{t+2}, \cdots$, 由马尔科夫链定义可知，==时刻 $x_{t+1}$ 的状态只与 $x_t$ 有关==，用数学公式来描述就是：$P(x_{t+1}∣\cdots x_{t-2}, x_{t-1}, x_t)=P(x_{t+1}∣x_t)$

既然某一时刻状态转移的概率只依赖前一个状态，那么只要求出系统中任意两个状态之间的转移概率，这个马尔科夫链的模型就定了。

看一个具体的例子:

![img](D:\Notes\raw_images\1042406-20170328104002045-972507912.png)

这个马尔科夫链是表示股市模型的，共有三种状态：牛市（Bull market）, 熊市（Bear market）和横盘（Stagnant market）。每一个状态都以一定的概率转化到下一个状态。比如，牛市以0.025的概率转化到横盘的状态。这个状态概率转化图可以以矩阵的形式表示。如果我们定义矩阵阵P某一位置 $P(i, j)$ 的值为 $P(j|i)$，即从状态i变为状态j的概率。另外定义牛市、熊市、横盘的状态分别为0、1、2，这样我们得到了马尔科夫链模型的状态转移矩阵为：

$$
P = \left( \begin{array}{ccc}
0.9 \ \ \ \  0.075 \ \ \ \  0.025 \\
0.15 \ \ \ \  0.8 \ \ \ \  0.05 \\
0.25 \ \ \ \  0.25 \ \ \ \  0.5 
\end{array} \right)
$$
当这个状态转移矩阵P确定以后，整个股市模型就已经确定！

### 状态转移矩阵

从上面的例子不难看出来，整个马尔可夫链模型的核心是状态转移矩阵P。那这个矩阵P有一些什么有意思的地方呢？接下来再看一下。
以股市模型为例，假设初始状态为 $t_0 = [0.1, 0.2, 0.7]$，然后算之后的状态。

```python
def markov():
    init_array = np.array([0.1, 0.2, 0.7])
    transfer_matrix = np.array([[0.9, 0.075, 0.025],
                               [0.15, 0.8, 0.05],
                               [0.25, 0.25, 0.5]])
    restmp = init_array
    for i in range(25):
        res = np.dot(restmp, transfer_matrix)
        print i, "\t", res
        restmp = res

markov() 
```

最终输出的结果：

```
0 	[ 0.295   0.3425  0.3625]
1 	[ 0.4075   0.38675  0.20575]
2 	[ 0.4762  0.3914  0.1324]
3 	[ 0.52039   0.381935  0.097675]
4 	[ 0.55006   0.368996  0.080944]
5 	[ 0.5706394  0.3566873  0.0726733]
6 	[ 0.58524688  0.34631612  0.068437  ]
7 	[ 0.59577886  0.33805566  0.06616548]
8 	[ 0.60345069  0.33166931  0.06487999]
9 	[ 0.60907602  0.32681425  0.06410973]
10 	[ 0.61321799  0.32315953  0.06362248]
11 	[ 0.61627574  0.3204246   0.06329967]
12 	[ 0.61853677  0.31838527  0.06307796]
13 	[ 0.62021037  0.31686797  0.06292166]
14 	[ 0.62144995  0.31574057  0.06280949]
15 	[ 0.62236841  0.31490357  0.06272802]
16 	[ 0.62304911  0.31428249  0.0626684 ]
17 	[ 0.62355367  0.31382178  0.06262455]
18 	[ 0.62392771  0.31348008  0.06259221]
19 	[ 0.624205   0.3132267  0.0625683]
20 	[ 0.62441058  0.31303881  0.06255061]
21 	[ 0.624563    0.31289949  0.06253751]
22 	[ 0.624676   0.3127962  0.0625278]
23 	[ 0.62475978  0.31271961  0.06252061]
24 	[ 0.6248219   0.31266282  0.06251528]  
```

从第18次开始，状态就开始收敛至[0.624,0.312,0.0625]。最终数字上略有不同，只是计算机浮点数运算造成的罢了。

如果我们换一个初始状态 $t_0$ ，比如[0.2,0.3.0.5]，继续运行上面的代码，只是将init_array变一下，最后结果为：

```
0 	[ 0.35  0.38  0.27]
1 	[ 0.4395   0.39775  0.16275]
2 	[ 0.4959   0.39185  0.11225]
3 	[ 0.53315   0.378735  0.088115]
4 	[ 0.558674  0.365003  0.076323]
5 	[ 0.5766378  0.3529837  0.0703785]
6 	[ 0.5895162   0.34322942  0.06725438]
7 	[ 0.59886259  0.33561085  0.06552657]
8 	[ 0.6056996   0.32978501  0.06451539]
9 	[ 0.61072624  0.32538433  0.06388944]
10 	[ 0.61443362  0.32208429  0.06348209]
11 	[ 0.61717343  0.31962047  0.0632061 ]
12 	[ 0.61920068  0.31778591  0.06301341]
13 	[ 0.62070185  0.31642213  0.06287602]
14 	[ 0.62181399  0.31540935  0.06277666]
15 	[ 0.62263816  0.31465769  0.06270415]
16 	[ 0.62324903  0.31410005  0.06265091]
17 	[ 0.62370187  0.31368645  0.06261168]
18 	[ 0.62403757  0.31337972  0.06258271]
19 	[ 0.62428645  0.31315227  0.06256128]
20 	[ 0.62447096  0.31298362  0.06254542]
21 	[ 0.62460776  0.31285857  0.06253366]
22 	[ 0.62470919  0.31276586  0.06252495]
23 	[ 0.62478439  0.31269711  0.0625185 ]
24 	[ 0.62484014  0.31264614  0.06251372] 
```

到第18次的时候，又收敛到了[0.624,0.312,0.0625]
这个转移矩阵就厉害了。不管我们的初始状态是什么样子的，只要状态转移矩阵不发生变化，当 $n \to \infty$时，最终状态始终会收敛到一个固定值。

在矩阵分析，自动控制原理等过程中，经常会提到矩阵的幂次方的性质。我们也看看这个状态转移矩阵 $P$ 的幂次方有什么有意思的地方？废话不多说，直接上代码。

```python
def matrixpower():
    transfer_matrix = np.array([[0.9, 0.075, 0.025],
                               [0.15, 0.8, 0.05],
                               [0.25, 0.25, 0.5]])
    restmp = transfer_matrix
    for i in range(25):
        res = np.dot(restmp, transfer_matrix)
        print i, "\t", res
        restmp = res

matrixpower() 
```

从第20次开始，结果开始收敛，并且每一行都为 [0.625, 0.312, 0.0625].

### 马尔可夫链细致平稳条件

首先，==马尔科夫链要能收敛，需要满足以下条件==：
1.可能的状态数是有限的。
2.状态间的转移概率需要固定不变。
3.从任意状态能够转变到任意状态。
4.不能是简单的循环，例如全是从x到y再从y到x。

以上是马尔可夫链收敛的必要条件。

假设有一个概率的单纯形向量 $v_0$ ，例如我们前面的例子 [0.2,0.3.0.5]

有一个概率转移矩阵P，例如我们前面的例子：


$$
P = \left( \begin{array}{ccc}0.9\quad 0.075 \quad 0.025 \\0.15 \quad 0.8 \quad 0.05 \\0.25 \quad 0.25 \quad 0.5 \end{array} \right)
$$
其中，$v_0$ 每个元素的取值范围为[0,1]，并且所有元素的和为1。而P的每一行也是个概率单纯形向量。由前面的例子我们不难看出，当 $v_0$ 与P的n次幂相乘以后，发现得到的向量都会收敛到一个稳定值，而且此稳定值与初始向量$v_0$ 无关！

那么所有的转移矩阵P都有这种现象嘛？或者说满足什么样的条件的转移矩阵P会有这种现象？

==细致平衡条件（Detailed Balance Condition）==：给定一个马尔科夫链，分布 $\pi$ 和概率转移矩阵P，如果下面等式成立:  $\pi_i P_{ij} = \pi_j P_{ji}$ 则此马尔科夫链具有一个平稳分布（Stationary Distribution)。

证明过程比较简单：
$$
\sum_{i=1}^{\infty} \pi(i) P(i,j) = \sum_{i=1}^{\infty} \pi(j) P(j,i) = \pi(j) \sum_{i=1}^{\infty} P(j, i) = \pi(j)
$$
上式取 $j \to \infty$，就可以得到矩阵表达式：$\pi P = \pi$

### 马尔可夫链收敛性质

如果一个==非周期==的马尔可夫链收敛，==有状态转移矩阵P==，并且==任何两个状态都是连通==的，==那么 $lim_{n \to \infty} P_{ij} $  为一定值，且与 $i$ 无关==。
1.
$$
lim_{n \to \infty} P_{ij} ^ n = \pi (j)
$$
2.
$$
lim_{n \to \infty} P ^ n = \left( \begin{array}{ccc}\pi(1) \quad \pi(2) \quad \cdots \quad \pi(j) \quad \cdots \\ \pi(1) \quad \pi(2) \quad \cdots \quad \pi(j) \quad \cdots \\ \cdots \quad \cdots \quad \cdots \quad \cdots \quad \cdots \\\pi(1) \quad \pi(2) \quad \cdots \quad \pi(j) \quad \cdots \\ \cdots \quad \cdots \quad \cdots \quad \cdots \quad \cdots \end{array} \right)
$$


$\pi$是方程$\pi P = \pi$的唯一非负解，其中：$π(j)=[π(1),π(2),⋯,π(j),⋯]\  \sum_{i=0}^∞ π(i) = 1$

### 应用

马尔可夫链在实际中有非常广泛的应用。例如==奠定互联网基础的PageRank算法，就是由马尔可夫链定义的==。如果$N$是已知网页的数量，一个页面$i$有 $k_i$ 个链接到这个页面，那么它到链接页面的转换概率为$\frac{\alpha}{k_i} + \frac{1 - \alpha}{N}$  ，到未链接页面的概率为 $\frac{1 - \alpha}{N}$  ，参数$\alpha$的取值大约是0.85。

另外像语音识别中的HMM隐马尔可夫模型，也在实际中使用非常多，并且在DNN问世之前一直都是语音识别领域中最主流的方法。
