# 常见的几种最优化方法

　

​		我们每个人都会在我们的生活或者工作中遇到各种各样的最优化问题，比如每个企业和个人都要考虑的一个问题“在一定成本下，如何使利润最大化”等。最优化方法是一种数学方法，它是研究在给定约束之下如何寻求某些因素(的量)，以使某一(或某些)指标达到最优的一些学科的总称。随着学习的深入，博主越来越发现最优化方法的重要性，学习和工作中遇到的大多问题都可以建模成一种最优化模型进行求解，比如我们现在学习的机器学习算法，大部分的机器学习算法的本质都是建立优化模型，通过最优化方法对目标函数（或损失函数）进行优化，从而训练出最好的模型。常见的最优化方法有==梯度下降法==、==牛顿法和拟牛顿法==、==共轭梯度法==等等。

### 1. 梯度下降法（Gradient Descent）

　　梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。

　　梯度下降法的缺点：

　　（1）靠近极小值时收敛速度减慢，如下图所示；

　　（2）直线搜索时可能会产生一些问题；

　　（3）可能会“之字形”地下降。

​		梯度下降法在接近最优解的区域收敛速度明显变慢，利用梯度下降法求解需要很多次的迭代。在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。

### 2. 牛顿法和拟牛顿法（Newton's method & Quasi-Newton Methods） 

　　**1）牛顿法（Newton's method）**

　　牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数*f* (*x*)的泰勒级数的前面几项来寻找方程*f* (*x*) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。

​		具体步骤：

　　首先，选择一个接近函数 *f* (*x*)零点的 *x*0，计算相应的 *f* (*x*0) 和切线斜率*f  '* (*x*0)（这里*f '* 表示函数 *f*  的导数）。然后我们计算穿过点(*x*0,  *f*  (*x*0)) 并且斜率为*f* '(*x*0)的直线和 *x* 轴的交点的*x*坐标，也就是求如下方程的解：

![img](https://images0.cnblogs.com/blog2015/764050/201508/222309088311820.png)

　　我们将新求得的点的 *x* 坐标命名为*x*1，通常*x*1会比*x*0更接近方程*f*  (*x*) = 0的解。因此我们现在可以利用*x*1开始下一轮迭代。迭代公式可化简为如下所示：

![img](https://images0.cnblogs.com/blog2015/764050/201508/222309221284615.png)

　　已经证明，如果*f*  ' 是连续的，并且待求的零点*x*是孤立的，那么在零点*x*周围存在一个区域，只要初始值*x*0位于这个邻近区域内，那么牛顿法必定收敛。 并且，如果*f*  ' (*x*)不为0, 那么牛顿法将具有平方收敛的性能. 粗略的说，这意味着每迭代一次，牛顿法结果的有效数字将增加一倍。下图为一个牛顿法执行过程的例子。

　　由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是"切线法"。牛顿法的搜索路径（二维情况）如下图所示：

　　牛顿法搜索动态示例图：

![img](https://upload.wikimedia.org/wikipedia/commons/e/e0/NewtonIteration_Ani.gif)

　　关于牛顿法和梯度下降法的效率对比：

　　从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。）

　　根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。

![img](https://images0.cnblogs.com/blog2015/764050/201508/222309373784741.png)

注：红色的牛顿法的迭代路径，绿色的是梯度下降法的迭代路径。

　　牛顿法的优缺点总结：

　　优点：二阶收敛，收敛速度快；

　　缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。

　　**2）拟牛顿法（Quasi-Newton Methods）**

　　拟牛顿法是求解非线性优化问题最有效的方法之一，于20世纪50年代由美国Argonne国家实验室的物理学家W.C.Davidon所提出来。Davidon设计的这种算法在当时看来是非线性优化领域最具创造性的发明之一。不久R. Fletcher和M. J. D. Powell证实了这种新的算法远比其他方法快速和可靠，使得非线性优化这门学科在一夜之间突飞猛进。

　　**拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。**拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。

　　**具体步骤：**

　　拟牛顿法的基本思想如下。首先构造目标函数在当前迭代xk的二次模型：

![img](https://images0.cnblogs.com/blog2015/764050/201508/222253268161863.png)

　　这里Bk是一个对称正定矩阵，于是我们取这个二次模型的最优解作为搜索方向，并且得到新的迭代点：

![img](https://images0.cnblogs.com/blog2015/764050/201508/222254384106201.png)

　　其中我们要求步长ak

满足Wolfe条件。这样的迭代与牛顿法类似，区别就在于用近似的Hesse矩阵Bk  

的更新。现在假设得到一个新的迭代xk+1，并得到一个新的二次模型：

![img](https://images0.cnblogs.com/blog2015/764050/201508/222256385508904.png)

　　我们尽可能地利用上一步的信息来选取Bk。具体地，我们要求

![img](https://images0.cnblogs.com/blog2015/764050/201508/222257530664204.png)

　　从而得到

![img](https://images0.cnblogs.com/blog2015/764050/201508/222258392223638.png)

　　这个公式被称为割线方程。常用的拟牛顿法有DFP算法和BFGS算法。

### 3. 共轭梯度法（Conjugate Gradient）

　　共轭梯度法是介于最速下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。 在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数。

　　具体的实现步骤请参加[wiki百科共轭梯度法](https://en.wikipedia.org/wiki/Conjugate_gradient_method#Example_code_in_MATLAB)。下图为共轭梯度法和梯度下降法搜索最优解的路径对比示意图：

![img](http://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Conjugate_gradient_illustration.svg/220px-Conjugate_gradient_illustration.svg.png)

​		注：绿色为梯度下降法，红色代表共轭梯度法

　　MATLAB代码：

```matlab
function [x] = conjgrad(A,b,x)
    r=b-A*x;
    p=r;
    rsold=r'*r;

    for i=1:length(b)
        Ap=A*p;
        alpha=rsold/(p'*Ap);
        x=x+alpha*p;
        r=r-alpha*Ap;
        rsnew=r'*r;
        if sqrt(rsnew)<1e-10
              break;
        end
        p=r+(rsnew/rsold)*p;
        rsold=rsnew;
    end
end
```

### 4. 启发式优化方法

　　启发式方法指人在解决问题时所采取的一种根据经验规则进行发现的方法。其特点是在解决问题时,利用过去的经验,选择已经行之有效的方法，而不是系统地、以确定的步骤去寻求答案。启发式优化方法种类繁多，包括经典的==模拟退火方法==、==遗传算法==、==蚁群算法==以及==粒子群算法==等等。

　　还有一种特殊的优化算法被称之多目标优化算法，它主要针对同时优化多个目标（两个及两个以上）的优化问题，这方面比较经典的算法有==NSGAII算法==、==MOEA/D算法==以及==人工免疫==算法等。

###  5. 解决约束优化问题——拉格朗日乘数法

​		拉格朗日乘数法的基本思想：作为一种优化算法，拉格朗日乘子法主要用于解决约束优化问题，它的基本思想就是通过引入拉格朗日乘子来将含有n个变量和k个约束条件的约束优化问题转化为含有（n+k）个变量的无约束优化问题。拉格朗日乘子背后的数学意义是其为约束方程梯度线性组合中每个向量的系数。

　　如何将一个含有n个变量和k个约束条件的约束优化问题转化为含有（n+k）个变量的无约束优化问题？拉格朗日乘数法从数学意义入手，通过引入拉格朗日乘子建立极值条件，对n个变量分别求偏导对应了n个方程，然后加上k个约束条件（对应k个拉格朗日乘子）一起构成包含了（n+k）变量的（n+k）个方程的方程组问题，这样就能根据求方程组的方法对其进行求解。